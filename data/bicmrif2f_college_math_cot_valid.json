[
    {
        "informal_statement": "Theorem 9.24 For a 2D almost-Riemannian structure, an extremal trajectory $\\gamma$ admits an abnormal lift if and only if $\\gamma$ is a constant curve contained in $\\mathcal{Z}$ .",
        "informal_proof": "Proof It is immediate to verify that if $\\gamma$ is a constant curve contained in $\\mathcal{Z}$ then $\\gamma$ admits an abnormal lift.\\n\\nLet $\\gamma : \\left\\lbrack {a, b}\\right\\rbrack \\rightarrow M, a < b$, be the projection of an abnormal extremal and let us prove that $\\gamma$ is a constant curve contained in $\\mathcal{Z}$ .\\n\\nWe first prove that $\\gamma \\left( \\left\\lbrack {a, b}\\right\\rbrack \\right) \\subset \\mathcal{Z}$ . By way of contradiction, assume that there exists $\\bar{t} \\in \\rbrack a, b\\lbrack$ such that $\\gamma \\left( \\bar{t}\\right) \\notin \\mathcal{Z}$ . By continuity there exists a nontrivial interval $\\left\\lbrack {c, d}\\right\\rbrack \\subset \\rbrack a, b\\lbrack$ such that $\\gamma \\left( \\left\\lbrack {c, d}\\right\\rbrack \\right) \\cap \\mathcal{Z} = \\varnothing$ . Then ${\\gamma }_{\\left\\lbrack c, d\\right\\rbrack }$ is a Riemannian extremal trajectory and hence cannot be abnormal. Recall that if an arc of an extremal trajectory is not abnormal, then neither is the extremal trajectory abnormal; hence it follows that $\\gamma$ is not abnormal. This contradicts the hypothesis that $\\gamma$ is the projection of an abnormal extremal.\\n\\nLet us now prove that $\\gamma$ is a constant curve. Let us fix a local system of coordinates and an orthonormal frame as in Proposition 9.8. If this is not possible globally on a neighborhood of $\\gamma \\left( \\left\\lbrack {a, b}\\right\\rbrack \\right)$, one can repeat the proof chart by chart. Let us write in coordinates $\\gamma \\left( t\\right) = \\left( {{\\gamma }_{1}\\left( t\\right) ,{\\gamma }_{2}\\left( t\\right) }\\right)$ . We have different cases.\\n\\n- If $\\left( {{\\gamma }_{1}\\left( t\\right) ,{\\gamma }_{2}\\left( t\\right) }\\right) = \\left( {{c}_{1},{c}_{2}}\\right)$ for every $t \\in \\left\\lbrack {a, b}\\right\\rbrack$, we already know that $\\gamma$ admits an abnormal lift.\\n\\n- If ${\\gamma }_{1}$ is not constant and ${\\gamma }_{2} = c$ in $\\left\\lbrack {a, b}\\right\\rbrack$ then ${\\dot{\\gamma }}_{2} = 0$ in $\\left\\lbrack {a, b}\\right\\rbrack$ and $\\mathcal{Z}$ contains a set of the type\\n\\n$$\\n\\overline{\\mathcal{Z}} = \\left\\{ {\\left( {{x}_{1}, c}\\right) \\mid {x}_{1} \\in \\left\\lbrack {{x}_{1}^{A},{x}_{1}^{B}}\\right\\rbrack }\\right\\} \\;\\text{ with }{x}_{1}^{A} < {x}_{1}^{B}.\\n$$\\n\\nSince $\\mathfrak{f} = 0$ on $\\mathcal{Z}$ and hence on $\\widetilde{\\mathcal{Z}}$, it follows that ${\\partial }_{{x}_{1}}^{r}\\mathfrak{f} \\equiv 0$ on $\\widetilde{\\mathcal{Z}}$ for every $r = 1,2,\\ldots$ As in the proof of Theorem 9.14, it follows that all brackets between ${F}_{1}$ and ${F}_{2}$ are zero on $\\overline{\\mathcal{Z}}$ and that the bracket-generating condition is violated. Hence this case is not possible.\\n\\n- There exists $\\bar{t} \\in \\rbrack a, b\\left\\lbrack \\right.$ such that ${\\dot{\\gamma }}_{2}\\left( \\bar{t}\\right)$ is defined and ${\\dot{\\gamma }}_{2}\\left( \\bar{t}\\right) \\neq 0$ . Now, since\\n\\n$$\\n\\dot{\\gamma }\\left( \\bar{t}\\right) = \\left( \\begin{matrix} {v}_{1} \\\\ {v}_{2}f\\left( {\\gamma \\left( \\bar{t}\\right) }\\right) \\end{matrix}\\right)\\n$$\\n\\nfor some ${v}_{1},{v}_{2} \\in \\mathbb{R}$, we have $\\mathfrak{f}\\left( {\\gamma \\left( \\bar{t}\\right) }\\right) \\neq 0$ and hence $\\gamma \\left( \\bar{t}\\right) \\notin \\mathcal{Z}$, violating the condition $\\gamma \\left( \\left\\lbrack {a, b}\\right\\rbrack \\right) \\subset \\mathcal{Z}$ . Hence this case is not possible either.",
        "id": "college_math_26599"
    },
    {
        "informal_statement": "Proposition 1.2.2. If $C$ is cofree, i.e. $C = {T}^{c}\\left( V\\right)$ for some vector space $V$, then a coderivation $d \\in \\operatorname{Coder}\\left( {{T}^{c}\\left( V\\right) }\\right)$ is completely determined by its weight 1 component\\n\\n$$ \\n{T}^{c}\\left( V\\right) \\overset{d}{ \\rightarrow }{T}^{c}\\left( V\\right) \\xrightarrow[]{{pro}{j}_{V}}V \\n$$",
        "informal_proof": "Proof. Let us denote by $f\\left( x\\right) \\mathrel{\\text{:=}} d{\\left( x\\right) }^{\\left( 1\\right) } = {\\operatorname{proj}}_{V}\\left( {d\\left( x\\right) }\\right)$ the weight-one component of $d\\left( x\\right)$ . Since ${T}^{c}\\left( V\\right)$ is cofree, the weight $n$ component of $d\\left( x\\right)$, denoted by $d{\\left( x\\right) }^{\\left( n\\right) } \\in {V}^{\\otimes n}$, is given by\\n\\n$$ \\nd{\\left( x\\right) }^{\\left( n\\right) } = \\mathop{\\sum }\\limits_{{i = 1}}^{n}\\mathop{\\sum }\\limits_{\\left( x\\right) }{\\operatorname{proj}}_{V}\\left( {x}_{\\left( 1\\right) }\\right) \\otimes \\cdots \\otimes f\\left( {x}_{\\left( i\\right) }\\right) \\otimes \\cdots \\otimes {\\operatorname{proj}}_{V}\\left( {x}_{\\left( n\\right) }\\right) , \\n$$\\n\\nwhere $\\mathop{\\sum }\\limits_{\\left( x\\right) }{x}_{\\left( 1\\right) } \\otimes \\cdots \\otimes {x}_{\\left( n\\right) } \\mathrel{\\text{:=}} {\\bar{\\Delta }}^{n - 1}\\left( x\\right)$ and $d{\\left( x\\right) }^{\\left( 0\\right) } = 0$ . So the coderivation $d$ is completely determined by $f \\mathrel{\\text{:=}} {\\operatorname{proj}}_{V} \\circ d : {T}^{c}\\left( V\\right) \\rightarrow V$ .",
        "id": "college_math_8886"
    },
    {
        "informal_statement": "Theorem 249 If \\( f\\left( x\\right) \\) is monotone on the closed interval \\( \\left\\lbrack {a, b}\\right\\rbrack \\), then it is Riemann integrable on \\( \\left\\lbrack {a, b}\\right\\rbrack \\) .",
        "informal_proof": "Proof We assume that \\( f\\left( x\\right) \\) is nondecreasing (the nonincreasing case is proved similarly; see Exercise 278). Since \\( f\\left( a\\right) \\leq f\\left( x\\right) \\leq f\\left( b\\right) \\) for all \\( x \\in \\left\\lbrack {a, b}\\right\\rbrack \\), it follows that \\( f \\) is bounded. Let \\( \\varepsilon > 0 \\) .\\n\\nFor each partition \\( P = \\left\\{ {{x}_{0},{x}_{1},\\cdots ,{x}_{N}}\\right\\} \\) of \\( \\left\\lbrack {a, b}\\right\\rbrack \\), the minimum \\( {m}_{i} \\) and maximum \\( {M}_{i} \\) of \\( f\\left( x\\right) \\) on each subinterval \\( \\left\\lbrack {{x}_{i - 1},{x}_{i}}\\right\\rbrack \\) occur at an endpoint:\\n\\n\\[ \\n{m}_{i} = f\\left( {x}_{i - 1}\\right) ,\\;{M}_{i} = f\\left( {x}_{i}\\right) \\n\\]\\n\\nHence,\\n\\n\\[ \\nU\\left( P\\right) - L\\left( P\\right) = \\mathop{\\sum }\\limits_{{i = 1}}^{N}\\left\\lbrack {f\\left( {x}_{i}\\right) - f\\left( {x}_{i - 1}\\right) }\\right\\rbrack \\Delta {x}_{i} \\n\\]\\n\\nRecall that \\( \\Delta {x}_{i} = {x}_{i} - {x}_{i - 1} \\leq \\parallel P\\parallel \\) for every index \\( i \\), so we have\\n\\n\\[ \\nU\\left( P\\right) - L\\left( P\\right) \\leq \\mathop{\\sum }\\limits_{{i = 1}}^{N}\\left\\lbrack {f\\left( {x}_{i}\\right) - f\\left( {x}_{i - 1}\\right) }\\right\\rbrack \\parallel P\\parallel = \\parallel P\\parallel \\mathop{\\sum }\\limits_{{i = 1}}^{N}\\left\\lbrack {f\\left( {x}_{i}\\right) - f\\left( {x}_{i - 1}\\right) }\\right\\rbrack \\n\\]\\n\\nThe last sum telescopes down to a single difference:\\n\\n\\[ \\n\\mathop{\\sum }\\limits_{{i = 1}}^{N}\\left\\lbrack {f\\left( {x}_{i}\\right) - f\\left( {x}_{i - 1}\\right) }\\right\\rbrack = f\\left( {x}_{N}\\right) - f\\left( {x}_{0}\\right) = f\\left( b\\right) - f\\left( a\\right) \\n\\]\\n\\nTherefore,\\n\\n\\[ \\nU\\left( P\\right) - L\\left( P\\right) \\leq \\parallel P\\parallel \\left\\lbrack {f\\left( b\\right) - f\\left( a\\right) }\\right\\rbrack .\\n\\]\\n\\nWe may choose a partition \\( P \\) so that \\( \\parallel P\\parallel \\left\\lbrack {f\\left( b\\right) - f\\left( a\\right) }\\right\\rbrack < \\varepsilon \\), then Theorem 246 implies that \\( f \\) is Riemann integrable on \\( \\left\\lbrack {a, b}\\right\\rbrack \\) .",
        "id": "college_math_294792"
    },
    {
        "informal_statement": "Theorem 12.3.1. Let \\( \\psi \\left( x\\right), x \\in {\\mathbb{R}}_{ + }^{n} \\), be a concave strictly monotone function. Let \\( \\xi = \\left\\{ {{x}_{0},{x}_{1},\\ldots ,{x}_{N}}\\right\\} \\) be a path with initial state \\( {x}_{0} \\) maximizing \\( \\psi \\left( {x}_{N}\\right) \\) over all such paths. Then \\( \\xi \\) is rapid.",
        "informal_proof": "Proof of Theorem 12.3.1. Consider the following maximization problem:\\n\\n\\[ \\text{Maximize}F\\left( \\theta \\right) = \\psi \\left( {a}_{N}\\right) \\tag{12.31} \\]\\n\\nover all sequences \\( \\theta = {\\left\\{ \\left( {a}_{t},{b}_{t}\\right) \\right\\} }_{t = 0}^{N} \\) satisfying\\n\\n\\[ {b}_{0} \\in {\\mathbb{R}}_{ + }^{n},\\left( {{a}_{t - 1},{b}_{t}}\\right) \\in {Z}_{t}\\left( {t = 1,\\ldots, N}\\right) ,{a}_{N} \\in {\\mathbb{R}}_{ + }^{n}, \\tag{12.32} \\]\\n\\n\\[ {b}_{t} \\geq {a}_{t}\\left( {t = 0,\\ldots, N}\\right) \\tag{12.33} \\]\\n\\nand \\( {b}_{0} = {x}_{0} \\) . Observe that the sequence \\( \\bar{\\theta } = {\\left\\{ \\left( {\\bar{a}}_{t},{\\bar{b}}_{t}\\right) \\right\\} }_{t = 0}^{N} \\) defined by \\( {\\bar{a}}_{t} = \\) \\( {\\bar{b}}_{t} = {x}_{t} \\) is a solution to the above maximization problem. Indeed, \\( \\bar{\\theta } \\) satisfies constraints (12.32) and (12.33) because \\( \\xi \\) is a path with initial state \\( {x}_{0} \\), and we have \\( F\\left( \\theta \\right) = \\psi \\left( {x}_{N}\\right) \\) . Further, for any \\( \\theta = {\\left\\{ \\left( {a}_{t},{b}_{t}\\right) \\right\\} }_{t = 0}^{N} \\) satisfying (12.32) and (12.33), the sequence \\( \\left\\{ {{x}_{0},{a}_{1},{a}_{2},\\ldots ,{a}_{N}}\\right\\} \\) is a path with initial state \\( {x}_{0} \\) (by virtue of hypothesis (Z.3)), and \\( F\\left( \\theta \\right) = \\psi \\left( {a}_{N}\\right) \\) . Consequently, \\( F\\left( \\theta \\right) = \\psi \\left( {a}_{N}\\right) \\leq \\) \\( \\psi \\left( {x}_{N}\\right) = F\\left( \\bar{\\theta }\\right) \\), which proves the optimality of \\( \\bar{\\theta } \\) .\\n\\nWe are going to apply the Kuhn-Tucker theorem (see the Appendix, Theorem A.1) to the concave optimization problem (12.31)-(12.33). From (Z.3) and (Z.4), it follows that \\( \\left( {e,{\\kappa e}}\\right) \\in {Z}_{t}\\left( {t = 1,2,\\ldots, N}\\right) \\) for some \\( \\kappa > 0 \\) . Define \\( \\check{\\theta } = {\\left\\{ \\left( {\\check{a}}_{t},{\\check{b}}_{t}\\right) \\right\\} }_{t = 0}^{N} \\), where\\n\\n\\[ {\\check{b}}_{0} = {x}_{0},\\left( {{\\check{a}}_{t - 1},{\\check{b}}_{t}}\\right) = {\\mu }^{t}\\left( {e,{\\kappa e}}\\right) \\left( {t = 1,\\ldots, N}\\right) ,{\\check{a}}_{N} = {\\mu }^{N + 1}e \\]\\n\\nand \\( \\mu > 0 \\) is some number satisfying \\( {\\mu e} < {x}_{0} \\) and \\( \\mu < \\kappa \\) . Then we have \\( {\\check{b}}_{t} > {\\check{a}}_{t} \\) for all \\( t = 0,\\ldots, N \\), and so the Slater condition holds. By virtue of the Kuhn-Tucker theorem, there exist vectors \\( {p}_{0},\\ldots ,{p}_{N} \\in {\\mathbb{R}}_{ + }^{n} \\) such that\\n\\n\\[ \\psi \\left( {a}_{N}\\right) + \\mathop{\\sum }\\limits_{{t = 0}}^{N}{p}_{t}\\left( {{b}_{t} - {a}_{t}}\\right) \\leq \\psi \\left( {x}_{N}\\right) \\tag{12.34} \\]\\n\\nfor all \\( {\\left\\{ \\left( {a}_{t},{b}_{t}\\right) \\right\\} }_{t = 0}^{N} \\) satisfying (12.32) and \\( {b}_{0} = {x}_{0} \\) . From (12.34), we get\\n\\n\\[ {p}_{t}{b}_{t} - {p}_{t - 1}{a}_{t - 1} \\leq {p}_{t}{x}_{t} - {p}_{t - 1}{x}_{t - 1}\\left( {t = 1,2,\\ldots, N}\\right) , \\tag{12.35} \\]\\n\\n\\[ \\psi \\left( {a}_{N}\\right) - {p}_{N}{a}_{N} \\leq \\psi \\left( {x}_{N}\\right) - {p}_{N}{x}_{N} \\tag{12.36} \\]\\n\\nfor all \\( \\left( {{a}_{t - 1},{b}_{t}}\\right) \\in {Z}_{t}\\left( {t = 1,\\ldots, N}\\right) \\) and \\( {a}_{N} \\in {\\mathbb{R}}_{ + }^{n} \\) . Inequalities (12.35) imply\\n\\n\\[ {p}_{t}{x}_{t} - {p}_{t - 1}{x}_{t - 1} = 0 \\tag{12.37} \\]\\n\\nand \\( {p}_{t}y - {p}_{t - 1}x \\leq 0,\\left( {x, y}\\right) \\in {Z}_{t} \\), because \\( {Z}_{t} \\) is a cone. Thus \\( \\left\\{ {p}_{t}\\right\\} \\) is a dual path.\\n\\nBy setting \\( {a}_{N} = {x}_{N} + {e}_{i} \\) in (12.36) and using the strict monotonicity of \\( \\psi \\) , we find\\n\\n\\[ {p}_{N}{e}_{i} = {p}_{N}\\left( {{a}_{N} - {x}_{N}}\\right) \\geq \\psi \\left( {a}_{N}\\right) - \\psi \\left( {x}_{N}\\right) = \\psi \\left( {{x}_{N} + {e}_{i}}\\right) - \\psi \\left( {x}_N\\right)",
        "id": "college_math_274872"
    },
    {
        "informal_statement": "Example 3.1. Recall the network in Figure 1.2(b) which depicts the conversation between two sources over a communication network. An equivalent representation of this network obtained by creating a single source node that generates both \\( {b}_{1} \\) and \\( {b}_{2} \\) and appending two imaginary incoming channels to the source node is shown in Figure 3.1. Let \\( {ST} \\) precede \\( {VT} \\) in the ordering among the channels. Similarly, let \\( S{T}^{\\prime } \\n\\n--- \\n\\n![019187b8-257d-7080-9e12-d57462bc4625_57_240439.jpg](images/019187b8-257d-7080-9e12-d57462bc4625_57_240439.jpg) \\n\\nFig. 3.1 A 2-dimensional linear broadcast on a cyclic network. \\n\\nprecede \\( V{T}^{\\prime } \\) . Given the local encoding kernels \\n\\n\\[ \\n{K}_{S} = \\left\\lbrack \\begin{array}{ll} 1 & 0 \\\\ 0 & 1 \\end{array}\\right\\rbrack ,{K}_{T} = {K}_{{T}^{\\prime }} = \\left\\lbrack \\begin{array}{l} 1 \\\\ 0 \\end{array}\\right\\rbrack ,{K}_{U} = \\left\\lbrack \\begin{array}{l} 1 \\\\ 1 \\end{array}\\right\\rbrack ,{K}_{V} = \\left\\lbrack \\begin{array}{ll} 1 & 1 \\end{array}\\right\\rbrack , \\n\\] \\n\\nthe equation (2.3) yields the following unique solution for the global encoding kernels:",
        "informal_proof": "\\[ \\n{f}_{ST} = {f}_{TU} = \\left\\lbrack \\begin{array}{l} 1 \\\\ 0 \\end{array}\\right\\rbrack ,{f}_{S{T}^{\\prime }} = {f}_{{T}^{\\prime }U} = \\left\\lbrack \\begin{array}{l} 0 \\\\ 1 \\end{array}\\right\\rbrack \\n\\] \\n\\n\\[ \\n{f}_{UV} = {f}_{VT} = {f}_{V{T}^{\\prime }} = \\left\\lbrack \\begin{array}{l} 1 \\\\ 1 \\end{array}\\right\\rbrack \\n\\] \\n\\nThese encoding kernels are shown in Figure 3.1 and in fact, define a 2-dimensional linear broadcast regardless of the choice of the base field.",
        "id": "college_math_358471"
    },
    {
        "informal_statement": "Lemma 7.30 Let \\( \\mathfrak{g} = \\mathfrak{{sl}}\\left( \\infty \\right) \\) . Then \\( {\\left( {\\mathcal{A}}_{\\mathfrak{g}}\\right) }_{1}^{p, q} \\) as a left module over \\( {\\left( {\\mathcal{A}}_{\\mathfrak{g}}\\right) }_{0}^{p - 1, q - 1} \\) is generated by the contractions \\( {\\Phi }_{i, j} : {T}^{p, q} \\rightarrow {T}^{p - 1, q - 1} \\), as defined in (7.3).",
        "informal_proof": "Proof Let \\( \\varphi \\in {\\left( {\\mathcal{A}}_{\\mathfrak{g}}\\right) }_{1}^{p, q} = {\\operatorname{Hom}}_{\\mathfrak{g}}\\left( {{T}^{p, q},{T}^{p - 1, q - 1}}\\right) \\) . By Theorem 7.4,\\n\\n\\[ \\operatorname{soc}{T}^{p, q} = \\mathop{\\bigcap }\\limits_{{i \\leq p, j \\leq q}}\\ker {\\Phi }_{i, j} \\]\\n\\nMoreover, Theorem 7.2 implies that \\( \\operatorname{soc}\\left( {T}^{p, q}\\right) \\subset \\ker \\varphi \\) . Define\\n\\n\\[ \\Phi : {T}^{p, q} \\rightarrow {\\bigoplus }_{i \\leq p, j \\leq q}{T}^{p - 1, q - 1} \\]\\n\\nas the direct sum \\( {\\bigoplus }_{i, j}{\\Phi }_{i, j} \\) . Then there exists\\n\\n\\[ \\alpha : {\\bigoplus }_{i \\leq p, j \\leq q}{T}^{p - 1, q - 1} \\rightarrow {T}^{p - 1, q - 1} \\]\\n\\nsuch that \\( \\varphi = \\alpha \\circ \\Phi \\) . Since \\( \\alpha = {\\bigoplus }_{i, j}{\\alpha }_{i, j} \\) for some \\( {\\alpha }_{i, j} \\in {\\left( {\\mathcal{A}}_{\\mathfrak{g}}\\right) }_{0}^{p - 1, q - 1} = \\mathbb{C}\\left\\lbrack {{S}_{p - 1} \\times }\\right. \\) \\( \\left. {S}_{q - 1}\\right\\rbrack \\), we have \\( \\varphi = \\mathop{\\sum }\\limits_{{i, j}}{\\alpha }_{i, j}{\\Phi }_{i, j} \\), which concludes the proof.",
        "id": "college_math_310232"
    },
    {
        "informal_statement": "Example 1. We consider the generalized Hilbert problem\\n\\n\\\\[ \\n\\\\operatorname{Re}\\\\left\\\\{ {{t}^{-\\\\kappa }\\\\left\\\\lbrack {u\\\\left( t\\\\right) + {iv}\\\\left( {-t}\\\\right) }\\\\right\\\\rbrack }\\\\right\\\\} = h\\\\left( t\\\\right) \\\\text{ on }\\\\mathbb{T}. \\\\tag{31} \\n\\\\]\\n\\nHere \\\\( \\\\alpha \\\\left( t\\\\right) \\\\left( { = - t}\\\\right) \\\\) is a direct Carleman shift \\\\( \\\\left( {\\\\gamma = 1}\\\\right) ,\\\\mathcal{A}\\\\left( t\\\\right) = \\\\operatorname{Re}{t}^{-\\\\kappa },\\\\mathcal{B}\\\\left( t\\\\right) = i\\\\operatorname{Im}{t}^{-\\\\kappa } \\\\), and \\\\( \\\\kappa \\\\) is an integer. The Noetherity condition (7) is fulfilled since\\n\\n\\\\[ \\n\\\\mathcal{A}\\\\left( t\\\\right) \\\\mathcal{A}\\\\left( {-t}\\\\right) - \\\\mathcal{B}\\\\left( t\\\\right) \\\\mathcal{B}\\\\left( {-t}\\\\right) = {\\\\left( -1\\\\right) }^{\\\\kappa } \\\\neq 0.\\n\\\\]\\n\\nThus \\\\( {I}_{0} = 0 \\\\) and hence \\\\( I = 1 \\\\) . Now we are concerned with the solution of boundary value problem (31). It is easy to see that problem (31) is equivalent to the following system of two problems\\n\\n\\\\[ \\n\\\\operatorname{Re}\\\\left\\\\{ {{t}^{-\\\\kappa }\\\\left\\\\lbrack {\\\\Phi \\\\left( t\\\\right) + \\\\Phi \\\\left( {-t}\\\\right) }\\\\right\\\\rbrack }\\\\right\\\\} = h\\\\left( t\\\\right) + {\\\\left( -1\\\\right) }^{\\\\kappa }h\\\\left( {-t}\\\\right) , \\\\tag{32} \\n\\\\]\\n\\n\\\\[ \\n\\\\operatorname{Re}\\\\left\\\\{ {{t}^{\\\\kappa }\\\\left\\\\lbrack {\\\\Phi \\\\left( t\\\\right) - \\\\Phi \\\\left( {-t}\\\\right) }\\\\right\\\\rbrack }\\\\right\\\\} = h\\\\left( t\\\\right) - {\\\\left( -1\\\\right) }^{\\\\kappa }h\\\\left( {-t}\\\\right) , \\\\tag{33} \\n\\\\]\\n\\nwhere \\\\( \\\\Phi \\\\left( t\\\\right) = u\\\\left( t\\\\right) + {iv}\\\\left( t\\\\right) \\\\) .",
        "informal_proof": "Thus we have reduced the generalized Hilbert problem (31) to a system of two problems of Schwartz, which involve finding functions analytic in a domain \\\\( D \\\\) if the real parts of these functions are given on \\\\( \\\\partial D \\\\) . The solution of the Schwartz problem is expounded, for instance, in Gakhov's book [Gak1]. The solutions of Schwartz's problems (32) and (33) are found in the class of even and odd analytic functions, respectively. In fact, we show that the solution of the Schwartz problem\\n\\n\\\\[ \\n\\\\operatorname{Re}F\\\\left( t\\\\right) = h\\\\left( t\\\\right) + {\\\\mu h}\\\\left( {-t}\\\\right), t \\\\in \\\\mathbb{T},\\\\operatorname{Im}F\\\\left( 0\\\\right) = 0\\n\\\\]\\n\\nis an even function if \\\\( \\\\mu = 1 \\\\) and an odd function if \\\\( \\\\mu = - 1 \\\\) . In the case of unit circle \\\\( \\\\mathbb{T} \\\\) the Schwartz kernel is the function \\\\( \\\\frac{\\\\tau + z}{\\\\tau - z} \\\\) . Then\\n\\n\\\\[ \\n\\\\frac{1}{2\\\\pi }{\\\\int }_{-\\\\pi }^{\\\\pi }h\\\\left( {-\\\\tau }\\\\right) \\\\frac{\\\\tau + z}{\\\\tau - z}{d\\\\sigma } = \\\\frac{1}{2\\\\pi i}{\\\\int }_{\\\\left| \\\\tau \\\\right| = 1}\\\\frac{h\\\\left( {-\\\\tau }\\\\right) }{\\\\tau }\\\\frac{\\\\tau + z}{\\\\tau - z}{d\\\\tau }\\n\\\\]\\n\\n\\\\[ \\n= \\\\frac{1}{2\\\\pi i}{\\\\int }_{\\\\left| \\\\tau \\\\right| = 1}\\\\frac{h\\\\left( \\\\tau \\\\right) }{-\\\\tau } \\\\cdot \\\\frac{-\\\\tau + z}{-\\\\tau - z}\\\\left( {-{d\\\\tau }}\\\\right) = \\\\frac{1}{2\\\\pi }{\\\\int }_{-\\\\pi }^{\\\\pi }h\\\\left( \\\\tau \\\\right) \\\\frac{\\\\tau - z}{\\\\tau + z}{d\\\\sigma }.\\n\\\\]\\n\\nTherefore, under \\\\( \\\\mu = 1 \\\\) ,\\n\\n\\\\[ \\nF\\\\left( z\\\\right) = \\\\frac{1}{2\\\\pi }{\\\\int }_{-\\\\pi }^{\\\\pi }h\\\\left( \\\\tau \\\\right) \\\\frac{\\\\tau + z}{\\\\tau - z}{d\\\\sigma } + \\\\frac{1}{2\\\\pi }{\\\\int }_{-\\\\pi }^{\\\\pi }h\\\\left( {-\\\\tau }\\\\right) \\\\frac{\\\\tau + z}{\\\\tau - z}{d\\\\sigma }\\n\\\\]\\n\\n\\\\[ \\n= \\\\frac{1}{2\\\\pi }{\\\\int }_{-\\\\pi }^{\\\\pi }h\\\\left( \\\\tau \\\\right) \\\\left\\\\lbrack {\\\\frac{\\\\tau + z}{\\\\tau - z} + \\\\frac{\\\\tau - z}{\\\\tau + z}}\\\\right\\\\rbrack {d\\\\sigma } = \\\\frac{1}{\\\\pi }{\\\\int }_{-\\\\pi }^{\\\\pi }h\\\\left( \\\\tau \\\\right) \\\\frac{{\\\\tau }^{2} + {z}^{2}}{{\\\\tau }^{2} - {z}^{2}}{d\\\\sigma }\\n\\\\]\\n\\ni.e. \\\\( F\\\\left( z\\\\right) \\\\) is an even function. For \\\\( \\\\mu = - 1 \\\\) we have\\n\\n\\\\[ \\nF\\\\left( z\\\\right) = \\\\frac{1}{2\\\\pi }{\\\\int }_{-\\\\pi }^{\\\\pi }h\\\\left( \\\\tau \\\\right) \\\\left\\\\lbrack {\\\\frac{\\\\tau + z}{\\\\tau - z} - \\\\frac{\\\\tau - z}{\\\\tau + z}}\\\\right\\\\rbrack {d\\\\sigma } = \\\\frac{2}{\\\\pi }{\\\\int }_{-\\\\pi }^{\\\\pi }h\\\\left( \\\\tau \\\\right) \\\\frac{\\\\tau z}{{\\\\tau }^{2} - {z}^{2}}{d\\\\sigma }\\n\\\\]\\n\\ni.e. \\\\( F\\\\left( z\\\\right) \\\\) is an odd function.",
        "id": "college_math_276155"
    },
    {
        "informal_statement": "Die Kommutatorgruppe \\( {G}^{\\prime } \\) einer Gruppe \\( G \\) ist ein Normalteiler von \\( G \\) .",
        "informal_proof": "Beweis: Es seien \\( x \\in G \\) und \\( k \\in {G}^{\\prime } \\) . Dann gilt \\( k = {k}_{1}\\cdots {k}_{r} \\) mit Kommutatoren \\( {k}_{i} = \\left\\lbrack {{a}_{i},{b}_{i}}\\right\\rbrack ,{a}_{i},{b}_{i} \\in G \\) für \\( i = 1,\\ldots, r \\) . Wegen\\n\\n\\[ \\nx{k}_{i}{x}^{-1} = x\\left\\lbrack {{a}_{i},{b}_{i}}\\right\\rbrack {x}^{-1} = \\left\\lbrack {x{a}_{i}{x}^{-1}, x{b}_{i}{x}^{-1}}\\right\\rbrack \\n\\]\\n\\nund\\n\\n\\[ \\n{xk}{x}^{-1} = x{k}_{1}\\cdots {k}_{r}{x}^{-1} = x{k}_{1}{x}^{-1}x{k}_{2}{x}^{-1}\\cdots x{k}_{r}{x}^{-1} \\n\\]\\n\\nist \\( {xk}{x}^{-1} \\) ein Produkt von Kommutatoren, d. h. \\( x{G}^{\\prime }{x}^{-1} \\subseteq {G}^{\\prime } \\) . Die Behauptung folgt nun aus Lemma 4.1.",
        "id": "college_math_269463"
    },
    {
        "informal_statement": "\\[ r\\left( \\mathbf{A}\\right) = 2 \\] \\[ r\\left( {\\mathbf{A} : \\mathbf{b}}\\right) = 2 \\] Hence, it is a consistent system with \\( r = 2 < n = 3 \\) .",
        "informal_proof": "Thus, the given system has infinite number of solutions.",
        "id": "college_math_327573"
    },
    {
        "informal_statement": "Theorem 2.1 (Zariski-Samuel) As locally ringed spaces, \\( \\operatorname{Zar}\\left( {F/k}\\right) \\) is the projective limit of the projective models of \\( F/k \\) .",
        "informal_proof": "The proof of the theorem shows that each valuation ring \\( V \\) of \\( F/k \\) is the union of the local rings that are the centers of \\( V \\) in the projective models of \\( F/k \\) . This elementary observation is not hard to show directly. In some contexts it is possible to considerably restrict the types of projective models needed to obtain the valuation rings in this way. For example, if \\( k \\) is a two-dimensional regular local ring with quotient field \\( F \\), then iterated blow-ups of closed points (quadratic transformations) suffice; see Sect. 7.",
        "id": "college_math_243019"
    },
    {
        "informal_statement": "Theorem 6.12 (Darboux’s Theorem) Let \\( f \\) be a differentiable function on an open interval \\( I \\) . Pick points \\( s < t \\) in \\( I \\) and suppose that \\( {f}^{\\prime }\\left( s\\right) < \\rho < {f}^{\\prime }\\left( t\\right) \\) . Then there is a point \\( u \\) between \\( s \\) and \\( t \\) such that \\( {f}^{\\prime }\\left( u\\right) = \\rho \\) .",
        "informal_proof": "Proof: Consider the function \\( g\\left( x\\right) = f\\left( x\\right) - {\\rho x} \\) . Then \\( {g}^{\\prime }\\left( s\\right) < 0 \\) and \\( {g}^{\\prime }\\left( t\\right) > 0 \\) . Assume for simplicity that \\( s < t \\) . The sign of the derivative at \\( s \\) shows that \\( g\\left( \\widehat{s}\\right) < g\\left( s\\right) \\) for \\( \\widehat{s} \\) greater than \\( s \\) and near \\( s \\) . The sign of the derivative at \\( t \\) implies that \\( g\\left( \\widehat{t}\\right) < g\\left( t\\right) \\) for \\( \\widehat{t} \\) less than \\( t \\) and near \\( t \\) . Thus the minimum of the continuous function \\( g \\) on the compact interval \\( \\left\\lbrack {s, t}\\right\\rbrack \\) must occur at some point \\( u \\) in the interior \\( \\left( {s, t}\\right) \\) . The preceding proposition guarantees that \\( {g}^{\\prime }\\left( u\\right) = 0 \\), or \\( {f}^{\\prime }\\left( u\\right) = \\rho \\) as claimed.",
        "id": "college_math_265059"
    },
    {
        "informal_statement": "Theorem 16.6.4 (Casorati Weierstrass) Let a be an isolated singularity and suppose for some \\( r > 0, f\\left( {{B}^{\\prime }\\left( {a, r}\\right) }\\right) \\) is not dense in \\( \\mathbb{C} \\) . Then either \\( a \\) is a removable singularity or there exist finitely many \\( {b}_{1},\\cdots ,{b}_{M} \\) for some finite number, \\( M \\) such that for \\( z \\) near \\( a \\) ,\\n\\n\\[ f\\left( z\\right) = g\\left( z\\right) + \\mathop{\\sum }\\limits_{{k = 1}}^{M}\\frac{{b}_{k}}{{\\left( z - a\\right) }^{k}} \\tag{16.11} \\]\\n\\nwhere \\( g\\left( z\\right) \\) is analytic near a.",
        "informal_proof": "Proof: Suppose \\( B\\left( {{z}_{0},\\delta }\\right) \\) has no points of \\( f\\left( {{B}^{\\prime }\\left( {a, r}\\right) }\\right) \\) . Such a ball must exist if \\( f\\left( {{B}^{\\prime }\\left( {a, r}\\right) }\\right) \\) is not dense. Then for \\( z \\in {B}^{\\prime }\\left( {a, r}\\right) ,\\left| {f\\left( z\\right) - {z}_{0}}\\right| \\geq \\delta > 0 \\) . It follows from Theorem 16.6.3 that \\( \\frac{1}{f\\left( z\\right) - {z}_{0}} \\) has a removable singularity at \\( a \\) . Hence, there exists \\( h \\) an analytic function such that for \\( z \\) near \\( a \\) ,\\n\\n\\[ h\\left( z\\right) = \\frac{1}{f\\left( z\\right) - {z}_{0}}. \\tag{16.12} \\]\\n\\nThere are two cases. First suppose \\( h\\left( a\\right) = 0 \\) . Then \\( \\mathop{\\sum }\\limits_{{k = 1}}^{\\infty }{a}_{k}{\\left( z - a\\right) }^{k} = \\frac{1}{f\\left( z\\right) - {z}_{0}} \\) for \\( z \\) near \\( a \\) . If all the \\( {a}_{k} = 0 \\), this would be a contradiction because then the left side would equal zero for \\( z \\) near \\( a \\) but the right side could not equal zero. Therefore, there is a first \\( m \\) such that \\( {a}_{m} \\neq 0 \\) . Hence there exists an analytic function, \\( k\\left( z\\right) \\) which is not equal to zero in some ball, \\( B\\left( {a,\\varepsilon }\\right) \\) such that\\n\\n\\[ k\\left( z\\right) {\\left( z - a\\right) }^{m} = \\frac{1}{f\\left( z\\right) - {z}_{0}}. \\]\\n\\nHence, taking both sides to the -1 power,\\n\\n\\[ f\\left( z\\right) - {z}_{0} = \\frac{1}{{\\left( z - a\\right) }^{m}}\\mathop{\\sum }\\limits_{{k = 0}}^{\\infty }{b}_{k}{\\left( z - a\\right) }^{k} \\]\\n\\nand so 16.11 holds.\\n\\nThe other case is that \\( h\\left( a\\right) \\neq 0 \\) . In this case, raise both sides of 16.12 to the -1 power and obtain\\n\\n\\[ f\\left( z\\right) - {z}_{0} = h{\\left( z\\right) }^{-1}, \\]\\n\\na function analytic near \\( a \\) . Therefore, the singularity is removable. -",
        "id": "college_math_155329"
    },
    {
        "informal_statement": "Proposition 3.185. Let \\( f \\in {\\mathcal{P}}^{\\prime } \\) and \\( \\varphi \\in \\mathcal{P} \\) . Then\\n\\n\\[ \\n\\langle f * \\varphi ,\\psi \\rangle = \\left\\langle {f{ * }_{1}\\varphi ,\\psi }\\right\\rangle \\;\\forall \\psi \\in \\mathcal{P}. \\tag{3.79} \\n\\]",
        "informal_proof": "Proof. Equation (3.79) means that \\( f * \\varphi \\) and \\( f{ * }_{1}\\varphi \\) coincide as periodic distributions. To establish this fact, it suffices to show that they have the same Fourier transform. We have\\n\\n\\[ \\n{\\left( f{ * }_{1}\\varphi \\right) }^{ \\land }\\left( k\\right) = \\frac{1}{2\\pi }\\left\\langle {f{ * }_{1}\\varphi ,{\\Theta }_{-k}}\\right\\rangle = \\frac{1}{2\\pi }\\left\\langle {f,\\widetilde{\\varphi } * {\\Theta }_{-k}}\\right\\rangle .\\n\\]\\n\\nBut\\n\\n\\[ \\n\\left( {\\widetilde{\\varphi } * {\\Theta }_{-k}}\\right) \\left( x\\right) = \\frac{1}{2\\pi }{\\int }_{-\\pi }^{\\pi }\\widetilde{\\varphi }\\left( y\\right) {\\Theta }_{-k}\\left( {x - y}\\right) {dy}\\n\\]\\n\\n\\[ \\n= \\frac{1}{2\\pi }{\\int }_{-\\pi }^{\\pi }\\varphi \\left( {-y}\\right) \\exp \\left( {-{ikx}}\\right) \\exp \\left( {iky}\\right) {dy}\\n\\]\\n\\n\\[ \\n= \\frac{\\exp \\left( {-{ikx}}\\right) }{2\\pi }{\\int }_{-\\pi }^{\\pi }\\varphi \\left( {-y}\\right) \\exp \\left( {iky}\\right) {dy} = {\\Theta }_{-k}\\left( x\\right) \\widehat{\\varphi }\\left( k\\right) ,\\n\\]\\n\\nso\\n\\n\\[ \\n\\frac{1}{2\\pi }\\left\\langle {f,\\widetilde{\\varphi } * {\\Theta }_{-k}}\\right\\rangle = \\frac{1}{2\\pi }\\left\\langle {f,{\\Theta }_{-k}}\\right\\rangle \\widehat{\\varphi }\\left( k\\right) = \\widehat{f}\\left( k\\right) \\widehat{\\varphi }\\left( k\\right) .\\n\\]\\n\\nIn view of Proposition 3.184, we obtain\\n\\n\\[ \\n{\\left( f{ * }_{1}\\varphi \\right) }^{ \\land }\\left( k\\right) = \\widehat{f}\\left( k\\right) \\widehat{\\varphi }\\left( k\\right) = {\\left( f * \\varphi \\right) }^{ \\land }\\left( k\\right)\\n\\]\\n\\nand the proof is complete.",
        "id": "college_math_207416"
    },
    {
        "informal_statement": "Corollary 18.34. Let \\( 1 \\leq p < \\infty \\) and let \\( G \\) be a nonempty open set in \\( {\\mathbb{R}}^{N}, N \\geq 1 \\) . Then \\( {C}_{0}^{\\infty }\\left( G\\right) \\) is dense in \\( {L}_{p}\\left( G\\right) \\) .",
        "informal_proof": "## Proof.\\n\\n(I) Let \\( G \\) be bounded and \\( H \\subset \\subset G \\) . Let \\( u \\in {L}_{p}\\left( G\\right) \\) . We set\\n\\n\\[ v\\left( x\\right) = \\left\\{ \\begin{array}{ll} u\\left( x\\right) & \\text{ on }H, \\\\ 0 & \\text{ on }G - H. \\end{array}\\right. \\]\\n\\nThen\\n\\n\\[ {\\int }_{G}{\\left| u - v\\right| }^{p}{dx} = {\\int }_{G - H}{\\left| u\\right| }^{p}{dx} \\]\\n\\nBy the absolute continuity of integrals \\( {A}_{2}\\left( {20}\\right) \\), the right integral is arbitrarily small provided the measure of \\( G - H \\) is sufficiently small. Thus,\\n\\nwe can choose the set \\( H \\) in such a way that\\n\\n\\[ \\parallel u - v{\\parallel }_{p} < \\varepsilon \\]\\n\\nBy Proposition 18.33(b),(c) there is a \\( w \\in {C}_{0}^{\\infty }\\left( G\\right) \\) with \\( \\parallel v - w{\\parallel }_{p} < \\varepsilon \\) . Hence \\( \\parallel u - w{\\parallel }_{p} < {2\\varepsilon } \\]\\n\\n(II) Let \\( G \\) be unbounded. Then, for each \\( \\varepsilon > 0 \\), there exists a sufficiently large open ball \\( B \\) such that\\n\\n\\[ {\\int }_{G - H}{\\left| u\\right| }^{p}{dx} < \\varepsilon \\]\\n\\nwhere \\( H = G \\cap B \\) . Now we can use the same argument as in (I).\\n\\nWe now consider the derivatives of \\( {u}_{\\varepsilon } \\) . In order to collect all the important properties of the smoothing operator in one place, we also investigate the case that \\( u \\) has generalized derivatives. The definition of \\( {D}^{\\alpha }u \\) can be found in Section 21.1.",
        "id": "college_math_235763"
    },
    {
        "informal_statement": "Theorem 31 (Nash & Tognoli) Any smooth compact manifold is diffeomorphic to a smooth real algebraic submanifold of some \\( {\\mathbb{E}}^{n} \\), i.e. to the set of solutions of a system of polynomial equations.",
        "informal_proof": "For a modern proof see chapter 14 of Bochnak, Coste, & Coste-Roy 1998 [209].",
        "id": "college_math_286427"
    },
    {
        "informal_statement": "Theorem 4.8 (Persistence under overflowing invariance) Let \\( k \\geq 2 \\) , \\( \\alpha \\in \\left\\lbrack {0,1}\\right\\rbrack \\) and \\( r = k + \\alpha \\) . Let \\( \\left( {X, g}\\right) \\) be a smooth, complete, connected Riemannian manifold of bounded geometry and \\( Y \\) a Banach space. Let \\( {v}_{\\delta } \\in {C}_{b, u}^{k,\\alpha } \\) be a family of vector fields defined on a uniformly sized neighborhood of the zero-section in \\( X \\times Y \\) such that \\( {\\begin{Vmatrix}{v}_{\\delta } - {v}_{0}\\end{Vmatrix}}_{1} \\leq \\delta \\) . Let \\( M \\) satisfy Definition 4.4 for the pair \\( \\left( {{v}_{0},{v}_{\\delta }}\\right) \\) for any \\( \\delta \\in \\left( {0,{\\delta }_{0}}\\right\\rbrack \\) and let \\( M \\) be \\( r \\) -normally attracting for the flow defined by \\( {v}_{0} \\), that is, \\( M \\) satisfies the overflowing invariant version of Definition 1.11 with \\( \\operatorname{rank}\\left( {E}^{ + }\\right) = 0 \\) .\\n\\nThen for each sufficiently small \\( \\eta > 0 \\) there exist \\( {\\delta }_{1} > 0 \\) such that for any \\( \\delta \\in \\left( {0,{\\delta }_{1}}\\right\\rbrack \\), there is a unique manifold with boundary \\( \\widetilde{M} = \\operatorname{Graph}\\left( \\widetilde{h}\\right) ,\\widetilde{h} : M \\rightarrow Y \\) , \\( \\parallel \\widetilde{h}{\\parallel }_{0} \\leq \\eta \\) such that \\( \\widetilde{M} \\) is negatively invariant under the flow defined by \\( {v}_{\\delta } \\) . Moreover, \\( \\widetilde{h} \\in {C}_{b, u}^{k,\\alpha } \\) and \\( \\parallel \\widetilde{h}{\\parallel }_{k - 1} \\) can be made arbitrary small by choosing \\( {\\begin{Vmatrix}{v}_{\\delta } - {v}_{0}\\end{Vmatrix}}_{k - 1} \\) sufficiently small. The function \\( h \\) extends continuously to \\( \\partial M \\) .",
        "informal_proof": "Proof The proof of Theorem 3.2 requires minimal changes. Note that regardless of the modifications and smoothing preparations performed in Sect. 3.4, the vector field \\( {\\widetilde{v}}_{X} \\) is precisely the horizontal component of the perturbed vector field \\( {v}_{\\delta } \\) . In Sect. 3.6 where we proved existence and uniqueness of \\( \\widetilde{M} \\), we take \\( \\eta \\) small enough that it satisfies condition (2) of Definition 4.4. This guarantees that \\( x = {T}_{X}\\left( {y,{x}_{0}}\\right) \\) is a solution curve such that \\( x(\\left( {-\\infty ,0\\rbrack \\rbrack \\subset \\bar{M}\\text{for any}y \\in {B}_{\\eta }^{\\rho }\\left( {I;Y}\\right) \\text{and}{x}_{0} \\in \\bar{M}}\\right) \\) . Hence, the contraction mapping \\( T = {T}_{Y} \\circ \\left( {{T}_{X},{\\mathrm{{pr}}}_{1}}\\right) \\) is well-defined with intermediate space \\( {\\mathcal{B}}_{\\beta }\\left( {I;\\bar{M}}\\right) \\) and we find a unique Lipschitz continuous fixed point map \\( {\\Theta }^{\\infty } \\) : \\( \\bar{M} \\rightarrow {B}_{\\eta }^{\\rho }\\left( {I;Y}\\right) \\) .\\n\\nNo essential changes are needed with respect to the smoothness proof i",
        "id": "college_math_368340"
    },
    {
        "informal_statement": "Example 8.3. Let \\( a = 7 \\) and \\( b = {10} \\) in an affine cipher with \\( C \\equiv {aP} + b\\left( {\\;\\operatorname{mod}\\;{26}}\\right) \\), so that \\( C \\equiv {7P} + {10}\\left( {\\;\\operatorname{mod}\\;{26}}\\right) \\). Note that \\( P \\equiv {15}\\left( {C - {10}}\\right) \\equiv {15C} + 6\\left( {\\;\\operatorname{mod}\\;{26}}\\right) \\), because 15 is an inverse of 7 modulo 26. The correspondence between letters is given in Table 8.3.",
        "informal_proof": "To illustrate how we obtained this correspondence, note that the plaintext letter L with numerical equivalent 11 corresponds to the ciphertext letter \\( \\mathrm{J} \\), because \\( 7 \\cdot {11} + {10} = \\) \\( {87} \\equiv 9\\left( {\\;\\operatorname{mod}\\;{26}}\\right) \\) and 9 is the numerical equivalent of \\( \\mathrm{J} \\).",
        "id": "college_math_357618"
    },
    {
        "informal_statement": "Example 1.1 Consider the following second-order ODE \\( {y}^{\\prime \\prime } = 3{y}^{\\prime } - {2y} \\) with initial conditions \\( y\\left( 0\\right) = {0e}{y}^{\\prime }\\left( 0\\right) = 1 \\).",
        "informal_proof": "One can verify that the particular solution is given by \\( y\\left( x\\right) = - {e}^{x} + {e}^{2x} \\)",
        "id": "college_math_252311"
    },
    {
        "informal_statement": "Theorem 7.2.\\n\\nThe matrix equations (7.12)-(7.13) are completely solvable and with \\( N \\) is Hurwitz if and only if the following two conditions hold [32]\\n\\nCondition 1:\\n\\n\\[ \\operatorname{rank}\\left\\lbrack \\begin{matrix} F{A}_{u} & F \\\\ \\bar{C}{A}_{u} & \\bar{C} \\\\ \\bar{C} & 0 \\\\ 0 & \\bar{E} \\\\ F & 0 \\end{matrix}\\right\\rbrack = \\operatorname{rank}\\left\\lbrack \\begin{matrix} \\bar{C}{A}_{u} & \\bar{C} \\\\ \\bar{C} & 0 \\\\ 0 & \\bar{E} \\\\ F & 0 \\end{matrix}\\right\\rbrack \\tag{7.17} \\]\\n\\nCondition 2:\\n\\n\\[ \\operatorname{rank}\\left\\lbrack \\begin{matrix} {sF} - F{A}_{u} & - F \\\\ \\bar{C}{A}_{u} & \\bar{C} \\\\ \\bar{C} & 0 \\\\ 0 & \\bar{E} \\end{matrix}\\right\\rbrack = \\operatorname{rank}\\left\\lbrack \\begin{matrix} \\bar{C}{A}_{u} & \\bar{C} \\\\ \\bar{C} & 0 \\\\ 0 & \\bar{E} \\\\ F & 0 \\end{matrix}\\right\\rbrack \\forall s \\in \\mathbf{C},\\Re \\left( s\\right) \\geq 0 \\tag{7.18} \\]\\n\\nwhere \\( {A}_{u} = \\left\\lbrack \\begin{matrix} \\bar{A} \\\\ {0}_{l \\times \\left( {n + l}\\right) } \\end{matrix}\\right\\rbrack \\in {\\mathbb{R}}^{\\left( {n + l}\\right) \\times \\left( {n + l}\\right) } \\).",
        "informal_proof": "Proof: By letting \\( {A}_{u} = \\left\\lbrack \\begin{matrix} \\bar{A} \\\\ {0}_{l \\times \\left( {n + l}\\right) } \\end{matrix}\\right\\rbrack \\) and since \\( \\bar{E} = \\left\\lbrack \\begin{array}{ll} {I}_{n} & {0}_{n \\times l} \\end{array}\\right\\rbrack \\), matrix \\( \\bar{A} \\) can now be expressed as\\n\\n\\[ \\bar{A} = \\bar{E}{A}_{u} \\tag{7.19} \\]\\n\\nBy substituting (7.13), i.e., \\( L\\bar{E} = F - E\\bar{C} \\) and (7.19) into (7.12) one obtains\\n\\n\\[ {NF} = F{A}_{u} - \\left\\lbrack \\begin{array}{ll} E & T \\end{array}\\right\\rbrack \\Theta \\tag{7.20} \\]\\n\\nwhere \\( T = \\left( {J - {NE}}\\right) \\) and \\( \\Theta = \\left\\lbrack \\begin{matrix} \\bar{C}{A}_{u} \\\\ \\bar{C} \\end{matrix}\\right\\rbrack \\).\\n\\nSince \\( F \\) is a known matrix, let one define the following full-row rank matrix\\n\\n\\[ \\left\\lbrack \\begin{array}{ll} {H}_{1} & {E}_{1} \\end{array}\\right\\rbrack = \\left\\lbrack \\begin{array}{ll} {F}^{ + } & \\left( {{I}_{n + l} - {F}^{ + }F}\\right) \\end{array}\\right\\rbrack \\tag{7.21} \\]\\n\\nwhere \\( {F}^{ + } \\) is the Moore-Penrose inverse of \\( F \\). Post-multiplying both sides of (7.20) by (7.21) gives\\n\\n\\[ N = F{A}_{u}{H}_{1} - \\left\\lbrack \\begin{array}{ll} E & T \\end{array}\\right\\rbrack \\Theta {H}_{1}, \\tag{7.22} \\]\\n\\n\\[ F{A}_{u}{E}_{1} = \\left\\lbrack \\begin{array}{ll} E & T \\end{array}\\right\\rbrack \\Theta {E}_{1} \\tag{7.23} \\]\\n\\nIn (7.22)-(7.23), \\( F,{A}_{u},\\Theta ,{H}_{1} \\) and \\( {E}_{1} \\) are known matrices. The unknown matrices are \\( E \\) and \\( T \\). Note that the knowledge of these unknown matrices is necessary for the determination of matrix \\( N \\).\\n\\nLet one now augment (7.23) with (7.13) as follows\\n\\n\\[ \\left\\lbrack \\begin{array}{lll} E & T & L \\end{array}\\right\\rbrack \\Delta = \\psi \\tag{7.24} \\]\\n\\nwhere \\( \\Delta \\in {\\mathbb{R}}^{\\left( {{2p} + n}\\right) \\times \\left( {{2n} + {2l}}\\right) } \\) and \\( \\psi \\in {\\mathbb{R}}^{r \\times \\left( {{2n} + {2l}}\\right) } \\) are known matrices as defined by\\n\\n\\[ \\Delta = \\left\\lbrack \\begin{matrix} \\bar{C}{A}_{u}{E}_{1} & \\bar{C} \\\\ \\bar{C}{E}_{1} & 0 \\\\ 0 & \\bar{E} \\end{matrix}\\right\\rbrack \\tag{7.25} \\]\\n\\n\\[ \\psi = \\left\\lbrack \\begin{array}{ll} F{A}_{u}{E}_{1} & F \\end{array}\\right\\rbrack \\tag{7.26} \\]\\n\\nFrom (7.24), one can derive a necessary and sufficient condition for the existence of a solution of the unknown matrices, i.e., \\( E, T \\) and \\( L \\). Then, by substituting the solution (i.e., \\( E \\) and \\( T \\) ) into (7.22), a necessary and sufficient condition for ensuring that matrix \\( N \\) be Hurwitz can be derived. Finally, from \\( T = J - {NE} \\), matrix \\( J \\) is obtained as \\( J = T + {NE} \\). As a result, all the unknown matrices \\( N, L, J, E \\) and \\( H \\) that satisfy the matrix equations (7.12)-(7.14) of the Theorem 7.1 are obtained.",
        "id": "college_math_308260"
    },
    {
        "informal_statement": "Theorem 7.2. The upper Snell envelope \\( {U}^{ \\uparrow } \\) of \\( H \\) is the smallest \\( \\mathcal{P} \\) -supermartingale that dominates \\( H \\) .",
        "informal_proof": "Proof. For each \\( {P}^{ * } \\in \\mathcal{P} \\) the recursive scheme (6.27) implies that \\( {P}^{ * } \\) -a.s.\\n\\n\\[ \\n{U}_{t}^{ \\uparrow } \\geq {H}_{t} \\vee {E}^{ * }\\left\\lbrack {{U}_{t + 1}^{ \\uparrow } \\mid {\\mathcal{F}}_{t}}\\right\\rbrack \\geq {E}^{ * }\\left\\lbrack {{U}_{t + 1}^{ \\uparrow } \\mid {\\mathcal{F}}_{t}}\\right\\rbrack \\n\\]\\n\\nSince \\( {U}_{0}^{ \\uparrow } \\) is a finite constant due to our integrability assumption (7.1), induction on \\( t \\) shows that \\( {U}_{t}^{ \\uparrow } \\) is integrable with respect to each \\( {P}^{ * } \\in \\mathcal{P} \\) and hence is a \\( \\mathcal{P} \\) -super-martingale dominating \\( H \\) .\\n\\nIf \\( \\widetilde{U} \\) is another \\( \\mathcal{P} \\) -supermartingale which dominates \\( H \\), then \\( {\\widetilde{U}}_{T} \\geq {H}_{T} = {U}_{T}^{ \\uparrow } \\) . Moreover, if \\( {\\widetilde{U}}_{t + 1} \\geq {U}_{t + 1}^{ \\uparrow } \\) for some \\( t \\), then\\n\\n\\[ \\n{\\widetilde{U}}_{t} \\geq {H}_{t} \\vee {E}^{ * }\\left\\lbrack {{\\widetilde{U}}_{t + 1} \\mid {\\mathcal{F}}_{t}}\\right\\rbrack \\geq {H}_{t} \\vee {E}^{ * }\\left\\lbrack {{U}_{t + 1}^{ \\uparrow } \\mid {\\mathcal{F}}_{t}}\\right\\rbrack \\n\\]\\n\\nThus,\\n\\n\\[ \\n{\\widetilde{U}}_{t} \\geq {H}_{t} \\vee \\underset{{P}^{ * } \\in \\mathcal{P}}{\\operatorname{ess}\\sup }{E}^{ * }\\left\\lbrack {{U}_{t + 1}^{ \\uparrow } \\mid {\\mathcal{F}}_{t}}\\right\\rbrack = {U}_{t}^{ \\uparrow }, \\n\\]\\n\\nand backward induction shows that \\( \\widetilde{U} \\) dominates \\( {U}^{ \\uparrow } \\) .",
        "id": "college_math_374908"
    },
    {
        "informal_statement": "Theorem 3.3.3. For an inverse system \\( P : I \\rightarrow {ANR} \\) there exists a spectral sequence with the second term\\n\\n\\[ \\n{E}_{2}^{st} = \\mathop{\\lim }\\limits^{s}\\operatorname{pro} - {H}_{-t}\\left( {\\mathbf{P}, A}\\right) \\]\\n\\nconverging to \\( {\\bar{H}}_{-s - t}\\left( {\\mathbf{P}, A}\\right) \\) .",
        "informal_proof": "Proof. See [26, Proposition 5.13]. \\( \\square \\)",
        "id": "college_math_343296"
    },
    {
        "informal_statement": "Proposition 2.4.2 The category of log schemes admits fibered products, and the functor \\( X \\rightarrow \\underline{X} \\) taking a log scheme to its underlying scheme commutes with fibered products. The fibered product of coherent log schemes is coherent.",
        "informal_proof": "Proof: Let \\( \\left\\{ {{\\alpha }_{i} : {M}_{i} \\rightarrow {\\mathcal{O}}_{X} : i \\in I}\\right\\} \\) be an inductive family of prelog structures on \\( X \\) and let \\( M \\) be the inductive limit of the system \\( {M}_{i} \\) in the category of sheaves of monoids on \\( X \\) . Then the maps \\( {\\alpha }_{i} \\) induce a map \\( \\beta : M \\rightarrow {\\mathcal{O}}_{X} \\) , and \\( \\beta \\) is the inductive limit of \\( \\left\\{ {{\\alpha }_{i} : i \\in I}\\right\\} \\) in the category of prelog structures on \\( X \\) . If each \\( {\\alpha }_{i} \\) is in fact a \\( \\log \\) structure, then the \\( \\log \\) structure \\( \\alpha \\mathrel{\\text{:=}} {\\beta }^{a} \\) associated to \\( \\beta \\) is the limit of \\( \\left\\{ {{\\alpha }_{i} : i \\in I}\\right\\} \\) in the category of \\( \\log \\) structures on \\( X \\) . It remains to show that \\( \\alpha \\) is coherent if each \\( {\\alpha }_{i} \\) is coherent and \\( I \\) is finite. It suffices to treat the case of amalgamated sums. Given a pair of maps of coherent log structures\\n\\n![019184d6-6c52-7314-9d7f-670fc1981df6_138_246683.jpg](images/019184d6-6c52-7314-9d7f-670fc1981df6_138_246683.jpg)\\n\\nlet \\( \\alpha \\) be the amalgamated sum in the category of \\( \\log \\) structures. We may assume that \\( {\\alpha }_{0} \\) admits a chart \\( {\\beta }_{0} \\) subordinate to a finitely generated monoid \\( {Q}_{0} \\) . By (2.2.3) we may, after shrinking \\( X \\) if necessary, find coherent charts \\( {\\phi }_{i} : {Q}_{0} \\rightarrow {Q}_{i} \\) for the morphisms \\( {\\theta }_{i} \\) . Let \\( Q \\) be the amalgamated sum \\( {Q}_{1}{ \\oplus }_{{Q}_{0}}{Q}_{2} \\) , with its canonical map \\( \\beta : Q \\rightarrow M \\) . Because the functor \\( \\beta \\mapsto {\\beta }^{a} \\) is a left adjoint, it commutes with inductive limits, and it follows that \\( {\\beta }^{a} \\cong \\alpha \\), in other words, that \\( \\beta \\) is a chart for \\( \\alpha \\) . This proves (1). For (2), it suffices to construct fibered products, and if \\( f : X \\rightarrow Z \\) and \\( g : Y \\rightarrow Z \\) are morphisms of schemes with coherent log structures, then on the fibered product \\( {X}^{\\prime } \\) of underlying schemes we have a pair of morphisms of \\( \\log \\) structures \\( p{r}_{Z}^{ * }{\\alpha }_{Z} \\rightarrow \\) \\( p{r}_{X}^{ * }{\\alpha }_{X} \\) and \\( p{r}_{Z}^{ * }{\\alpha }_{X} \\rightarrow p{r}_{Y}^{ * }{\\alpha }_{Y} \\) . One checks immediately that, if \\( {\\alpha }_{{X}^{\\prime }} \\) is the inductive limit of this family in the category of \\( \\log \\) structures, then \\( \\left( {{X}^{\\prime },{\\alpha }_{{X}^{\\prime }}}\\right) \\) together with the induced maps to \\( X, Y \\), and \\( Z \\), is the fibered product in the category of coherent log schemes.",
        "id": "college_math_252378"
    },
    {
        "informal_statement": "Proposition 14.31. The dual of the spectral norm is given by\\n\\n$$ \\n\\\\parallel A{\\\\parallel }_{2}^{D} = {\\\\sigma }_{1} + \\\\cdots + {\\\\sigma }_{r}\\n$$ \\n\\nwhere ${\\\\sigma }_{1} > \\\\cdots > {\\\\sigma }_{r} > 0$ are the singular values of $A \\\\in {\\\\mathrm{M}}_{n}\\\\left( \\\\mathbb{C}\\\\right)$ (which has rank $r$ ).",
        "informal_proof": "Proof. In this case the inner product on ${\\\\mathrm{M}}_{n}\\\\left( \\\\mathbb{C}\\\\right)$ is the Frobenius inner product $\\\\langle A, B\\\\rangle =$ $\\\\operatorname{tr}\\\\left( {{B}^{ * }A}\\\\right)$, and the dual norm of the spectral norm is given by\\n\\n$$ \\n\\\\parallel A{\\\\parallel }_{2}^{D} = \\\\sup \\\\left\\\\{ {\\\\left| {\\\\operatorname{tr}\\\\left( {{A}^{ * }B}\\\\right) }\\\\right| \\\\mid \\\\parallel B{\\\\parallel }_{2} = 1}\\\\right\\\\} .\\n$$ \\n\\nIf we factor $A$ using an SVD as $A = {V\\\\sum }{U}^{ * }$, where $U$ and $V$ are unitary and $\\\\sum$ is a diagonal matrix whose $r$ nonzero entries are the singular values ${\\\\sigma }_{1} > \\\\cdots > {\\\\sigma }_{r} > 0$, where $r$ is the rank of $A$, then\\n\\n$$ \\n\\\\left| {\\\\operatorname{tr}\\\\left( {{A}^{ * }B}\\\\right) }\\\\right| = \\\\left| {\\\\operatorname{tr}\\\\left( {{U\\\\sum }{V}^{ * }B}\\\\right) }\\\\right| = \\\\left| {\\\\operatorname{tr}\\\\left( {\\\\sum {V}^{ * }{BU}}\\\\right) }\\\\right|\\n$$ \\n\\nso if we pick $B = V{U}^{ * }$, a unitary matrix such that $\\\\parallel B{\\\\parallel }_{2} = 1$, we get\\n\\n$$ \\n\\\\left| {\\\\operatorname{tr}\\\\left( {{A}^{ * }B}\\\\right) }\\\\right| = \\\\operatorname{tr}\\\\left( \\\\sum \\\\right) = {\\\\sigma }_{1} + \\\\cdots + {\\\\sigma }_{r},\\n$$ \\n\\nand thus\\n\\n$$ \\n\\\\parallel A{\\\\parallel }_{2}^{D} \\\\geq {\\\\sigma }_{1} + \\\\cdots + {\\\\sigma }_{r}\\n$$ \\n\\nSince $\\\\parallel B{\\\\parallel }_{2} = 1$ and $U$ and $V$ are unitary, by Proposition 9.10 we have ${\\\\begin{Vmatrix}{V}^{ * }BU\\\\end{Vmatrix}}_{2} =$ $\\\\parallel B{\\\\parallel }_{2} = 1$ . If $Z = {V}^{ * }{BU}$, by definition of the operator norm\\n\\n$$ \\n1 = \\\\parallel Z{\\\\parallel }_{2} = \\\\sup \\\\left\\\\{ {\\\\parallel {Zx}{\\\\parallel }_{2} \\\\mid \\\\parallel x{\\\\parallel }_{2} = 1}\\\\right\\\\} ,\\n$$ \\n\\nso by picking $x$ to be the canonical vector ${e}_{j}$, we see that ${\\\\begin{Vmatrix}{Z}^{j}\\\\end{Vmatrix}}_{2} \\\\leq 1$ where ${Z}^{j}$ is the $j$ th column of $Z$, so $\\\\left| {z}_{jj}\\\\right| \\\\leq 1$, and since\\n\\n$$ \\n\\\\left| {\\\\operatorname{tr}\\\\left( {\\\\sum {V}^{ * }{BU}}\\\\right) }\\\\right| = \\\\left| {\\\\operatorname{tr}\\\\left( {\\\\sum Z}\\\\right) }\\\\right| = \\\\left| {\\\\mathop{\\\\sum }\\\\limits_{{j = 1}}^{r}{\\\\sigma }_{j}{z}_{jj}}\\\\right| \\\\leq \\\\mathop{\\\\sum }\\\\limits_{{j = 1}}^{r}{\\\\sigma }_{j}\\\\left| {z}_{jj}\\\\right| \\\\leq \\\\mathop{\\\\sum }\\\\limits_{{j = 1}}^{r}{\\\\sigma }_{j},\\n$$ \\n\\nand we conclude that\\n\\n$$ \\n\\\\left| {\\\\operatorname{tr}\\\\left( {\\\\sum {V}^{ * }{BU}}\\\\right) }\\\\right| \\\\leq \\\\mathop{\\\\sum }\\\\limits_{{j = 1}}^{r}{\\\\sigma }_{j}\\n$$ \\n\\nThe above implies that\\n\\n$$ \\n\\\\parallel A{\\\\parallel }_{2}^{D} \\\\leq {\\\\sigma }_{1} + \\\\cdots + {\\\\sigma }_{r}\\n$$ \\n\\nand since we also have $\\\\parallel A{\\\\parallel }_{2}^{D} \\\\geq {\\\\sigma }_{1} + \\\\cdots + {\\\\sigma }_{r}$, we conclude that\\n\\n$$ \\n\\\\parallel A{\\\\parallel }_{2}^{D} = {\\\\sigma }_{1} + \\\\cdots + {\\\\sigma }_{r}\\n$$ \\n\\nproving our proposition.",
        "id": "college_math_13172"
    },
    {
        "informal_statement": "Lemma 3.1.35. Let \\( \\mathbf{A} \\in {\\mathbb{R}}^{\\left( {n + 1}\\right) \\times \\left( {n + 1}\\right) } \\) be a non-singular matrix, \\( n \\in \\mathbb{N} \\) . Then the induced rescaling operator\\n\\n\\[ \\n{\\left. {\\sigma }_{\\mathbf{A}}\\right| }_{{\\mathcal{C}}_{\\infty }\\left( {\\mathbb{R}}^{n + 1}\\right) } : {\\mathcal{C}}_{\\infty }\\left( {\\mathbb{R}}^{n + 1}\\right) \\subseteq {L}_{2}\\left( {\\mathbb{R}}^{n + 1}\\right) \\rightarrow {\\mathcal{C}}_{\\infty }\\left( {\\mathbb{R}}^{n + 1}\\right) \\subseteq {L}_{2}\\left( {\\mathbb{R}}^{n + 1}\\right) \\n\\]\\n\\ndefined by\\n\\n\\[ \\n{\\left. {\\sigma }_{\\mathbf{A}}\\right| }_{{\\overset{ \\circ }{\\mathcal{C}}}_{\\infty }\\left( {\\mathbb{R}}^{n + 1}\\right) }\\phi \\mathrel{\\text{:=}} {\\left| \\det \\left( \\mathbf{A}\\right) \\right| }^{1/2}\\phi \\left( {\\mathbf{A} \\cdot }\\right) \\n\\]\\n\\nfor \\( \\phi \\in {\\mathcal{C}}_{\\infty }^{ \\circ }\\left( {\\mathbb{R}}^{n + 1}\\right) \\) extends to a unitary mapping \\( {\\sigma }_{\\mathbf{A}} \\) in \\( {L}_{2}\\left( {\\mathbb{R}}^{n + 1}\\right) \\) .",
        "informal_proof": "Proof. The result follows by an elementary calculation. Following the standard convention of identifying \\( x = \\left( {{x}_{0},\\ldots ,{x}_{n}}\\right) \\in {\\mathbb{R}}^{n + 1} \\) with the column matrix\\n\\n\\[ \\n\\left( \\begin{matrix} {x}_{0} \\\\ \\vdots \\\\ {x}_{n} \\end{matrix}\\right) \\in {\\mathbb{R}}^{\\left( {n + 1}\\right) \\times 1} \\n\\]\\n\\nwe have\\n\\n\\[ \\n{\\int }_{{\\mathbb{R}}^{n + 1}}{\\left| \\left( {\\sigma }_{\\mathbf{A}}\\phi \\right) \\left( x\\right) \\right| }^{2}{dx} = {\\int }_{{\\mathbb{R}}^{n + 1}}{\\left| \\phi \\left( Ax\\right) \\right| }^{2}\\left| {\\det \\left( \\mathbf{A}\\right) }\\right| {dx} \\n\\]\\n\\n\\[ \\n= {\\int }_{{\\mathbb{R}}^{n + 1}}{\\left| \\phi \\left( y\\right) \\right| }^{2}{dy} \\n\\]\\n\\nfor all \\( \\phi \\in {\\mathcal{C}}_{\\infty }^{ \\circ }\\left( {\\mathbb{R}}^{n + 1}\\right) \\), from which the result follows by taking limits.",
        "id": "college_math_233382"
    },
    {
        "informal_statement": "Theorem 12. Let $L$ be a line, and $M$ a plane, in ${v}_{3}$ . If $L$ is parallel to $M$, then their intersection is either empty or all of $L$ . If L is not parallel to $M$, then their intersection is a single point.",
        "informal_proof": "Proof. Let $L$ have parametric equation $X = P + {tA}$ ; let $M$ have cartesian equation $\\mathrm{N} \\cdot \\mathrm{X} = \\mathrm{b}$ . We wish to determine for what values of $\\mathrm{t}$ the point $x = P + {tA}$ lies on the plane $M$ ; that is, to determine the solutions of the equation\\n\\n$$ \\mathrm{N} \\cdot \\left( {\\mathrm{P} + \\mathrm{{tA}}}\\right) = \\mathrm{b}\\text{.} $$\\n\\nNow if $L$ is parallel to $M$, then the vector $A$ is perpendicular to the normal vector $\\mathrm{N}$ to $\\mathrm{M}$ ; that is, $\\mathrm{N} \\cdot \\mathrm{A} = 0$ . In this case, the equation\\n\\n$$ \\mathrm{N} \\cdot \\left( {\\mathrm{P} + \\mathrm{{tA}}}\\right) = \\mathrm{b} $$\\n\\nholds for all $t$ if it happens that $N \\cdot P = b$, and it holds for no $t$\\n\\nif $\\mathrm{N} \\cdot \\mathrm{P} \\neq \\mathrm{b}$ . Thus the intersection of $\\mathrm{L}$ and $\\mathrm{M}$ is either all of $\\mathrm{L}$, or it is empty.\\n\\nOn the other hand, if $L$ is not parallel to $M$, then $N \\cdot A \\neq 0$ .\\n\\nIn this case the equation can be solved uniquely for t. Thus the intersection of $L$ and $M$ consists of a single point. $\\square$",
        "id": "college_math_80705"
    },
    {
        "informal_statement": "Theorem 3.22. Let \\( F \\) be a distribution function and let \\( {\\xi }_{p} = {F}^{-1}\\left( p\\right) = \\) \\( \\inf \\{ x : F\\left( x\\right) \\geq p\\} \\) . Then\\n\\n1. \\( {\\xi }_{p} = {F}^{-1}\\left( p\\right) \\) is a non-decreasing function of \\( p \\in \\left( {0,1}\\right) \\) .\\n\\n2. \\( {\\xi }_{p} = {F}^{-1}\\left( p\\right) \\) is left-continuous function of \\( p \\in \\left( {0,1}\\right) \\) .\\n\\n3. \\( {F}^{-1}\\left\\lbrack {F\\left( x\\right) }\\right\\rbrack \\leq x \\) for all \\( x \\in \\mathbb{R} \\) .\\n\\n4. \\( F\\left\\lbrack {{F}^{-1}\\left( p\\right) }\\right\\rbrack \\geq p \\) for all \\( p \\in \\left( {0,1}\\right) \\) .\\n\\n5. \\( F\\left( x\\right) \\geq p \\) if and only if \\( x \\geq {F}^{-1}\\left( p\\right) \\) .",
        "informal_proof": "Proof. To prove Part 1 we consider \\( {p}_{1} \\in \\left( {0,1}\\right) \\) and \\( {p}_{2} \\in \\left( {0,1}\\right) \\) such that \\( {p}_{1} < {p}_{2} \\) . Then we have that \\( {\\xi }_{{p}_{1}} = {F}^{-1}\\left( {p}_{1}\\right) = \\inf \\left\\{ {x : F\\left( x\\right) > {p}_{1}}\\right\\} \\) . The key idea to this proof is establishing that \\( {\\xi }_{{p}_{1}} = {F}^{-1}\\left( {p}_{1}\\right) = \\inf \\{ x : F\\left( x\\right) > \\) \\( \\left. {p}_{1}\\right\\} \\leq \\inf \\left\\{ {x : F\\left( x\\right) > {p}_{2}}\\right\\} = {\\xi }_{{p}_{2}} \\) . The reason for this follows from the fact that \\( F \\) is a non-decreasing function. Therefore, the smallest value of \\( x \\) such that \\( F\\left( x\\right) > {p}_{2} \\) must be at least as large as the smallest value of \\( x \\) such that \\( F\\left( x\\right) > {p}_{1} \\) .",
        "id": "college_math_256578"
    },
    {
        "informal_statement": "Assume that \\( H \\subset G \\) is a dense subgroup, then a continuous determinant on \\( G \\) is uniquely determined by its restriction to \\( H \\).",
        "informal_proof": "Indeed, if two such determinants \\( {\\mathrm{D}}_{1} \\) and \\( {\\mathrm{D}}_{2} \\) coincide on \\( \\mathbb{Z}\\left\\lbrack H\\right\\rbrack \\), and if \\( n \\) denotes their common dimension, then for each \\( \\alpha \\in {I}_{n} \\) the continuous maps \\( {\\mathrm{D}}_{1}^{\\left\\lbrack \\alpha \\right\\rbrack },{\\mathrm{D}}_{2}^{\\left\\lbrack \\alpha \\right\\rbrack } : {G}^{n} \\rightarrow A \\) coincide on \\( {H}^{n} \\), hence on the whole of \\( {G}^{n} \\), so \\( {\\mathrm{D}}_{1} = {\\mathrm{D}}_{2} \\).",
        "id": "college_math_366846"
    },
    {
        "informal_statement": "Proposition 25. Given \\( \\mu \\in {ML} \\) and \\( q \\in {\\mathcal{P}}_{\\mu }^{ + } \\cup \\mathcal{F} \\), there exists \\( \\varepsilon > 0 \\), depending on \\( \\mu \\) and \\( q \\), such that if \\( \\left| \\tau \\right| < \\varepsilon \\), then \\( {G}_{\\mu }^{q}\\left( \\tau \\right) \\in \\mathcal{{QF}} \\) and \\( {\\mathbf{D}}_{\\mu }^{q}\\left( \\tau \\right) \\) is a component of \\( \\partial \\mathcal{C}\\left( {{G}_{\\mu }^{q}\\left( \\tau \\right) }\\right) \\).",
        "informal_proof": "Proof. This is proved in [18, Proposition 8.10] for the case in which the basepoint \\( q \\) is in \\( \\mathcal{F} \\) . It is clear that the same proof works in our more general case.",
        "id": "college_math_222661"
    },
    {
        "informal_statement": "Lemma 5.1 (Upcrossing inequality). For all \\( k \\in \\mathbb{N} \\) ,\\n\\n\\[ P\\left( {u\\left( {a, b}\\right) > k}\\right) \\leq {\\left( b - a\\right) }^{-1}E\\left\\lbrack {{\\left( {X}_{\\infty } - a\\right) }^{ - }\\mathbf{1}\\{ u\\left( {a, b}\\right) = k\\} }\\right\\rbrack . \\tag{5.5} \\]",
        "informal_proof": "Proof. Since the process \\( \\left( {{X}_{n} - a}\\right) \\) is also a supermartingale, it suffices to prove the result when \\( a = 0 \\) . Let\\n\\n\\[ \\left( {{\\sigma }_{1},{\\sigma }_{2}}\\right) = \\text{time interval when the}\\left( {k + 1}\\right) \\text{th upcrossing occurs} \\]\\n\\n\\[ = \\left( {{T}_{{2k} + 1},{T}_{{2k} + 2}}\\right) \\]\\n\\n![01918243-0f0a-7956-8d24-1769af05f759_81_915324.jpg](images/01918243-0f0a-7956-8d24-1769af05f759_81_915324.jpg)\\n\\nFig. 5.1 Picture of the \\( \\left( {k + 1}\\right) \\) th upcrossing of the interval \\( \\left( {a, b}\\right) \\) .\\n\\nas shown in Figure 5.1. Note that\\n\\n\\[ \\{ u\\left( {0, b}\\right) > k\\} = \\left\\{ {{\\sigma }_{2} < \\infty }\\right\\} \\subset \\left\\{ {{\\sigma }_{1} < \\infty }\\right\\} \\cap \\left\\{ {{X}_{{\\sigma }_{2}} \\geq b}\\right\\} \\]\\n\\nfrom which it follows that\\n\\n\\[ P\\left( {u\\left( {0, b}\\right) > k}\\right) = E\\left( {\\mathbf{1}\\{ u\\left( {0, b}\\right) > k\\} }\\right) \\leq \\left( {1/b}\\right) E\\left\\lbrack {{X}_{{\\sigma }_{2}}\\mathbf{1}\\{ u\\left( {0, b}\\right) > k\\} }\\right\\rbrack \\]\\n\\n\\[ \\leq \\left( {1/b}\\right) \\mathbb{E}\\left\\lbrack {{X}_{{\\sigma }_{2}}^{ + }\\;\\mathbf{1}\\{ u\\left( {0, b}\\right) > k\\} }\\right\\rbrack \\tag{5.6} \\]\\n\\n\\[ \\leq \\left( {1/b}\\right) \\mathbb{E}\\left\\lbrack {{X}_{{\\sigma }_{2}}^{ + }\\;\\mathbf{1}\\{ {\\sigma }_{1} < \\infty \\} }\\right\\rbrack . \\]\\n\\nSince \\( \\left( {X}_{n}\\right) \\) is a supermartingale and \\( {X}_{{\\sigma }_{1}} \\leq 0 \\) on the event \\( {\\sigma }_{1} < \\infty \\), it follows from the optional stopping theorem that\\n\\n\\[ E\\left\\lbrack {{X}_{{\\sigma }_{2}}^{ + }\\;\\mathbf{1}\\{ {\\sigma }_{1} < \\infty \\} }\\right\\rbrack - E\\left\\lbrack {{X}_{{\\sigma }_{2}}^{ - }\\;\\mathbf{1}\\{ {\\sigma }_{1} < \\infty \\} }\\right\\rbrack \\tag{5.7} \\]\\n\\n\\[ = E\\left\\lbrack {{X}_{{\\sigma }_{2}}\\mathbf{1}\\{ {\\sigma }_{1} < \\infty \\} }\\right\\rbrack \\leq E\\left\\lbrack {{X}_{{\\sigma }_{1}}\\mathbf{1}\\{ {\\sigma }_{1} < \\infty \\} }\\right\\rbrack \\leq 0. \\]\\n\\nCombining (5.6) and (5.7), we deduce that\\n\\n\\[ P\\left( {u\\left( {0, b}\\right) > k}\\right) \\leq \\left( {1/b}\\right) E\\left\\lbrack {{X}_{{\\sigma }_{2}}^{ - }\\mathbf{1}\\left\\{ {{\\sigma }_{1} < \\infty }\\right\\} }\\right\\rbrack \\]\\n\\n\\[ \\leq \\left( {1/b}\\right) E\\left\\lbrack {{X}_{{\\sigma }_{2}}^{ - }\\mathbf{1}\\{ {\\sigma }_{1} < \\infty ,{X}_{{\\sigma }_{2}} \\leq 0\\} }\\right\\rbrack \\]\\n\\n\\[ \\leq \\left( {1/b}\\right) E\\left\\lbrack {{X}_{{\\sigma }_{2}}^{ - }\\mathbf{1}\\{ {\\sigma }_{1} < \\infty ,{\\sigma }_{2} = \\infty \\} }\\right\\rbrack \\]\\n\\n\\[ \\leq \\left( {1/b}\\right) E\\left\\lbrack {{X}_{\\infty }^{ - }\\mathbf{1}\\{ u\\left( {0, b}\\right) = k\\} }\\right\\rbrack \\]\\n\\nwhich completes the proof.",
        "id": "college_math_154984"
    },
    {
        "informal_statement": "Theorem 8.2.3 Suppose, in \\( \\left( {{L}^{2}\\left\\lbrack {a, b}\\right\\rbrack ,{\\mathcal{F}}_{\\mathcal{H}},\\mathcal{T}}\\right) \\), that the following conditions hold:\\n\\n(a) \\( {\\int }_{a}^{t}\\left( {x\\left( s\\right) - y\\left( s\\right) }\\right) {ds} \\leq x\\left( t\\right) - y\\left( t\\right) \\) for all \\( x, y \\in {L}^{2}\\left\\lbrack {a, b}\\right\\rbrack \\) ;\\n\\n(b) \\( T \\) be a linear self-mapping on \\( \\left( {{L}^{2}\\left\\lbrack {a, b}\\right\\rbrack ,{\\mathcal{F}}_{\\phi ,\\varphi },\\mathcal{T}}\\right) \\) defined as follows:\\n\\n\\[ \\left( {Tx}\\right) \\left( t\\right) = f\\left( t\\right) + \\lambda {\\int }_{a}^{t}h\\left( {t, s}\\right) x\\left( s\\right) {ds}, \\]\\n\\nwhere \\( f \\in {L}^{2}\\left\\lbrack {a, b}\\right\\rbrack \\) is a given function, \\( h\\left( {t, s}\\right) \\) is a continuous function defined on \\( a \\leq t \\leq b, a \\leq s \\leq t \\), and \\( \\lambda \\) is a constant;\\n\\n(c) by putting \\( \\mathop{\\max }\\limits_{{a \\leq t \\leq b, a \\leq s \\leq t}}h\\left( {t, s}\\right) = M \\), we have \\( {\\lambda M} \\in \\left( {0,1}\\right) \\) .\\n\\nThen \\( T \\) has a unique fixed point in \\( {L}^{2}\\left\\lbrack {a, b}\\right\\rbrack \\) . Furthermore, for any \\( {x}_{0} \\in {L}^{2}\\left\\lbrack {a, b}\\right\\rbrack \\) , the sequence \\( \\left\\{ {{T}^{n}{x}_{0}}\\right\\} \\) converges to the fixed point in \\( \\left( {{L}^{2}\\left\\lbrack {a, b}\\right\\rbrack ,{\\mathcal{F}}_{\\mathcal{H}},\\mathcal{T}}\\right) \\) .",
        "informal_proof": "Proof Note that\\n\\n\\[ \\langle {Tx} - {Ty}, v\\rangle = {\\int }_{a}^{b}\\left( {{Tx} - {Ty}}\\right) \\left( t\\right) v\\left( t\\right) {dt} \\]\\n\\n\\[ = {\\int }_{a}^{b}\\lambda \\left( {{\\int }_{a}^{t}h\\left( {t, s}\\right) x\\left( s\\right) {ds} - {\\int }_{a}^{t}h\\left( {t, s}\\right) y\\left( s\\right) {ds}}\\right) v\\left( t\\right) {dt} \\]\\n\\n\\[ = \\lambda {\\int }_{a}^{b}\\left( {{\\int }_{a}^{t}h\\left( {t, s}\\right) \\left( {x\\left( s\\right) - y\\left( s\\right) }\\right) {ds}}\\right) v\\left( t\\right) {dt}. \\]\\n\\nBy the mean-value theorem and the continuity of \\( h\\left( {t, s}\\right) \\), there exist \\( {s}_{1},{t}_{1} \\) with \\( a \\leq \\) \\( {t}_{1} \\leq b, a \\leq {s}_{1} \\leq t \\) such that\\n\\n\\[ \\langle {Tx} - {Ty}, v\\rangle \\leq {\\lambda h}\\left( {{t}_{1},{s}_{1}}\\right) {\\int }_{a}^{b}\\left( {{\\int }_{a}^{t}\\left( {x\\left( s\\right) - y\\left( s\\right) }\\right) {ds}}\\right) v\\left( t\\right) {dt} \\]\\n\\n\\[ = {\\lambda M}{\\int }_{a}^{b}\\left( {{\\int }_{a}^{t}\\left( {x\\left( s\\right) - y\\left( s\\right) }\\right) {ds}}\\right) v\\left( t\\right) {dt}. \\]\\n\\nBy (a), we have\\n\\n\\[ \\langle {Tx} - {Ty}, v\\rangle \\leq {\\lambda M}\\left( {{\\int }_{a}^{b}\\left( {x\\left( t\\right) - y\\left( t\\right) }\\right) v\\left( t\\right) {dt}}\\right) = {\\lambda M}\\langle x - y, v\\rangle . \\]\\n\\nHence we have\\n\\n\\[ \\langle {Tx} - {Ty}, v\\rangle \\leq {\\lambda M}\\langle x - y, v\\rangle \\]\\n\\nand\\n\\n\\[ \\mathcal{H}\\left( {t-\\langle {Tx} - {Ty}, v\\rangle }\\right) { \\geq }_{{L}^{ * }}\\mathcal{H}\\left( {t - {\\lambda M}\\langle x - y, v\\rangle }\\right) . \\]\\n\\nAlso, we have\\n\\n\\[ \\mathcal{H}\\left( {t-\\langle {Tx} - {Ty}, v\\rangle }\\right) { \\geq }_{{L}^{ * }}\\mathcal{H}\\left( {\\frac{t}{\\lambda M}-\\langle x - y, v\\rangle }\\right) , \\]\\n\\n\\[ {\\mathcal{F}}_{\\mathcal{H}}\\left( {{Tx} - {Ty}, v, t}\\right) { \\geq }_{{L}^{ * }}{\\mathcal{F}}_{\\mathcal{H}}\\left( {x - y, v,\\frac{t}{\\lambda M}}\\right) . \\]\\n\\nThus, by Theorem 8.2.1, \\( T \\) has a unique fixed point in \\( {L}^{2}\\left\\lbrack {a, b}\\right\\rbrack \\) . Moreover, for each \\( {x}_{0} \\in {L}^{2}\\left\\lbrack {a, b}\\right\\rbrack \\), the sequence \\( \\left\\{ {{T}^{n}{x}_{0}}\\right\\} \\) converges to the fixed point in \\( \\left( {{L}^{2}\\left\\lbrack {a, b}\\right\\rbrack ,{\\mathcal{F}}_{\\mathcal{H}},\\mathcal{T}}\\right) \\) . This completes the proof.",
        "id": "college_math_320341"
    },
    {
        "informal_statement": "Theorem 7.39. If \\( {\\mathcal{D}}_{U} \\) can be defined on \\( {C}^{\\infty }\\left( U\\right) \\) and \\( \\mathfrak{X}\\left( U\\right) \\) for each open \\( U \\subset M \\) so that\\n\\n(1) \\( {\\mathcal{D}}_{U}\\left( {fg}\\right) = \\left( {{\\mathcal{D}}_{U}f}\\right) g + f{\\mathcal{D}}_{U}g \\) for all \\( f, g \\in {C}^{\\infty }\\left( U\\right) \\),\\n\\n(2) \\( {\\left. \\left( {\\mathcal{D}}_{M}f\\right) \\right| }_{U} = {\\left. {\\mathcal{D}}_{U}f\\right| }_{U} \\) for each \\( f \\in {C}^{\\infty }\\left( M\\right) \\),\\n\\n(3) \\( {\\mathcal{D}}_{U}\\left( {fX}\\right) = \\left( {{\\mathcal{D}}_{U}f}\\right) X + f{\\mathcal{D}}_{U}X \\) for all \\( f \\in {C}^{\\infty }\\left( U\\right) \\) and \\( X \\in \\mathfrak{X}\\left( U\\right) \\),\\n\\n(4) \\( {\\left. \\left( {\\mathcal{D}}_{M}X\\right) \\right| }_{U} = {\\left. {\\mathcal{D}}_{U}X\\right| }_{U} \\) for each \\( X \\in \\mathfrak{X}\\left( M\\right) \\),\\n\\nthen there is a unique tensor derivation \\( \\mathcal{D} \\) that is equal to \\( {\\mathcal{D}}_{U} \\) on \\( {C}^{\\infty }\\left( U\\right) \\) and \\( \\mathfrak{X}\\left( U\\right) \\) for all \\( U \\) .",
        "informal_proof": "Sketch of proof. We wish to define \\( \\mathcal{D} \\) on \\( {\\mathfrak{X}}^{ * }\\left( U\\right) \\) so that\\n\\n(7.9)\\n\\n\\[ \\n{\\mathcal{D}}_{U}\\left( {\\alpha \\otimes X}\\right) = {\\mathcal{D}}_{U}\\alpha \\otimes X + \\alpha \\otimes {\\mathcal{D}}_{U}X.\\n\\]\\n\\nBy contraction we see that we must have \\( \\left( {{\\mathcal{D}}_{U}\\alpha }\\right) \\left( X\\right) = {\\mathcal{D}}_{U}\\left( {\\alpha \\left( X\\right) }\\right) - \\alpha \\left( {{\\mathcal{D}}_{U}X}\\right) \\) , which we take as the definition. Then check that (7.9) holds. Now define \\( {\\mathcal{D}}_{U} \\) by formula (7.8) and verify that we really have a map \\( {\\mathcal{T}}_{s}^{r}\\left( U\\right) \\rightarrow {\\mathcal{T}}_{s}^{r}\\left( U\\right) \\) . Check that \\( {\\mathcal{D}}_{U} \\) commutes with contraction \\( C : {\\mathcal{T}}_{1}^{1}\\left( U\\right) \\rightarrow {C}^{\\infty }\\left( U\\right) \\) for simple tensors \\( \\alpha \\otimes X \\in {\\mathcal{T}}_{1}^{1}\\left( U\\right) \\) . Use the fact that, locally, every element of \\( {\\mathcal{T}}_{1}^{1} \\) can be written as a sum of simple tensors. Next extend to \\( {\\mathcal{T}}_{s}^{r} \\) along the lines exemplified by the case of \\( {\\mathcal{T}}_{2}^{1}\\left( U\\right) \\) and the contraction \\( {C}_{2}^{1} \\) as follows: For \\( \\tau \\in {\\mathcal{T}}_{2}^{1}\\left( U\\right) \\), we have\\n\\n\\[ \\n\\left( {{\\mathcal{D}}_{U}{C}_{2}^{1}\\tau }\\right) \\left( X\\right) = {\\mathcal{D}}_{U}\\left( {\\left( {{C}_{2}^{1}\\tau }\\right) \\left( X\\right) }\\right) - \\left( {{C}_{2}^{1}\\tau }\\right) {\\mathcal{D}}_{U}X\\n\\]\\n\\n\\[ \\n= {\\mathcal{D}}_{U}\\left( {C\\left( {\\tau \\left( {\\cdot, X, \\cdot }\\right) }\\right) }\\right) - C\\left( {\\tau \\left( {\\cdot ,{\\mathcal{D}}_{U}X, \\cdot }\\right) }\\right)\\n\\]\\n\\n\\[ \\n= C{\\mathcal{D}}_{U}\\left( {\\tau \\left( {\\cdot, X, \\cdot }\\right) - \\tau \\left( {\\cdot ,{\\mathcal{D}}_{U}X, \\cdot }\\right) }\\right)\\n\\]\\n\\n\\[ \\n= C\\left( {\\left( {{\\mathcal{D}}_{U}\\tau }\\right) \\left( {\\cdot, X, \\cdot }\\right) }\\right) = \\left( {{C}_{2}^{1}{\\mathcal{D}}_{U}\\tau }\\right) \\left( X\\right) .\\n\\]\\n\\nThe general case would involve an inconvenient profusion of parentheses. Uniqueness follows from Exercise 7.38. Finally check by direct calculation that (3) of Definition 7.36 holds.",
        "id": "college_math_336657"
    },
    {
        "informal_statement": "Lemma 2.5.7 Let \\( \\mathcal{E} \\) be a regular Mal’cev category. Consider the commutative squares of diagram 2.28, with the same bottom morphism. Suppose that the morphisms \\( f, g \\) and \\( {g}^{\\prime } \\) are regular epimorphisms and that the vertical morphisms of the left hand square are split epimorphisms: \\( p \\circ s = {\\mathrm{{id}}}_{X}, q \\circ t = {\\mathrm{{id}}}_{Y} \\). In those conditions, the factorization \\[ g{ \\times }_{f}{g}^{\\prime } : A{ \\times }_{X}{A}^{\\prime } \\rightarrow B{ \\times }_{Y}{B}^{\\prime } \\] through the pullbacks is a regular epimorphism as well.",
        "informal_proof": "Proof We shall pull \\( q \\) back along both composites of the second square and analyze the situations obtained in this way.\\n\\nFirst, we consider diagram 2.29, where both bottom squares are pullbacks: The rectangle with curved arrows and the bottom left square are pullbacks, from which a factorization \\( g{ \\times }_{f}{A}^{\\prime } \\) which makes the upper square a pullback as well. But the morphism \\( \\left( {g, p}\\right) \\) is a regular epimorphism by 2.5.6; thus the factorization \\( g{ \\times }_{f}{A}^{\\prime } \\) is a regular epimorphism, by A.5.1.\\n\\nNext we consider diagram 2.30 where the right hand square is a pullback and the outer rectangle is the pullback of the beginning of this proof. As a consequence, we obtain a factorization \\( B{ \\times }_{Y}{g}^{\\prime } \\) which makes the left hand square a pullback as well. Since \\( {g}^{\\prime } \\) is a regular epimorphism, so is this factorization \\( B{ \\times }_{Y}{g}^{\\prime } \\), by A.5.1.\\n\\nThe conclusion follows at once by A.5.4, since \\[ g{ \\times }_{f}{g}^{\\prime } = \\left( {B{ \\times }_{Y}{g}^{\\prime }}\\right) \\circ \\left( {g{ \\times }_{f}{A}^{\\prime }}\\right) \\] is the composite of two regular epimorphisms.",
        "id": "college_math_228683"
    },
    {
        "informal_statement": "Theorem 13.5.5. The quiver (left quiver) of a semiperfect semiprime two-sided Noetherian hereditary ring \\( A \\) is a disconnected union of points and cycles.",
        "informal_proof": "Proof. Suppose that there exist arrows going from vertex 1 to two different vertices \\( i \\) and \\( j \\) . Then \\( {P}_{1}R \\) contains as a direct summand a module \\( N \\) which is isomorphic to \\( {P}_{i} \\oplus {P}_{j} \\) . Fix monomorphisms \\( \\varphi : {P}_{i} \\rightarrow {P}_{1},\\psi : {P}_{j} \\rightarrow {P}_{1} \\) and write \\( {Im}\\left( {\\varphi \\oplus \\psi }\\right) = N. \\n\\nBecause the ring \\( A \\) is semiprime, the sets \\( \\operatorname{Hom}\\left( {{P}_{1},{P}_{i}}\\right) \\) and \\( \\operatorname{Hom}\\left( {{P}_{1},{P}_{j}}\\right) \\) are both different from zero. Obviously, the sets \\( \\varphi \\operatorname{Hom}\\left( {{P}_{1},{P}_{i}}\\right) \\) and \\( \\psi \\operatorname{Hom}\\left( {{P}_{1},{P}_{j}}\\right) \\) are right ideals in the ring \\( {End}{P}_{1} \\) which are not contained one in another. This contradicts proposition 10.2.7. By propositions 5.5.7 and 11.2.9 one can assume that the ring \\( A \\) is reduced. We are going to show that there does not exist more than one arrow going to each vertex. Consider the vertex with number \\( k \\) . Suppose that there exist two arrows going to the vertex \\( k \\) from different vertices \\( {j}_{1} \\) and \\( {j}_{2} \\) . Then by the \\( Q \\) -Lemma we have strict inclusions:\\n\\n\\[ \\n{e}_{{j}_{1}}{R}^{2}{E}_{k} \\subset {e}_{{j}_{1}}R{e}_{k},\\;{e}_{{j}_{2}}{R}^{2}{e}_{k} \\subset {e}_{{j}_{2}}R{e}_{k}. \\n\\] \\n\\nSet \\( {Q}_{k} = A{e}_{k} \\), where \\( {e}_{k} \\) is an idempotent corresponding to the principal module \\( {P}_{k} \\) . By the \\( Q \\) -lemma, the simple modules \\( {V}_{{j}_{1}} \\) and \\( {V}_{{j}_{2}} \\) are contained in the quotient module \\( R{Q}_{k}/{R}^{2}{Q}_{k} \\) . Therefore \\( R{Q}_{k} = {Q}_{{j}_{k}} \\oplus {Q}_{{j}_{2}} \\oplus X \\) . This again contradicts the fact that \\( {End}{Q}_{k} \\) is a discrete valuation ring. Now the theorem follows from lemma 13.5.4.",
        "id": "college_math_201719"
    },
    {
        "informal_statement": "Theorem 7.2.1. (Euler's Theorem) Let \\( a \\) and \\( n \\) be relatively prime positive integers. Then \\( {a}^{\\varphi \\left( n\\right) } \\equiv 1\\left( {\\;\\operatorname{mod}\\;n}\\right) \\) .",
        "informal_proof": "Proof. Consider the set \\( S = \\left\\{ {{a}_{1},{a}_{2},\\ldots ,{a}_{\\varphi \\left( n\\right) }}\\right\\} \\) consisting of all positive integer less than \\( n \\) that are relatively prime to \\( n \\) . Because \\( \\gcd \\left( {a, n}\\right) = 1 \\), it follows that \\( a{a}_{1}, a{a}_{2},\\ldots, a{a}_{\\varphi \\left( n\\right) } \\) is a permutation of \\( {a}_{1},{a}_{2},\\ldots ,{a}_{\\varphi \\left( n\\right) } \\) . Then\\n\\n\\[ \\left( {a{a}_{1}}\\right) \\left( {a{a}_{2}}\\right) \\ldots \\left( {a{a}_{\\varphi \\left( n\\right) }}\\right) \\equiv {a}_{1}{a}_{2}\\ldots {a}_{\\varphi \\left( n\\right) }\\;\\left( {\\;\\operatorname{mod}\\;n}\\right) . \\]\\n\\nUsing that \\( \\gcd \\left( {{a}_{k}, n}\\right) = 1, k = 1,2,\\ldots ,\\varphi \\left( n\\right) \\), the conclusion now follows.",
        "id": "college_math_241782"
    },
    {
        "informal_statement": "Proposition 24.40. Let $\\\\left( {A, * }\\\\right)$ be a separable algebra over $k$ with an involution of the second kind. Let $K$ be the centre of $A$, and assume that $k = \\\\{ a \\\\in K \\\\mid$ $\\\\left. {{a}^{ * } = a}\\\\right\\\\}$ .\\n\\n(a) The centre $K$ of $\\\\left( {A, * }\\\\right)$ is an étale $k$ -algebra of degree two.\\n\\n(b) Let $L$ be an extension of $k$ such that $K{ \\\\otimes }_{k}L = {K}_{1} \\\\times {K}_{2}$, and let ${A}_{1} =$ $A{ \\\\otimes }_{K}{K}_{1}$ . Then ${A}_{1}$ is a central simple algebra over ${K}_{1}$, and\\n\\n$$ a \\\\otimes c \\\\mapsto \\\\left( {a \\\\otimes c,{a}^{ * } \\\\otimes c}\\\\right) : \\\\left( {A{ \\\\otimes }_{k}L, * }\\\\right) \\\\simeq \\\\left( {{A}_{1} \\\\times {A}_{1}^{\\\\text{opp }},\\\\varepsilon }\\\\right) $$\\n\\nis an isomorphism of algebras over $k$ with involution.",
        "informal_proof": "Proof. (a) The $k$ -algebra $K$ is étale because $A$ is separable. That its degree over $k$ is at most 2 is a standard result in Galois theory when $K$ is a field, and otherwise we can apply A.62.\\n\\n(b) The algebra ${A}_{1}$ over ${K}_{1}$ is semisimple, and hence a product of simple ${K}_{1}$ -algebras (24.18). But its centre is ${K}_{1}$, and so ${A}_{1}$ itself must be simple. The map is the composite of the canonical isomorphisms\\n\\n$$ A{ \\\\otimes }_{k}L \\\\simeq A{ \\\\otimes }_{K}\\\\left( {K{ \\\\otimes }_{k}L}\\\\right) = A{ \\\\otimes }_{K}\\\\left( {{K}_{1} \\\\times {K}_{2}}\\\\right) $$\\n\\n$$ \\\\simeq \\\\left( {A{ \\\\otimes }_{K}{K}_{1}}\\\\right) \\\\times \\\\left( {A{ \\\\otimes }_{K}{K}_{2}}\\\\right) \\\\simeq {A}_{1} \\\\times {A}_{1}^{\\\\text{opp }}. $$",
        "id": "college_math_8251"
    },
    {
        "informal_statement": "Proposition 6.5. A pseudo-Kähler submanifold of an indefinite complex space form \\( {M}_{s}^{m}\\left( {4c}\\right) \\) has constant holomorphic sectional curvature \\( {4c} \\) if and only if \\( N \\) is null-isotropic.",
        "informal_proof": "Proof. Let \\( N \\) be a pseudo-Kähler submanifold of an indefinite complex space form \\( {M}_{s}^{m}\\left( c\\right) \\) . For any unit vector \\( v \\in {TN} \\), it follows from Gauss’ equation and Proposition 6.3 that the holomorphic sectional curvature of \\( N \\) satisfies\\n\\n\\[ H\\left( v\\right) = {4c} - 2\\langle \\sigma \\left( {v, v}\\right) ,\\sigma \\left( {v, v}\\right) \\rangle . \\tag{6.32} \\]\\n\\nTherefore \\( N \\) has constant holomorphic sectional curvature \\( {4c} \\) if and only if \\( \\langle \\sigma \\left( {v, v}\\right) ,\\sigma \\left( {v, v}\\right) \\rangle = 0 \\) for any unit vector \\( v \\) .",
        "id": "college_math_260780"
    },
    {
        "informal_statement": "Proposition 9.43 Let $X$ be an $m$ -dimensional geometric Poincaré complex which admits a normal invariant.\\n\\n(i) The normal structure set $\\mathcal{T}\\left( X\\right)$ is in natural bijective correspondence with the set of normal bordism classes of degree 1 normal maps $\\left( {f, b}\\right) : M \\rightarrow X$ for varying reductions $\\eta : X \\rightarrow {BO}$ of the Spivak normal fibration ${\\nu }_{X} : X \\rightarrow {BG}$ .\\n\\n(ii) The normal structure set $\\mathcal{T}\\left( X\\right)$ is in unnatural bijective correspondence with the set $\\left\\lbrack {X, G/O}\\right\\rbrack$ of fibre homotopy trivialised stable vector bundles over $X$ .",
        "informal_proof": "Proof (i) Immediate from the transversality construction of a normal map $\\left( {f, b}\\right) : M \\rightarrow X$ from a normal invariant $\\left( {\\eta ,\\rho }\\right)$, with $M = {\\rho }^{-1}\\left( X\\right)$ .\\n\\n(ii) An element $\\left( {\\alpha ,\\beta }\\right) \\in \\left\\lbrack {X, G/O}\\right\\rbrack$ is a vector bundle $\\alpha : X \\rightarrow {BO}\\left( j\\right)$ ( $j$ large) together with a fibre homotopy trivialisation $\\beta : {J\\alpha } \\simeq \\{ * \\} : X \\rightarrow {BG}\\left( j\\right)$ . Given a normal invariant $\\left( {\\eta : X \\rightarrow {BO}\\left( k\\right) ,\\rho : {S}^{m + k} \\rightarrow T\\left( \\eta \\right) }\\right)$ define a normal invariant $\\left( {{\\eta }^{\\prime },{\\rho }^{\\prime }}\\right)$ by\\n\\n$${\\eta }^{\\prime } = \\eta \\oplus \\alpha : X \\rightarrow {BO}\\left( {j + k}\\right) ,$$\\n\\n$${\\rho }^{\\prime } : {S}^{m + j + k}\\overset{{\\sum }^{j}\\rho }{ \\rightarrow }{\\sum }^{j}T\\left( \\eta \\right) = T\\left( {\\eta \\oplus {\\epsilon }^{j}}\\right) \\overset{1 \\oplus T{\\left( \\beta \\right) }^{-1}}{ \\rightarrow }T\\left( {\\eta }^{\\prime }\\right) .$$\\n\\nThe construction defines a bijection\\n\\n$${\\iota }_{\\eta ,\\rho } : \\left\\lbrack {X, G/O}\\right\\rbrack \\rightarrow \\mathcal{T}\\left( X\\right) ;\\left( {\\alpha ,\\beta }\\right) \\mapsto \\left( {{\\eta }^{\\prime },{\\rho }^{\\prime }}\\right) .$$\\n\\nThe bijection is unnatural in that it depends on the choice of normal invariant $\\left( {\\eta ,\\rho }\\right)$ .",
        "id": "college_math_153419"
    },
    {
        "informal_statement": "Proposition 2.7 The following are equivalent.\\n\\n(1) The flat dimension \\( {\\mathrm{{fd}}}_{\\Lambda }{E}^{i} \\leq i \\) for all \\( i \\geq 0 \\).\\n\\n(2) Every \\( N \\in {\\;\\operatorname{mod}\\;{\\Lambda }^{\\text{op }}} \\) satisfies the condition (Au).",
        "informal_proof": "Proof (1) \\( \\Rightarrow \\) (2): Note that \\( {M}^{ * } = 0 \\) if and only if \\( {\\operatorname{Hom}}_{\\Lambda }\\left( {M,{E}^{0}}\\right) = 0 \\) for \\( M \\in \\) \\( {\\mathrm{{mod}}\\Lambda }{\\mathrm{{by}}{Lemma}}{2.4}. \\) Let \\( i > 0. \\) Since \\( {\\mathrm{{Hom}}}_{\\Lambda }\\left( {{\\mathrm{{Ext}}}_{{\\Lambda }^{\\mathrm{{op}}}}^{i}\\left( {N,\\Lambda }\\right) ,{E}^{j}}\\right) \\cong {\\mathrm{{Tor}}}_{i}^{\\Lambda }\\left( {N,{E}^{j}}\\right) = \\) 0 for \\( i > j \\), we have \\( {\\operatorname{Hom}}_{\\Lambda }\\left( {{N}^{\\prime },{E}^{j}}\\right) = 0 \\) for all \\( \\Lambda \\) -submodules \\( {N}^{\\prime } \\subset {\\operatorname{Ext}}_{{\\Lambda }^{\\mathrm{{op}}}}^{i}\\left( {N,\\Lambda }\\right) \\) . Hence \\( {\\operatorname{Ext}}_{\\Lambda }^{j}\\left( {{N}^{\\prime },\\Lambda }\\right) = 0 \\) for \\( j < i \\).\\n\\n\\( \\left( 2\\right) \\Rightarrow \\left( 1\\right) \\) : Let \\( f : X \\rightarrow Y \\) be a monomorphism in \\( {\\;\\operatorname{mod}\\;{\\Lambda }^{\\text{op }}} \\) . Since \\( \\operatorname{Cok}{f}^{ * } \\) is a \\( \\Lambda \\) -submodule of \\( {\\operatorname{Ext}}_{{\\Lambda }^{\\text{op }}}^{1}\\left( {\\operatorname{Cok}f,\\Lambda }\\right) \\), we have \\( {\\left( \\operatorname{Cok}{f}^{ * }\\right) }^{ * } = 0 \\) by assumption. Hence \\( {f}^{* * } : {X}^{* * } \\rightarrow {Y}^{* * } \\) is a monomorphism, so that \\( {E}^{0} \\) is flat by Lemma 2.4. Let \\( p > 0 \\) and assume that \\( {\\operatorname{fd}}_{\\Lambda }{E}^{i} \\leq i \\) for \\( i < p \\) . Suppose that there exists \\( N \\in {\\;\\operatorname{mod}\\;{\\Lambda }^{\\text{op }}} \\) such that \\( {\\operatorname{Tor}}_{p + 1}^{\\Lambda }\\left( {N,{E}^{p}}\\right) \\neq 0 \\) . Then we have \\( {\\operatorname{Hom}}_{\\Lambda }\\left( {{\\operatorname{Ext}}_{{\\Lambda }^{\\text{op }}}^{p + 1}\\left( {N,\\Lambda }\\right) ,{E}^{p}}\\right) \\neq 0 \\), so that there exists \\( M \\subset {\\operatorname{Ext}}_{{\\Lambda }^{\\mathrm{{op}}}}^{p + 1}\\left( {M,\\Lambda }\\right) \\) with \\( {\\operatorname{Hom}}_{\\Lambda }\\left( {M,{\\Omega }^{-p}\\Lambda }\\right) \\neq 0 \\) . Since \\( {\\operatorname{fd}}_{\\Lambda }{E}^{p - 1} \\leq p - 1 \\), we have \\( {\\operatorname{Hom}}_{\\Lambda }\\left( {M,{E}^{p - 1}}\\right) = 0 \\) by the proof \\( \\left( 1\\right) \\Rightarrow \\left( 2\\right) \\) . Therefore, \\( {\\operatorname{Hom}}_{\\Lambda }\\left( {M,{\\Omega }^{-p}\\Lambda }\\right) = \\) \\( {\\operatorname{Ext}}_{\\Lambda }^{p}\\left( {M,\\Lambda }\\right) = 0 \\) by assumption. This is a contradiction.",
        "id": "college_math_252212"
    },
    {
        "informal_statement": "Lemma 10.2.4 The collections\\n\\n$$ \\n\\\\left\\\\{ {{\\\\widetilde{\\\\exp }}_{u}^{-1} : {\\\\mathcal{U}}_{u} \\\\rightarrow {W}^{k, p}\\\\left( {{u}^{ * }{TM}}\\\\right) \\\\mid u \\\\in {C}^{\\\\infty }\\\\left( {\\\\sum, M}\\\\right) }\\\\right\\\\} \\n$$\\n\\nform a ${C}^{\\\\infty }$ atlas of ${\\\\mathcal{F}}^{k, p}$ .",
        "informal_proof": "Proof We need to prove that the transition map\\n\\n$$ \\n{\\\\widetilde{\\\\exp }}_{{u}_{1}}^{-1} \\\\circ {\\\\widetilde{\\\\exp }}_{{u}_{2}} : {\\\\widetilde{\\\\exp }}_{{u}_{2}}^{-1}\\\\left( {{\\\\mathcal{U}}_{{u}_{2}} \\\\cap {\\\\mathcal{U}}_{{u}_{1}}}\\\\right) \\\\rightarrow {\\\\widetilde{\\\\exp }}_{{u}_{1}}^{-1}\\\\left( {{\\\\mathcal{U}}_{{u}_{2}} \\\\cap {\\\\mathcal{U}}_{{u}_{1}}}\\\\right) \\n$$\\n\\n is ${C}^{\\\\ell }$ for all $\\\\ell \\\\in \\\\mathbb{N}$ for any choice of ${u}_{1},{u}_{2} \\\\in {C}^{\\\\infty }\\\\left( {\\\\sum, M}\\\\right)$ .\\n\\n![images/352.jpg?x=295&y=193&w=723&h=550](images/352.jpg?x=295&y=193&w=723&h=550)\\n\\nFigure 10.1 Exponential chart.\\n\\nFrom the definition of $\\\\widetilde{\\\\exp }$, we have\\n\\n$$ \\n{\\\\widetilde{\\\\exp }}_{{u}_{1}}^{-1} \\\\circ {\\\\widetilde{\\\\exp }}_{{u}_{2}}\\\\left( \\\\xi \\\\right) \\\\left( z\\\\right) = {\\\\exp }_{{u}_{1}\\\\left( z\\\\right) }^{-1} \\\\circ {\\\\exp }_{{u}_{2}\\\\left( z\\\\right) }\\\\left( {\\\\xi \\\\left( z\\\\right) }\\\\right) .\\n$$\\n\\nNote that, since we have chosen ${u}_{1},{u}_{2}$ to be smooth, the map $z \\\\rightarrow {\\\\widetilde{\\\\exp }}_{{u}_{1}}^{-1} \\\\circ$ ${\\\\widetilde{\\\\exp }}_{{u}_{2}}\\\\left( {\\\\xi \\\\left( z\\\\right) }\\\\right)$ lies in ${W}^{k, p}$ if and only if $\\\\xi$ is in ${W}^{k, p}$ and hence defines a ${W}^{k, p}$ section of ${u}_{1}^{ * }{TM}$ . In addition, it again follows from the smoothness of ${u}_{1},{u}_{2}$ that the assignment $\\\\xi \\\\rightarrow {\\\\widetilde{\\\\exp }}_{{u}_{1}}^{-1} \\\\circ {\\\\widetilde{\\\\exp }}_{{u}_{2}}$ defines a map\\n\\n$$ \\n{\\\\mathcal{V}}_{{u}_{1}} \\\\cap {\\\\widetilde{\\\\exp }}_{{u}_{1}}^{-1} \\\\circ {\\\\widetilde{\\\\exp }}_{{u}_{2}}\\\\left( {\\\\mathcal{V}}_{2}\\\\right) \\\\rightarrow {\\\\mathcal{V}}_{{u}_{2}} \\\\cap {\\\\widetilde{\\\\exp }}_{{u}_{2}}^{-1} \\\\circ {\\\\widetilde{\\\\exp }}_{{u}_{1}}\\\\left( {\\\\mathcal{V}}_{1}\\\\right) \\\\tag{10.2.4} \\n$$\\n\\nthat is differentiable infinitely many times. By changing the role of ${u}_{1}$ and ${u}_{2}$, we prove that the inverse map ${\\\\widetilde{\\\\exp }}_{{u}_{2}}^{-1} \\\\circ {\\\\widetilde{\\\\exp }}_{{u}_{1}}$ is also differentiable infinitely many times. Hence we have shown that the transition maps are ${C}^{\\\\infty }$ . See Figure 10.1.\\n\\nThis finishes the proof.",
        "id": "college_math_125852"
    },
    {
        "informal_statement": "Assume that a rod of length \\( {4m} \\) is heated so that its temperature at time \\( t = 0 \\) is given by\\n\\n\\[ f\\left( x\\right) = \\left\\{ \\begin{array}{ll} 0 & \\text{ for }0 \\leq x < 1 \\\\ {100} & \\text{ for }1 \\leq x < 3 \\\\ 0 & \\text{ for }3 \\leq x \\leq 4 \\end{array}\\right. \\]\\n\\nThe rod is insulated around the sides, and for all \\( t > 0 \\) the temperature is kept at constant values \\( {T}_{1} = {80} \\) at the left end and \\( {T}_{2} = {20} \\) at the right end. Find the temperature in the rod for all \\( t > 0 \\) . Assume that \\( a = \\frac{\\bar{K}}{s\\rho } = {0.1} \\) .",
        "informal_proof": "Solution. The steady state temperature to which the temperature in the rod converges over time is \\( \\sigma \\left( x\\right) = {T}_{1} + \\frac{{T}_{2} - {T}_{1}}{4}x = {80} - {15x} \\) ; therefore,\\n\\n\\[ u\\left( {x, t}\\right) = {80} - {15x} + \\mathop{\\sum }\\limits_{{n = 1}}^{\\infty }{b}_{n}\\sin \\left( \\frac{n\\pi x}{4}\\right) {e}^{-\\frac{{n}^{2}{\\pi }^{2}{0.1t}}{16}}, \\]\\n\\nwhere\\n\\n\\[ {b}_{n} = \\frac{2}{4}{\\int }_{0}^{4}\\left( {f\\left( x\\right) - \\sigma \\left( x\\right) }\\right) \\sin \\left( \\frac{n\\pi x}{4}\\right) {dx}, n = 1,2,\\ldots \\]\\n\\nFigure 8.3 shows the temperature \\( u\\left( {x, t}\\right) \\) in the rod at times \\( t = 0,{0.5},2,5 \\), and 30 .",
        "id": "college_math_251065"
    },
    {
        "informal_statement": "Corollary 2. Let $P \\geq 0$ be a positive semi-definite matrix. Then $\\det P \\geq 0$ .",
        "informal_proof": "Proof. Since $P$ is symmetric, we may write $P = {SD}{S}^{-1}$ where $D$ is a diagonal matrix, having the eigenvalues of $P$ as diagonal entries. By assumption, these eigenvalues are nonnegative, and the determinant of $P$, equal to the determinant of $D$, is the product of these nonnegative eigenvalues.",
        "id": "college_math_152"
    },
    {
        "informal_statement": "Problem 5.1 Let $\\\\mathbf{f} = \\\\left( {{f}_{1},\\\\ldots ,{f}_{m}}\\\\right)$ be a differentiable vector function in the $k$ real variables ${x}_{1},\\\\ldots$,\\n\\n${x}_{k}$, and assume that these again are differentiable functions in the $n$ variables ${u}_{1},\\\\ldots ,{u}_{n}$ .\\n\\nFind the (partial) derivatives of $\\\\left( {{f}_{1},\\\\ldots ,{f}_{m}}\\\\right)$ after ${u}_{1},\\\\ldots ,{u}_{n}$ .",
        "informal_proof": "## Procedure.\\n\\n1) Sketch the general diagram as in figure 1, and reduce the the $i$ -th f-coordinate and the $j$ -th $\\\\mathbf{u}$ -coordinate as shown on figure 2.\\n\\n2) \\",
        "id": "college_math_38260"
    },
    {
        "informal_statement": "Example 3 Let $S = \\\\left\\\\{ {{s}_{0},{s}_{1},{s}_{2}}\\\\right\\\\}$ and $I = \\\\{ a, b, d\\\\}$ . Consider the finite-state machine $M =$ $\\\\left( {S, I,\\\\mathcal{F}}\\\\right)$ defined by the digraph shown in Figure 42. Compute the functions ${f}_{bad}$ , ${f}_{add}$, and ${f}_{\\\\text{badadd }}$, and verify that\\n\\n$$ \\n{f}_{add} \\\\circ {f}_{bad} = {f}_{badadd} \\n$$",
        "informal_proof": "## Solution\\n\\n${f}_{\\\\text{bad }}$ is computed by the following sequence of transitions:\\n\\n$$ \\n{s}_{0}\\\\overset{b}{ \\\\rightarrow }{s}_{0}\\\\overset{a}{ \\\\rightarrow }{s}_{0}\\\\overset{d}{ \\\\rightarrow }{s}_{1} \\n$$ \\n\\n$$ \\n{s}_{1}\\\\overset{b}{ \\\\rightarrow }{s}_{1}\\\\overset{a}{ \\\\rightarrow }{s}_{2}\\\\overset{d}{ \\\\rightarrow }{s}_{1} \\n$$ \\n\\n$$ \\n{s}_{2}\\\\overset{b}{ \\\\rightarrow }{s}_{1}\\\\overset{a}{ \\\\rightarrow }{s}_{2}\\\\overset{d}{ \\\\rightarrow }{s}_{1} \\n$$ \\n\\nThus ${f}_{\\\\text{bad }}\\\\left( {s}_{0}\\\\right) = {s}_{1},{f}_{\\\\text{bad }}\\\\left( {s}_{1}\\\\right) = {s}_{1}$, and ${f}_{\\\\text{bad }}\\\\left( {s}_{2}\\\\right) = {s}_{1}$ .\\n\\nSimilarly, for ${f}_{add}$ ,\\n\\n$$ \\n{s}_{0}\\\\overset{a}{ \\\\rightarrow }{s}_{0}\\\\overset{d}{ \\\\rightarrow }{s}_{1}\\\\overset{d}{ \\\\rightarrow }{s}_{0} \\n$$ \\n\\n$$ \\n{s}_{1}\\\\overset{a}{ \\\\rightarrow }{s}_{2}\\\\overset{d}{ \\\\rightarrow }{s}_{1}\\\\overset{d}{ \\\\rightarrow }{s}_{0} \\n$$ \\n\\n$$ \\n{s}_{2}\\\\overset{a}{ \\\\rightarrow }{s}_{2}\\\\overset{d}{ \\\\rightarrow }{s}_{1}\\\\overset{d}{ \\\\rightarrow }{s}_{0} \\n$$ \\n\\nso ${f}_{add}\\\\left( {s}_{i}\\\\right) = {s}_{0}$ for $i = 0,1,2$ . A similar computation shows that\\n\\n$$ \\n{f}_{\\\\text{badadd }}\\\\left( {s}_{0}\\\\right) = {s}_{0},\\\\;{f}_{\\\\text{badadd }}\\\\left( {s}_{1}\\\\right) = {s}_{0},\\\\;{f}_{\\\\text{badadd }}\\\\left( {s}_{2}\\\\right) = {s}_{0} \\n$$ \\n\\nand the same results hold for ${f}_{add} \\\\circ {f}_{bad}$ . In fact,\\n\\n$$ \\n\\\\left( {{f}_{add} \\\\circ {f}_{bad}}\\\\right) \\\\left( {s}_{0}\\\\right) = {f}_{add}\\\\left( {{f}_{bad}\\\\left( {s}_{0}\\\\right) }\\\\right) = {f}_{add}\\\\left( {s}_{1}\\\\right) = {s}_{0} \\n$$ \\n\\n$$ \\n\\\\left( {{f}_{add} \\\\circ {f}_{bad}}\\\\right) \\\\left( {s}_{1}\\\\right) = {f}_{add}\\\\left( {{f}_{bad}\\\\left( {s}_{1}\\\\right) }\\\\right) = {f}_{add}\\\\left( {s}_{1}\\\\right) = {s}_{0} \\n$$ \\n\\n$$ \\n\\\\left( {{f}_{add} \\\\circ {f}_{bad}}\\\\right) \\\\left( {s}_{2}\\\\right) = {f}_{add}\\\\left( {{f}_{bad}\\\\left( {s}_{2}\\\\right) }\\\\right) = {f}_{add}\\\\left( {s}_{1}\\\\right) = {s}_{0}. \\n$$",
        "id": "college_math_39461"
    },
    {
        "informal_statement": "Corollary 8.3.1 (Continuous Mapping Theorem) Let $\\\\left\\\\{ {{X}_{n}, n \\\\geq 0}\\\\right\\\\}$ be a sequence of random variables such that\\n\\n$${X}_{n} \\\\Rightarrow {X}_{0}.\\\\text{.}$$\\n\\nFor $n \\\\geq 0$, assume ${F}_{n}$ is the distribution function of ${X}_{n}$ . Let $h : \\\\mathbb{R} \\\\mapsto \\\\mathbb{R}$ satisfy\\n\\n$$P\\\\left\\\\lbrack {{X}_{0} \\\\in \\\\operatorname{Disc}\\\\left( h\\\\right) }\\\\right\\\\rbrack = 0.$$\\n\\nThen\\n\\n$$h\\\\left( {X}_{n}\\\\right) \\\\Rightarrow h\\\\left( {X}_{0}\\\\right) ,$$\\n\\nand if $h$ is bounded, dominated convergence implies\\n\\n$${Eh}\\\\left( {X}_{n}\\\\right) = \\\\int h\\\\left( x\\\\right) {F}_{n}\\\\left( {dx}\\\\right) \\\\rightarrow {Eh}\\\\left( x\\\\right) = \\\\int h\\\\left( x\\\\right) {F}_{0}\\\\left( {dx}\\\\right) .$$\\n\\nRemark. Disc $\\\\left( h\\\\right)$ is always measurable even if $h$ is not.",
        "informal_proof": "Proof. The proof of the corollary uses the Baby Skorohod Theorem which identifies new random variables ${X}_{n}^{\\\\# } = {X}_{n}, n \\\\geq 0$, with ${X}_{n}^{\\\\# }$ defined on $\\\\left\\\\lbrack {0,1}\\\\right\\\\rbrack$ . Also ${X}_{n}^{\\\\# }\\\\left( t\\\\right) \\\\rightarrow {X}_{0}^{\\\\# }\\\\left( t\\\\right)$ for a.a. $t$ . If ${X}_{0}^{\\\\# }\\\\left( t\\\\right) \\\\in \\\\mathcal{C}\\\\left( h\\\\right)$, then $h\\\\left( {{X}_{n}^{\\\\# }\\\\left( t\\\\right) }\\\\right) \\\\rightarrow h\\\\left( {{X}_{0}^{\\\\# }\\\\left( t\\\\right) }\\\\right)$ . Thus,\\n\\n$$\\\\lambda \\\\left\\\\{ {t \\\\in \\\\left\\\\lbrack {0,1}\\\\right\\\\rbrack : h\\\\left( {{X}_{n}^{\\\\# }\\\\left( t\\\\right) }\\\\right) \\\\rightarrow h\\\\left( {{X}_{0}^{\\\\# }\\\\left( t\\\\right) }\\\\right) }\\\\right\\\\}$\\n\\n$$\\\\geq \\\\lambda \\\\{ t \\\\in \\\\left\\\\lbrack {0,1}\\\\right\\\\rbrack : {X}_{0}^{\\\\# }\\\\left( t\\\\right) \\\\in {\\\\left( \\\\mathrm{{Disc}}\\\\left( h\\\\right) \\\\right) }^{c}\\\\}$\\n\\n$$= P\\\\left( {\\\\left\\\\lbrack {X}_{0} \\\\in \\\\operatorname{Disc}\\\\left( h\\\\right) \\\\right\\\\rbrack }^{c}\\\\right) = 1.$$\\n\\nSo $h\\\\left( {X}_{n}^{\\\\# }\\\\right) \\\\rightarrow h\\\\left( {X}_{0}^{\\\\# }\\\\right)$ almost surely with respect to $\\\\lambda$, and since almost sure convergence implies convergence in distribution, we have\\n\\n$$h\\\\left( {X}_{n}\\\\right) \\\\overset{d}{ = }h\\\\left( {X}_{n}^{\\\\# }\\\\right) \\\\Rightarrow h\\\\left( {X}_{0}^{\\\\# }\\\\right) \\\\overset{d}{ = }h\\\\left( {X}_{0}\\\\right)$\\n\\nso that $h\\\\left( {X}_{n}\\\\right) \\\\Rightarrow h\\\\left( {X}_{0}\\\\right)$ .",
        "id": "college_math_31444"
    },
    {
        "informal_statement": "The variety ${OG}\\left( {1, n}\\right)$ is a smooth quadric hypersurface $Q$ in ${\\mathbb{P}}^{n - 1}$ . The Schubert classes are\\n\\n(1) Isotropic linear spaces $\\mathbb{P}{L}_{j}$ for $0 \\leq j < \\frac{n}{2}$ ,\\n\\n(2) If $n$ is even, isotropic linear spaces $\\mathbb{P}{L}_{\\frac{n}{2}}$ and $\\mathbb{P}{L}_{\\frac{n}{2}}^{\\prime }$ ,\\n\\n(3) The quadric sections $Q \\cap \\mathbb{P}{Q}_{d}^{n - d}$ for $n \\geq d > \\frac{n}{2} + 1$ .",
        "informal_proof": "The linear spaces are smooth and their classes are rigid. When $d < n$, the quadric sections $Q \\cap \\mathbb{P}{Q}_{d}^{n - d}$ are singular with the singular locus isomorphic to ${\\mathbb{P}}^{n - d - 1}$ . The cohomology class of the Schubert variety $Q \\cap \\mathbb{P}{Q}_{d}^{n - d}$ is the same as the cohomology class of any linear section $Q \\cap \\mathbb{P}{Q}_{d}^{r}$ with $r \\leq n - \\ddot{d}$ . The singular locus of this variety is isomorphic to ${\\mathbb{P}}^{r - 1}$ . Hence, this variety is not isomorphic to a Schubert variety if $r < n - d$ . Therefore, these classes are not rigid. In particular, since $Q \\cap \\mathbb{P}{Q}_{d}^{0}$ is a smooth quadric, every Schubert class in ${OG}\\left( {1, n}\\right)$ can be represented by a smooth subvariety of ${OG}\\left( {1, n}\\right)$ . The classes of the linear spaces $\\mathbb{P}{L}_{j}$, for $1 < j \\leq \\frac{n - 1}{2}$ , are rigid but not multi rigid. For example, twice the class of $\\mathbb{P}{L}_{j}$ can be represented by a smooth quadric of the same dimension. If ${2k} = n$, the classes of the Schubert varieties $\\mathbb{P}{L}_{k}$ and $\\mathbb{P}{L}_{k}^{\\prime }$ are multi rigid [Ho1].",
        "id": "college_math_112839"
    },
    {
        "informal_statement": "Explore the convergence of the Fourier series \\( s\\\\left( t\\\\right) \\) associated with \\( f\\\\left( t\\\\right) \\) and decompose \\( f\\\\left( t\\\\right) \\) into Fourier series.",
        "informal_proof": "Convergence. The function \\( f\\\\left( t\\\\right) \\) satisfies Dirichlet’s conditions. It is absolutely integrable and bounded on interval \\( \\\\langle 0,2\\\\rangle \\) and it is continuous on interval \\( \\\\langle 0,2\\\\rangle \\) except the point \\( d = 1 \\), where the point of jump discontinuity is placed (Fig. 5.1). The Fourier series \\( s\\\\left( t\\\\right) \\) associated with \\( f\\\\left( t\\\\right) \\) converges (see Fig. 5.3) and\\n\\n\\( s\\\\left( {t}_{0}\\\\right) = f\\\\left( {t}_{0}\\\\right) \\) in points \\( {t}_{0},0 < {t}_{0} < 1 \\) or \\( 1 < {t}_{0} < 2, f\\\\left( t\\\\right) \\) is continuous here;\\n\\n\\( s\\\\left( 1\\\\right) = \\\\frac{1}{2}\\\\left( {\\\\mathop{\\\\lim }\\\\limits_{{t \\\\rightarrow {1}^{ - }}}f\\\\left( t\\\\right) + \\\\mathop{\\\\lim }\\\\limits_{{t \\\\rightarrow {1}^{ + }}}f\\\\left( t\\\\right) }\\\\right) = \\\\frac{1}{2}\\\\left( {2 + 1}\\\\right) = \\\\frac{3}{2} \\) in the point of jump discontinuity \\( d = 1 \\) ;\\n\\n\\( s\\\\left( 0\\\\right) = s\\\\left( 2\\\\right) = \\\\frac{1}{2}\\\\left( {\\\\mathop{\\\\lim }\\\\limits_{{t \\\\rightarrow {0}^{ + }}}f\\\\left( t\\\\right) + \\\\mathop{\\\\lim }\\\\limits_{{t \\\\rightarrow {2}^{ - }}}f\\\\left( t\\\\right) }\\\\right) = \\\\frac{1}{2}\\\\left( {2 + 0}\\\\right) = 1 \\) in endpoints of interval \\( a = 0, b = 2 \\) . Decomposition.",
        "id": "college_math_237899"
    },
    {
        "informal_statement": "Example 4.9 Although the Lebesgue measures of the intervals \\( \\left\\lbrack {a, b}\\right\\rbrack ,(a, b\\rbrack \\) , \\( \\lbrack a, b),\\left( {a, b}\\right) \\) are all the same, this is not always the case for the Lebesgue-Stieltjes measures. Furthermore, the Lebesgue-Stieltjes measure of a single point may be nonzero. For example, let\\n\\n\\[ F\\left( x\\right) = \\left\\{ \\begin{array}{ll} 0 & \\text{ if }x < 1 \\\\ 1 & \\text{ if }x \\geq 1 \\end{array}\\right. \\]",
        "informal_proof": "Then\\n\\n\\[ {\\mu }_{F}\\left( \\left( {0,1}\\right) \\right) = F\\left( {1 - }\\right) - F\\left( {0 + }\\right) = F\\left( {1 - }\\right) - F\\left( 0\\right) = 0 - 0 = 0, \\]\\n\\n\\[ {\\mu }_{F}(\\left( {0,1\\rbrack }\\right) = F\\left( {1 + }\\right) - F\\left( {0 + }\\right) = F\\left( {1 + }\\right) - F\\left( 0\\right) = 1 - 0 = 1, \\]\\n\\n\\[ {\\mu }_{F}\\left( \\left\\lbrack {1,1}\\right\\rbrack \\right) = F\\left( {1 + }\\right) - F\\left( {1 - }\\right) = 1 - 0 = 1, \\]\\n\\n\\[ {\\mu }_{F}\\left( \\left( {1,2}\\right) \\right) = F\\left( {2 - }\\right) - F\\left( {1 + }\\right) = F\\left( 2\\right) - F\\left( {1 + }\\right) = 1 - 1 = 0, \\]\\n\\n\\[ {\\mu }_{F}\\left( {\\lbrack 1,2}\\right) ) = F\\left( {2 - }\\right) - F\\left( {1 - }\\right) = F\\left( 2\\right) - F\\left( {1 - }\\right) = 1 - 0 = 1. \\]",
        "id": "college_math_372527"
    },
    {
        "informal_statement": "Lemma 2. For the convex decomposition, \\( \\alpha {\\bar{x}}^{ * } = \\mathop{\\sum }\\limits_{{i \\in \\mathcal{I}}}{\\mu }_{i}{\\chi }^{i} \\), we have\\n\\n\\[ \\n{\\beta }_{u} \\leq \\alpha \\frac{{b}_{u} - {d}_{S}{x}_{S}^{ * }}{\\max \\left\\{ {{b}_{u} - {d}_{S} + 1,\\bar{\\delta }}\\right\\} }.\\n\\]",
        "informal_proof": "Proof. Let \\( {\\mathcal{I}}_{\\bar{S}} \\) be the indices of the bins which cannot accommodate \\( S \\) . Thus by definition,\\n\\n\\[ \\n{\\beta }_{u} = \\mathop{\\sum }\\limits_{{i \\in {\\mathcal{I}}_{\\bar{S}}}}{\\mu }_{i}\\n\\]\\n\\nThe total volume of all such bins is at most the total \\( u \\) -volume of \\( \\alpha \\bar{x} \\), which does not contain \\( S \\) :\\n\\n\\[ \\n\\mathop{\\sum }\\limits_{{i \\in {\\mathcal{I}}_{\\bar{S}}}}{\\mu }_{i}{h}_{i} \\leq \\alpha \\left( {{b}_{u} - {d}_{S}{x}_{S}^{ * }}\\right)\\n\\]\\n\\nEach bin in \\( {\\mathcal{I}}_{\\bar{S}} \\) must have height large enough to block \\( S \\) and must also contain at least one edge since \\( S \\) fits on its own, by the no clipping assumption. Thus \\( {h}_{i} \\geq \\max \\left\\{ {{b}_{u} - {d}_{S} + 1,\\bar{\\delta }}\\right\\} \\), yielding the desired result when coupled with the above equation and inequality:\\n\\n\\[ \\n\\max \\left\\{ {{b}_{u} - {d}_{S} + 1,\\bar{\\delta }}\\right\\} {\\beta }_{u} = \\max \\left\\{ {{b}_{u} - {d}_{S} + 1,\\bar{\\delta }}\\right\\} \\mathop{\\sum }\\limits_{{i \\in {\\mathcal{I}}_{\\bar{S}}}}{\\mu }_{i} \\leq \\alpha \\left( {{b}_{u} - {d}_{S}{x}_{S}^{ * }}\\right) .\\n\\]",
        "id": "college_math_159759"
    },
    {
        "informal_statement": "Lemma 15.2. (i) For all \\( n : {\\mathrm{K}}_{0}{\\vartheta }_{n} < {\\vartheta }_{n},{\\vartheta }_{n} < {\\vartheta }_{n + 1} \\) and \\( {\\zeta }_{n} = {}_{nf}◐{\\vartheta }_{n} \\) .",
        "informal_proof": "Proof. We show (i) by induction on \\( n \\) . This is obvious when \\( n = 0 \\) . Let \\( n = m + \\) 1. By the induction hypothesis we have \\( {\\mathrm{K}}_{0}{\\vartheta }_{n} = {\\mathrm{K}}_{0}{\\zeta }_{m} = \\left\\{ {\\vartheta }_{m}\\right\\} \\cup {\\mathrm{K}}_{0}{\\vartheta }_{m} < {\\vartheta }_{n} \\) , and consequently \\( {\\zeta }_{n} = {}_{nf}◐{\\vartheta }_{n},{\\zeta }_{m} < {\\zeta }_{n} \\) and \\( {\\vartheta }_{n} = \\Phi {\\zeta }_{m}0 < \\Phi {\\zeta }_{n}0 = {\\vartheta }_{n + 1} \\) .",
        "id": "college_math_227514"
    },
    {
        "informal_statement": "Theorem 13.7 (Galois Correspondence) Let \\( k \\subset \\mathbb{K} \\) be a finite Galois extension with Galois group \\( G = {\\operatorname{Aut}}_{k}\\mathbb{K} \\). Then there is a canonical bijection between the subgroups \\( H \\subset G \\) and the subfields \\( \\mathbb{L} \\subset \\mathbb{K} \\) such that \\( \\mathbb{k} \\subset \\mathbb{L} \\). It takes a subgroup \\( H \\subset G \\) to the subfield of \\( H \\)-invariants \\( {\\mathbb{K}}^{H} \\subset \\mathbb{K} \\). The inverse map sends a field \\( \\mathbb{L} \\) such that \\( \\mathbb{k} \\subset \\mathbb{L} \\subset \\mathbb{K} \\) to the subgroup \\( {\\operatorname{Aut}}_{\\mathbb{L}}\\mathbb{K} \\subset G \\). Under this correspondence, the normal subgroups \\( H \\vartriangleleft G \\) are in bijection with the Galois extensions \\( \\mathbb{L} \\supset \\mathbb{k} \\) contained in \\( \\mathbb{K} \\), and \\( \\operatorname{Gal}\\mathbb{L}/\\mathbb{k} \\simeq G/H \\) for every such Galois extension \\( \\mathbb{L} \\supset \\mathbb{k} \\).",
        "informal_proof": "Proof Given a tower of fields \\( \\mathbb{k} \\subset \\mathbb{L} \\subset \\mathbb{K} \\), the extension \\( \\mathbb{L} \\subset \\mathbb{K} \\) is normal by Lemma 13.4 and separable by Corollary 13.2. Thus, \\( \\mathbb{L} \\subset \\mathbb{K} \\) is a Galois extension with Galois group \\( H = {\\operatorname{Aut}}_{\\mathbb{L}}\\mathbb{K} \\), and \\( \\left| H\\right| = \\deg \\mathbb{K}/\\mathbb{L} \\). Certainly, the group \\( H \\) is a subgroup of \\( G = {\\operatorname{Aut}}_{\\mathbb{k}}\\mathbb{K} \\), and by Corollary 13.3, \\( {K}^{H} = \\mathbb{L} \\). This proves the first statement, concerning the one-to-one correspondence between subgroups \\( H \\subset G \\) and subfields \\( \\mathbb{L} \\subset \\mathbb{K} \\) such that \\( \\mathbb{k} \\subset \\mathbb{L} \\). To prove the second statement, consider the action of \\( G = \\mathrm{{Gal}}\\mathbb{K}/\\mathbb{k} \\) on the subfields \\( \\mathbb{L} \\subset \\mathbb{K} \\) such that \\( \\mathbb{k} \\subset \\mathbb{L} \\). We have proved already that the centralizer \\( {C}_{\\mathbb{L}}\\overset{\\text{ def }}{ = }\\left\\{ {g \\in G \\mid {\\left. g\\right| }_{\\mathbb{L}} = {\\operatorname{Id}}_{\\mathbb{L}}}\\right\\} = {\\operatorname{Aut}}_{\\mathbb{L}}\\mathbb{K} \\) of every such \\( \\mathbb{L} \\) is the subgroup \\( H \\subset G \\) corresponding to \\( \\mathbb{L} \\). Since the extension \\( \\mathbb{K} \\supset \\mathbb{k} \\) is normal and separable, every embedding \\[ \\varphi : \\mathbb{L} \\hookrightarrow \\mathbb{K} \\tag{13.11} \\] over \\( \\mathbb{k} \\) can be extended to an automorphism \\( g : \\mathbb{K} \\rightarrow \\mathbb{K} \\) over \\( \\mathbb{k} \\). Therefore, \\( \\varphi \\left( \\mathbb{L}\\right) = g\\left( \\mathbb{L}\\right) \\) for some \\( g \\in G \\), and the centralizer of \\( \\varphi \\left( \\mathbb{L}\\right) \\) in \\( G \\) is conjugate to H: \\[ {\\operatorname{Aut}}_{\\varphi \\left( \\mathbb{L}\\right) }\\mathbb{K} = {C}_{\\varphi \\left( \\mathbb{L}\\right) } = {C}_{g\\left( \\mathbb{L}\\right) } = g{C}_{\\mathbb{L}}{g}^{-1} = {gH}{g}^{-1}. \\] It follows from Lemma 13.4 and Corollary 13.2 that an extension \\( \\mathbb{L} \\supset \\mathbb{k} \\) is always separable. It is normal if and only if \\( \\varphi \\left( \\mathbb{L}\\right) = \\mathbb{L} \\) for all embeddings (13.11), which means that all subgroups conjugate to \\( H \\) coincide with \\( H \\), i.e., that \\( H \\vartriangleleft G \\) is normal. In this case, the Galois group Gal \\( \\mathbb{K}/\\mathbb{k} \\) maps \\( \\mathbb{L} \\) to itself. This leads to a surjective homomorphism of groups \\( \\operatorname{Gal}\\mathbb{K}/\\mathbb{k} \\rightarrow \\operatorname{Gal}\\mathbb{L}/\\mathbb{k} \\) with kernel \\( \\operatorname{Gal}\\mathbb{K}/\\mathbb{L} \\). Therefore, Gal \\( \\mathbb{L}/\\mathbb{k} = \\left( {\\text{Gal}\\mathbb{K}/\\mathbb{k}}\\right) /\\left( {\\text{Gal}\\mathbb{K}/\\mathbb{L}}\\right) \\).",
        "id": "college_math_288126"
    },
    {
        "informal_statement": "Problem 3.1. Solution: The partial sums\\n\\n\\\\[ \\n{W}_{N}\\left( {t,\\omega }\\right) = \\mathop{\\sum }\\limits_{{n = 0}}^{{N - 1}}{G}_{n}\\left( \\omega \\right) {S}_{n}\\left( t\\right) ,\\;t \\in \\left\\lbrack {0,1}\\right\\rbrack ,\\n\\\\]\\n\\nconverge as \\( N \\rightarrow \\infty \\mathbb{P} \\) -a.s. uniformly for \\( t \\) towards \\( B\\left( {t,\\omega }\\right), t \\in \\left\\lbrack {0,1}\\right\\rbrack \\) -cf. Problem 3.3. Therefore, the random variables\\n\\n\\\\[ \\n{\\int }_{0}^{1}{W}_{N}\\left( t\\right) {dt} = \\mathop{\\sum }\\limits_{{n = 0}}^{{N - 1}}{G}_{n}{\\int }_{0}^{1}{S}_{n}\\left( t\\right) {dt}\\xrightarrow[{N \\rightarrow \\infty }]{\\text{ P-a.s. }}X = {\\int }_{0}^{1}B\\left( t\\right) {dt}.\\n\\\\]\\n\\nThis shows that \\( {\\int }_{0}^{1}{W}_{N}\\left( t\\right) {dt} \\) is the sum of independent \\( \\mathrm{N}\\left( {0,1}\\right) \\) -random variables, hence itself normal and so is its limit \\( X \\) .",
        "informal_proof": "From the definition of the Schauder functions (cf. Figure 3.2) we find\\n\\n\\\\[ \\n{\\int }_{0}^{1}{S}_{0}\\left( t\\right) {dt} = \\frac{1}{2}\\n\\\\]\\n\\n\\\\[ \\n{\\int }_{0}^{1}{S}_{{2}^{j} + k}\\left( t\\right) {dt} = \\frac{1}{4}{2}^{-\\frac{3}{2}j},\\;k = 0,1,\\ldots ,{2}^{j} - 1, j \\geq 0.\\n\\\\]\\n\\nand this shows\\n\\n\\\\[ \\n{\\int }_{0}^{1}{W}_{{2}^{n + 1}}\\left( t\\right) {dt} = \\frac{1}{2}{G}_{0} + \\frac{1}{4}\\mathop{\\sum }\\limits_{{j = 0}}^{n}\\mathop{\\sum }\\limits_{{l = 0}}^{{{2}^{j} - 1}}{2}^{-\\frac{3}{2}j}{G}_{{2}^{j} + l}.\\n\\\\]\\n\\nConsequently, since the \\( {G}_{j} \\) are iid \\( \\mathrm{N}\\left( {0,1}\\right) \\) random variables,\\n\\n\\\\[ \\n\\mathbb{E}{\\int }_{0}^{1}{W}_{{2}^{n + 1}}\\left( t\\right) {dt} = 0\\n\\\\]\\n\\n\\\\[ \\n\\mathbb{V}{\\int }_{0}^{1}{W}_{{2}^{n + 1}}\\left( t\\right) {dt} = \\frac{1}{4} + \\frac{1}{16}\\mathop{\\sum }\\limits_{{j = 0}}^{n}\\mathop{\\sum }\\limits_{{l = 0}}^{{{2}^{j} - 1}}{2}^{-{3j}}\\n\\\\]\\n\\n\\\\[ \\n= \\frac{1}{4} + \\frac{1}{16}\\mathop{\\sum }\\limits_{{j = 0}}^{n}{2}^{-{2j}}\\n\\\\]\\n\\n\\\\[ \\n= \\frac{1}{4} + \\frac{1}{16}\\frac{1 - {2}^{-2\\left( {n + 1}\\right) }}{1 - \\frac{1}{4}}\\n\\\\]\\n\\n\\\\[ \\n\\underset{n \\rightarrow \\infty }{ \\rightarrow }\\frac{1}{4} + \\frac{1}{16}\\frac{4}{3} = \\frac{1}{3}\\n\\\\]\\n\\nThis means that\\n\\n\\\\[ \\nX = \\frac{1}{2}{G}_{0} + \\mathop{\\sum }\\limits_{{j = 0}}^{\\infty }\\frac{1}{4}{2}^{-\\frac{3}{2}}j\\underset{ \\sim \\mathrm{N}\\left( {0,{2}^{j}}\\right) }{\\underbrace{\\mathop{\\sum }\\limits_{{l = 0}}^{{{2}^{j} - 1}}{G}_{{2}^{j} + l}}}\\n\\\\]\\n\\nwhere the series converges \\( \\mathbb{P} \\) -a.s. and in mean square, and \\( X \\sim \\mathrm{N}\\left( {0,\\frac{1}{3}}\\right) \\) .",
        "id": "college_math_321329"
    },
    {
        "informal_statement": "Theorem 3.6.5 Let \\( X \\) be a strongly symmetric Borel right process and assume that its \\( \\alpha \\) -potential density, \\( {u}^{\\alpha }\\left( {x, y}\\right) \\), is finite for all \\( x, y \\in S \\) . Let \\( {L}_{t}^{y} \\) be a local time of \\( X \\) at \\( y \\), with\\n\\n\\[ \\n{E}^{x}\\left( {{\\int }_{0}^{\\infty }{e}^{-{\\alpha t}}d{L}_{t}^{y}}\\right) = {u}^{\\alpha }\\left( {x, y}\\right) \\tag{3.106} \\n\\]\\n\\nThen\\n\\n\\[ \\n{E}^{x}\\left( {e}^{-\\alpha {T}_{y}}\\right) = \\frac{{u}^{\\alpha }\\left( {x, y}\\right) }{{u}^{\\alpha }\\left( {y, y}\\right) } \\tag{3.107} \\n\\]\\n\\nand for every \\( t \\)\\n\\n\\[ \\n{E}^{x}\\left( {L}_{t}^{y}\\right) = {\\int }_{0}^{t}{p}_{s}\\left( {x, y}\\right) {ds}. \\tag{3.108} \\n\\]\\n\\nFurthermore, if \\( u\\left( {x, x}\\right) \\) and \\( u\\left( {y, y}\\right) \\) are finite,\\n\\n\\[ \\n{P}^{x}\\left( {{T}_{y} < \\infty }\\right) = \\frac{u\\left( {x, y}\\right) }{u\\left( {y, y}\\right) } \\tag{3.109} \\n\\]",
        "informal_proof": "Proof When \\( x = y \\) ,(3.107) is just (3.83). When \\( x \\neq y \\) ,(3.107) follows from (3.106), the support property of \\( d{L}_{t}^{y} \\), and the strong Markov property as follows:\\n\\n\\[ \\n{u}^{\\alpha }\\left( {x, y}\\right) = {E}^{x}\\left( {{\\int }_{0}^{\\infty }{e}^{-{\\alpha s}}d{L}_{s}^{y}}\\right) \\tag{3.110} \\n\\]\\n\\n\\[ \\n= {E}^{x}\\left( {{1}_{\\left\\{ {T}_{y} < \\infty \\right\\} }{\\int }_{{T}_{y}}^{\\infty }{e}^{-{\\alpha s}}d{L}_{s}^{y}}\\right) \\n\\]\\n\\n\\[ \\n= {E}^{x}\\left( {{e}^{-\\alpha {T}_{y}}{1}_{\\left\\{ {T}_{y} < \\infty \\right\\} }\\left( {{\\int }_{0}^{\\infty }{e}^{-{\\alpha s}}d{L}_{s}^{y}}\\right) \\circ {\\theta }_{{T}_{y}}}\\right) \\n\\]\\n\\n\\[ \\n= {E}^{x}\\left( {{e}^{-\\alpha {T}_{y}}{1}_{\\left\\{ {T}_{y} < \\infty \\right\\} }{E}^{{X}_{{T}_{y}}}\\left( {{\\int }_{0}^{\\infty }{e}^{-{\\alpha s}}d{L}_{s}^{y}}\\right) }\\right) \\n\\]\\n\\n\\[ \\n= {E}^{x}\\left( {e}^{-\\alpha {T}_{y}}\\right) {u}^{\\alpha }\\left( {y, y}\\right) \\n\\]\\n\\nsince \\( {X}_{{T}_{y}} = y \\) on \\( \\left\\{ {{T}_{y} < \\infty }\\right\\} \\) .\\n\\nWe get (3.109) by taking the limit in (3.107) as \\( \\alpha \\) goes to zero.\\n\\nTo obtain (3.108) we first note that if \\( \\lambda \\) is an independent exponential random variable with mean \\( 1/\\alpha \\), then\\n\\n\\[ \\n\\alpha {\\int }_{0}^{\\infty }{e}^{-{\\alpha t}}{L}_{t}^{y}{dt} = {E}_{\\lambda }^{x}\\left( {L}_{\\lambda }^{y}\\right) = {E}_{\\lambda }^{x}\\left( {{\\int }_{0}^{\\infty }{1}_{\\{ \\lambda > t\\} }d{L}_{t}^{y}}\\right) \\tag{3.111} \\n\\]\\n\\n\\[ \\n= {E}^{x}\\left( {{\\int }_{0}^{\\infty }{e}^{-{\\alpha t}}d{L}_{t}^{y}}\\right) = {u}^{\\alpha }\\left( {x, y}\\right) .\\n\\]\\n\\nTherefore\\n\\n\\[ \\n\\alpha {\\int }_{0}^{\\infty }{e}^{-{\\alpha t}}{E}^{x}\\left( {L}_{t}^{y}\\right) {dt} = {\\int }_{0}^{\\infty }{e}^{-{\\alpha t}}{p}_{t}\\left( {x, y}\\right) {dt} \\tag{3.112} \\n\\]\\n\\n\\[ \\n= \\alpha {\\int }_{0}^{\\infty }{e}^{-{\\alpha t}}\\left( {{\\int }_{0}^{t}{p}_{s}\\left( {x, y}\\right) {ds}}\\right) {dt} \\n\\]\\n\\nby integration by parts. Here we use the fact that\\n\\n\\[ \\n\\mathop{\\lim }\\limits_{{t \\rightarrow \\infty }}{e}^{-{\\alpha t}}\\left( {{\\int }_{0}^{t}{p}_{s}\\left( {x, y}\\right) {ds}}\\right) \\tag{3.113} \\n\\]\\n\\n\\[ \\n\\leq \\mathop{\\lim }\\limits_{{t \\rightarrow \\infty }}{e}^{-{\\alpha t}/2}\\left( {{\\int }_{0}^{t}{e}^{-{\\alpha s}/2}{p}_{s}\\left( {x, y}\\right) {ds}}\\right) \\n\\]\\n\\n\\[ \\n\\leq \\mathop{\\lim }\\limits_{{t \\rightarrow \\infty }}{e}^{-{\\alpha t}/2}{u}^{\\alpha /2}\\left( {x, y}\\right) = 0. \\n\\]\\n\\nEquation (3.112) holds as long as \\( {u}^{\\alpha }\\left( {x, y}\\right) \\) is finite. Since \\( {u}^{\\lambda }\\left( {x, y}\\right) \\) is decreasing in \\( \\lambda \\) ,(3.112) holds for all \\( \\lambda \\geq \\alpha \\) . By the uniqueness property of Laplace transforms, this implies that (3.108) holds for almost every \\( t \\) . Since both sides of (3.108) are continuous in \\( t \\), it follows that (3.108) holds for every \\( t \\) .",
        "id": "college_math_259181"
    },
    {
        "informal_statement": "Lemma 2.8.3. (a) The map $$ \\\\operatorname{Maps}\\\\left( {{\\\\operatorname{Sq}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{\\\\text{vert, horiz }}\\\\right), B}\\\\right) \\\\rightarrow $$ $$ \\\\rightarrow \\\\operatorname{Maps}\\\\left( {{\\\\mathrm{{Sq}}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}^{ \\\\leq {m}_{1}}\\\\right), B}\\\\right) \\\\underset{\\\\operatorname{Maps}\\\\left( {{\\\\mathrm{{Sq}}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}^{ = {m}_{1}}\\\\right), B}\\\\right) }{ \\\\times }\\\\operatorname{Maps}\\\\left( {{\\\\mathrm{{Sq}}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}^{ \\\\geq {m}_{1}}\\\\right), B}\\\\right) , $$ induced by (2.25), is an isomorphism whenever $B$ is a double Segal space.",
        "informal_proof": "2.8.5. Proof of Lemma 2.8.3. We will prove point (a) of the lemma; point (b) is similar but simpler. We have $$ {\\\\operatorname{Sq}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}\\\\right) = \\\\mathop{\\\\operatorname{colim}}\\\\limits_{{\\\\left\\\\lbrack i\\\\right\\\\rbrack \\\\rightarrow \\\\left\\\\lbrack m\\\\right\\\\rbrack ,\\\\left\\\\lbrack j\\\\right\\\\rbrack \\\\rightarrow \\\\left\\\\lbrack n\\\\right\\\\rbrack ,\\\\operatorname{Im}\\\\left( {\\\\left\\\\lbrack i\\\\right\\\\rbrack \\\\times \\\\left\\\\lbrack j\\\\right\\\\rbrack }\\\\right) \\\\subset Q}}{\\\\operatorname{Seq}}_{ \\\\bullet }\\\\left( \\\\left\\\\lbrack i\\\\right\\\\rbrack \\\\right) \\\\boxtimes {\\\\operatorname{Seq}}_{ \\\\bullet }\\\\left( \\\\left\\\\lbrack j\\\\right\\\\rbrack \\\\right) . $$ Cofinal in the above index category is the full subcategory, denoted $E$, that consists of those maps for which one of the following three scenarios happens: (1) The image of $\\{ 0,\\\\ldots, i\\}$ in $\\{ 0,\\\\ldots, m\\}$ is $\\\\left\\\\langle {m}_{1}\\\\right.$ ; (2) The image of $\\{ 0,\\\\ldots, i\\}$ in $\\{ 0,\\\\ldots, m\\}$ is $> {m}_{1}$ ; (3) The element ${m}_{1} \\\\in \\{ 0,\\\\ldots, m\\}$ has a unique preimage in $\\{ 0,\\\\ldots, i\\}$ . For an object $$ \\\\left( {\\\\left\\\\lbrack i\\\\right\\\\rbrack \\\\rightarrow \\\\left\\\\lbrack m\\\\right\\\\rbrack ,\\\\left\\\\lbrack j\\\\right\\\\rbrack \\\\rightarrow \\\\left\\\\lbrack n\\\\right\\\\rbrack ,\\\\operatorname{Im}\\\\left( {\\\\left\\\\lbrack i\\\\right\\\\rbrack \\\\times \\\\left\\\\lbrack j\\\\right\\\\rbrack }\\\\right) \\\\subset Q}\\\\right) = : e \\\\in E $$ consider the fiber product $$ {B}_{e} \\\\mathrel{\\\\text{:=}} \\\\left( {{\\\\operatorname{Sq}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}^{ \\\\leq {m}_{1}}\\\\right) \\\\underset{{\\\\operatorname{Sq}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}^{ = {m}_{1}}\\\\right) }{ \\\\sqcup }{\\\\operatorname{Sq}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}^{ \\\\geq {m}_{1}}\\\\right) }\\\\right) $$ $$ \\\\underset{{\\\\mathrm{{Sq}}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}\\\\right) }{ \\\\times }\\\\left( {{\\\\operatorname{Seq}}_{ \\\\bullet }\\\\left( \\\\left\\\\lbrack i\\\\right\\\\rbrack \\\\right) \\\\boxtimes {\\\\operatorname{Seq}}_{ \\\\bullet }\\\\left( \\\\left\\\\lbrack j\\\\right\\\\rbrack \\\\right) }\\\\right) , $$ taken in the category ${\\\\mathrm{{Spc}}}^{{\\\\mathbf{\\\\Delta }}^{\\\\mathrm{{op}}} \\\\times {\\\\mathbf{\\\\Delta }}^{\\\\mathrm{{op}}}}$ . We have: $$ {\\\\operatorname{Sq}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}\\\\right) \\\\simeq {\\\\operatorname{colim}}_{e \\\\in E}{\\\\operatorname{Seq}}_{ \\\\bullet }\\\\left( \\\\left\\\\lbrack i\\\\right\\\\rbrack \\\\right) \\\\boxtimes {\\\\operatorname{Seq}}_{ \\\\bullet }\\\\left( \\\\left\\\\lbrack j\\\\right\\\\rbrack \\\\right) , $$ and since fiber products in ${\\\\mathrm{{Spc}}}^{{\\\\mathbf{\\\\Delta }}^{\\\\mathrm{{op}}} \\\\times {\\\\mathbf{\\\\Delta }}^{\\\\mathrm{{op}}}}$ commute with colimits, we also have $$ {\\\\operatorname{Sq}}_{\\\\bullet , \\\\bullet }\\\\left( {\\\\mathbf{Q}}_{{vert},{horiz}}^{ \\\\leq {m}_{1}}\\\\right) \\\\underset{{\\\\operatorname{Sq}}_{\\\\bullet",
        "id": "college_math_62"
    },
    {
        "informal_statement": "Theorem 10.4.30 If \\( \\mathcal{A} \\) is \\( \\sigma \\) -finite, there exists an increasing sequence \\( {\\left( {\\mathbf{p}}_{n}\\right) }_{n \\geq 0} \\) in \\( \\mathcal{Q} \\) such that \\( {\\mathbf{p}}_{T} = \\mathop{\\sup }\\limits_{{n \\geq 0}}{\\mathbf{p}}_{n} \\) . In particular, \\( {\\mathbf{p}}_{T} \\in \\mathcal{Q} \\) .",
        "informal_proof": "Proof Let \\( {\\left\\{ {e}_{n}\\right\\} }_{n \\geq 0} \\) be a countable subset of \\( {\\mathbf{p}}_{T}\\left( \\mathbb{H}\\right) \\) that is separating for \\( {\\mathbf{p}}_{T}\\mathcal{A}{\\mathbf{p}}_{T} \\) and suppose that \\( {e}_{n} \\in \\overline{\\mathcal{U}\\left( {\\mathbf{x}}_{n}\\right) \\left( \\mathbb{H}\\right) } \\) for some \\( {\\mathbf{x}}_{n} \\in {\\mathcal{A}}_{\\text{int }}, n \\geq 0 \\) (see Lemma (10.4.29)). Define \\( {\\mathbf{y}}_{n} \\mathrel{\\text{:=}} \\mathop{\\sum }\\limits_{{k = 0}}^{n}{\\mathbf{x}}_{k} \\) and let \\( {\\mathbf{p}}_{n} \\mathrel{\\text{:=}} \\left\\lbrack {\\mathcal{U}\\left( {\\mathbf{y}}_{n}\\right) }\\right\\rbrack \\) for all \\( n \\geq 0 \\) . Then, any \\( {\\mathbf{y}}_{n} \\) belongs to \\( {\\mathcal{A}}_{int} \\) and \\( {\\left\\{ {\\mathbf{p}}_{n}\\right\\} }_{n \\geq 0} \\) is an increasing sequence in \\( \\mathcal{P} \\) . Moreover, since\\n\\n\\[ \\ker \\left( {\\mathcal{U}\\left( {\\mathbf{y}}_{n}\\right) }\\right) = { \\cap }_{k = 0}^{n}\\ker \\left( {\\mathcal{U}\\left( {\\mathbf{x}}_{n}\\right) }\\right) ,\\;\\forall n \\geq 0, \\]\\n\\nwe have\\n\\n\\[ {e}_{n} \\in \\overline{\\mathcal{U}\\left( {\\mathbf{x}}_{n}\\right) \\left( \\mathbb{H}\\right) } \\subset \\overline{\\mathcal{U}\\left( {\\mathbf{y}}_{n}\\right) \\left( \\mathbb{H}\\right) } = {\\mathbf{p}}_{n}\\left( \\mathbb{H}\\right) , \\]\\n\\nwhich implies \\( \\left( {{\\mathbf{p}}_{T} - \\mathop{\\sup }\\limits_{{m \\geq 0}}{\\mathbf{p}}_{m}}\\right) {e}_{n} = 0 \\) for all \\( n \\geq 0 \\) . Therefore, \\( {\\mathbf{p}}_{T} = \\mathop{\\sup }\\limits_{{n \\geq 0}}{\\mathbf{p}}_{n} \\) for \\( {\\left\\{ {e}_{n}\\right\\} }_{n \\geq 0} \\) is separating for \\( {\\mathbf{p}}_{T}\\mathcal{A}{\\mathbf{p}}_{T} \\) and \\( {\\mathbf{p}}_{T} - \\mathop{\\sup }\\limits_{{n \\geq 0}}{\\mathbf{p}}_{n} \\in {\\mathbf{p}}_{T}\\mathcal{A}{\\mathbf{p}}_{T} \\) . The last statement follows from Part (1) of Lemma (10.4.28). This proves the theorem.",
        "id": "college_math_211081"
    },
    {
        "informal_statement": "Lemma 6. There is a polynomial-time algorithm that yields an optimal solution to $\\\\mathrm{LP3}$ . The solution has at most one fractional variable, which is associated with a facility $l \\\\in \\\\mathcal{S}_1$ satisfying $\\\\left| \\\\mathcal{Q}_l\\\\right| > 1$ .",
        "informal_proof": "We find an optimal solution $z^* \\\\; to \\\\; LP3 \\\\; using \\\\; Lemma \\\\; 6. \\\\; Let \\\\; \\\\mathcal{L}_0 = \\\\left\\\\{ i \\\\in \\\\mathcal{S}_1 \\\\right. : \\\\left. z_i^* = 0 \\\\right\\\\} \\\\; and \\\\; \\\\mathcal{L}_1 = \\\\left\\\\{ i \\\\in \\\\mathcal{S}_1 : z_i^* = 1 \\\\right\\\\} . \\\\; If \\\\; z^* \\\\; has \\\\; a \\\\; fractional \\\\; variable, \\\\; then \\\\; let \\\\; l \\\\in \\\\mathcal{S}_1 \\\\; denote \\\\; the \\\\; facility \\\\; associated \\\\; with \\\\; this \\\\; variable. \\\\; Our \\\\; solution \\\\; to \\\\; \\\\mathcal{I} \\\\; is \\\\; constructed \\\\; as \\\\; follows.\\n\\n- If \\\\; facility \\\\; l \\\\in \\\\mathcal{S}_1 \\\\; is \\\\; associated \\\\; with \\\\; a \\\\; fractional \\\\; variable \\\\; z_l^* , \\\\; then \\\\; define \\\\; \\\\mathcal{Q}^\\\\dagger = \\\\underset{\\\\mathcal{Q} \\\\subset \\\\mathcal{Q}_l \\\\land \\\\left| \\\\mathcal{Q} \\\\right| = \\\\left\\\\lfloor z_l^* \\\\left| \\\\mathcal{Q}_l \\\\right| \\\\right\\\\rfloor }{\\\\arg \\\\max } \\\\mathop{\\\\sum }\\\\limits_{i' \\\\in \\\\mathcal{Q}} \\\\Phi_2 \\\\left( i' \\\\right) . \\\\; We \\\\; open \\\\; l \\\\; and \\\\; each \\\\; facility \\\\; from \\\\; \\\\mathcal{Q}^\\\\dagger . \\\\; We\\n\\nassign \\\\; each \\\\; j \\\\in \\\\gamma_2 \\\\left( \\\\mathcal{Q}^\\\\dagger \\\\right) \\\\; to \\\\; i_2 \\\\left( j \\\\right) \\\\; and \\\\; each \\\\; j \\\\in \\\\gamma_2 \\\\left( \\\\mathcal{Q}_l \\\\smallsetminus \\\\mathcal{Q}^\\\\dagger \\\\right) \\\\; to \\\\; l . \\\\; Lemma \\\\; 4 \\\\; implies \\\\; that \\\\; the \\\\; total \\\\; assignment \\\\; cost \\\\; of \\\\; the \\\\; clients \\\\; from \\\\; \\\\gamma_2 \\\\left( \\\\mathcal{Q}_l \\\\right) \\\\; is \\\\; no \\\\; more \\\\; than \\\\; \\\\mathop{\\\\sum }\\\\limits_{j \\\\in \\\\gamma_2 \\\\left( \\\\mathcal{Q}^\\\\dagger \\\\right) } \\\\Delta_2 \\\\left( j \\\\right) + \\\\mathop{\\\\sum }\\\\limits_{j \\\\in \\\\gamma_2 \\\\left( \\\\mathcal{Q}_l \\\\smallsetminus \\\\mathcal{Q}^\\\\dagger \\\\right) } \\\\left( 6 \\\\Delta_2 \\\\left( j \\\\right) + 3 \\\\Delta_1 \\\\left( j \\\\right) \\\\right) . \\\\; If \\\\; z^* \\\\; does \\\\; not \\\\; involve \\\\; a \\\\; fractional \\\\; variable, \\\\; then \\\\; let \\\\; \\\\mathcal{Q}_l = \\\\mathcal{Q}^\\\\dagger = \\\\varnothing .\\n\\n- For \\\\; each \\\\; i \\\\in \\\\mathcal{L}_1 \\\\; and \\\\; i' \\\\in \\\\mathcal{Q}_i , \\\\; we \\\\; open \\\\; i' \\\\; and \\\\; assign \\\\; each \\\\; j \\\\in \\\\gamma_2 \\\\left( i' \\\\right) \\\\; to \\\\; i' . \\\\; The \\\\; total \\\\; assignment \\\\; cost \\\\; of \\\\; the \\\\; clients \\\\; from \\\\; \\\\gamma_2 \\\\left( \\\\mathop{\\\\bigcup }\\\\limits_{i \\\\in \\\\mathcal{L}_1} \\\\mathcal{Q}_i \\\\right) \\\\; is \\\\; \\\\mathop{\\\\sum }\\\\limits_{j \\\\in \\\\gamma_2 \\\\left( \\\\mathop{\\\\bigcup }\\\\limits_{i \\\\in \\\\mathcal{L}_1} \\\\mathcal{Q}_i \\\\right) } \\\\Delta_2 \\\\left( j \\\\right)\\n\\n- For \\\\; each \\\\; i \\\\in \\\\mathcal{L}_0 , \\\\; we \\\\; open \\\\; i \\\\; and \\\\; assign \\\\; each \\\\; j \\\\in \\\\gamma_2 \\\\left( \\\\mathcal{Q}_i \\\\right) \\\\; to \\\\; i . \\\\; Lemma \\\\; 4 \\\\; implies \\\\; that \\\\; the \\\\; total \\\\; assignment \\\\; cost \\\\; of \\\\; the \\\\; clients \\\\; from \\\\; \\\\gamma_2 \\\\left( \\\\mathop{\\\\bigcup }\\\\limits_{i \\\\in \\\\mathcal{L}_0} \\\\mathcal{Q}_i \\\\right) \\\\; is \\\\; at \\\\; most \\\\; \\\\mathop{\\\\sum }\\\\limits_{j \\\\in \\\\gamma_2 \\\\left( \\\\mathop{\\\\bigcup }\\\\limits_{i \\\\in \\\\mathcal{L}_0} \\\\mathcal{Q}_i \\\\right) } \\\\left( 6 \\\\Delta_2 \\\\left( j \\\\right) + 3 \\\\Delta_1 \\\\left( j \\\\right) \\\\right) .\\n\\nWe \\\\; first \\\\; show \\\\; that \\\\; our \\\\; solution \\\\; opens \\\\; no \\\\; more \\\\; than \\\\; k \\\\; facilities \\\\; and \\\\; is \\\\; feasible \\\\; for \\\\; SM- k -FL. \\\\; If \\\\; z^* \\\\; does \\\\; not \\\\; have \\\\; a \\\\; fractional \\\\; variable, \\\\; then \\\\; the \\\\; number \\\\; of \\\\; the \\\\; facilities \\\\; opened \\\\; in \\\\; the \\\\; solution \\\\; is\\n\\n$$ \\n\\\\left| \\\\mathcal{L}_0 \\\\right| + \\\\mathop{\\\\sum }\\\\limits_{i \\\\in \\\\mathcal{L}_1} \\\\left| \\\\mathcal{Q}_i \\\\right| = \\\\mathop{\\\\sum }\\\\limits_{i \\\\in \\\\mathcal{L}_1} \\\\left( \\\\left| \\\\mathcal{Q}_i \\\\right| - 1 \\\\right) + k_1 = \\\\mathop{\\\\sum }\\\\limits_{i \\\\in \\\\mathcal{S}_1} z_i^* \\\\left( \\\\left| \\\\mathcal{Q}_i \\\\right| - 1 \\\\right) + k_1,\\n$$\\n\\nwhich \\\\; is \\\\; exactly \\\\; k \\\\; due \\\\; to \\\\; constraint \\\\; (10). \\\\; For \\\\; the \\\\; case \\\\; where \\\\; z^* \\\\; involves \\\\; a \\\\; fractional \\\\; variable \\\\; z_l^* , \\\\; the \\\\; facilities \\\\; opened \\\\; in \\\\; our \\\\; solution \\\\; is \\\\; no \\\\; more \\\\; than\\n\\n$$ \\n1 + z_l^* \\\\left| \\\\mathcal{Q}_l \\\\right| + \\\\left| \\\\mathcal{L}_0 \\\\right| + \\\\mathop{\\\\sum }\\\\limits_{i",
        "id": "college_math_43055"
    },
    {
        "informal_statement": "Example 2.3.10 We start with \\( {P}_{{\\mu }_{1},{\\mu }_{2}}^{2,1}\\left( {{A}_{1},{A}_{2}}\\right) \\) . Consider the permutation \\( \\sigma : \\{ 1,2,3\\} \\rightarrow \\) \\( \\{ 1,2,3\\} \\) defined by \\( \\sigma \\left( 1\\right) = 2,\\sigma \\left( 2\\right) = 3,\\sigma \\left( 3\\right) = 1 \\) . (Note that we could just as well consider the permutation \\( {\\sigma }^{\\prime } \\) defined by \\( {\\sigma }^{\\prime }\\left( 1\\right) = 3,{\\sigma }^{\\prime }\\left( 2\\right) = 2,{\\sigma }^{\\prime }\\left( 3\\right) = 1 \\) .) It then follows from Proposition 2.3.7 that\\n\\n\\[ \\n{P}_{{\\mu }_{1},{\\mu }_{2}}^{2,1}\\left( {{A}_{1},{A}_{2}}\\right) = {P}_{{\\mu }_{2},{\\mu }_{1}}^{1,2}\\left( {{A}_{2},{A}_{1}}\\right) . \\tag{2.3.25} \\n\\]",
        "informal_proof": "In the spirit of Example 2.3.9, we next verify this equality by a direct computation. Because of the added complexity due to the presence of the second power, we will first write the results of the calculation in a manner that makes the time dependence explicit. We will then take into account the fact that the operators \\( {A}_{1} \\) and \\( {A}_{2} \\) do not depend on time.\\n\\nWe have, successively,\\n\\n\\[ \\n{P}_{{\\mu }_{1},{\\mu }_{2}}^{2,1}\\left( {{A}_{1},{A}_{2}}\\right) \\n\\]\\n\\n\\[ \\n= {\\int }_{\\left\\lbrack {s}_{1} < {s}_{2} < {s}_{3}\\right\\rbrack }{A}_{2}\\left( {s}_{3}\\right) {A}_{1}\\left( {s}_{2}\\right) {A}_{1}\\left( {s}_{1}\\right) \\left( {{\\mu }_{1}^{2} \\times {\\mu }_{2}}\\right) \\left( {d{s}_{1}, d{s}_{2}, d{s}_{3}}\\right) \\n\\]\\n\\n\\[ \\n+ {\\int }_{\\left\\lbrack {s}_{1} < {s}_{3} < {s}_{2}\\right\\rbrack }{A}_{1}\\left( {s}_{2}\\right) {A}_{2}\\left( {s}_{3}\\right) {A}_{1}\\left( {s}_{1}\\right) \\left( {{\\mu }_{1} \\times {\\mu }_{2} \\times {\\mu }_{1}}\\right) \\left( {d{s}_{1}, d{s}_{2}, d{s}_{3}}\\right) \\n\\]\\n\\n\\[ \\n+ {\\int }_{\\left\\lbrack {s}_{2} < {s}_{1} < {s}_{3}\\right\\rbrack }{A}_{2}\\left( {s}_{3}\\right) {A}_{1}\\left( {s}_{1}\\right) {A}_{1}\\left( {s}_{2}\\right) \\left( {{\\mu }_{1}^{2} \\times {\\mu }_{2}}\\right) \\left( {d{s}_{1}, d{s}_{2}, d{s}_{3}}\\right) \\n\\]\\n\\n\\[ \\n+ {\\int }_{\\left\\lbrack {s}_{2} < {s}_{3} < {s}_{1}\\right\\rbrack }{A}_{1}\\left( {s}_{1}\\right) {A}_{2}\\left( {s}_{3}\\right) {A}_{1}\\left( {s}_{2}\\right) \\left( {{\\mu }_{1} \\times {\\mu }_{2} \\times {\\mu }_{1}}\\right) \\left( {d{s}_{1}, d{s}_{2}, d{s}_{3}}\\right) \\n\\]\\n\\n\\[ \\n+ {\\int }_{\\left\\lbrack {s}_{3} < {s}_{1} < {s}_{2}\\right\\rbrack }{A}_{1}\\left( {s}_{2}\\right) {A}_{1}\\left( {s}_{1}\\right) {A}_{2}\\left( {s}_{3}\\right) \\left( {{\\mu }_{2} \\times {\\mu }_{1}^{2}}\\right) \\left( {d{s}_{1}, d{s}_{2}, d{s}_{3}}\\right) \\n\\]\\n\\n\\[ \\n+ {\\int }_{\\left\\lbrack {s}_{3} < {s}_{2} < {s}_{1}\\right\\rbrack }{A}_{1}\\left( {s}_{1}\\right) {A}_{1}\\left( {s}_{2}\\right) {A}_{2}\\left( {s}_{3}\\right) \\left( {{\\mu }_{2} \\times {\\mu }_{1}^{2}}\\right) \\left( {d{s}_{1}, d{s}_{2}, d{s}_{3}}\\right) \\n\\]\\n\\n\\[ \\n= {A}_{2}{A}_{1}^{2}\\left( {{\\mu }_{1}^{2} \\times {\\mu }_{2}}\\right) \\left( \\left\\lbrack {{s}_{1} < {s}_{2} < {s}_{3}}\\right\\rbrack \\right) \\n\\]\\n\\n\\[ \\n+ {A}_{1}{A}_{2}{A}_{1}\\left( {{\\mu }_{1} \\times {\\mu }_{2} \\times {\\mu }_{1}}\\right) \\left( \\left\\lbrack {{s}_{1} < {s}_{3} < {s}_{2}}\\right\\rbrack \\right) \\n\\]\\n\\n\\[ \\n+ {A}_{2}{A}_{1}^{2}\\left( {{\\mu }_{1}^{2} \\times {\\mu }_{2}}\\right) \\left( \\left\\lbrack {{s}_{2} < {s}_{1} < {s}_{3}}\\right\\rbrack \\right) \\n\\]\\n\\n\\[ \\n+ {A}_{1}{A}_{2}{A}_{1}\\left( {{\\mu }_{1} \\times {\\mu }_{2} \\times {\\mu }_{1}}\\right) \\left( \\left\\lbrack {{s}_{2} < {s}_{3} < {s}_{1}}\\right\\rbrack \\right) \\n\\]\\n\\n\\[ \\n+ {A}_{1}^{2}{A}_{2}\\left( {{\\mu }_{2} \\times {\\mu }_{1}^{2}}\\right) \\left( \\left\\lbrack {{s}_{3} < {s}_{1} < {s}_{2}}\\right\\rbrack \\right) \\n\\]\\n\\n\\[ \\n+ {A}_{1}^{2}{A}_{2}\\left( {{\\mu }_{2} \\times {\\mu }_{1}^{2}}\\right) \\left( \\left\\lbrack {{s}_{3} < {s}_{2} < {s}_{1}}\\right\\rbrack \\right) . \\tag{2.3.26} \\n\\]",
        "id": "college_math_283229"
    },
    {
        "informal_statement": "Example 9.8 The set of irrational reals is uncountable.",
        "informal_proof": "The set of reals is the union of the set of rationals and the set of irrationals. The set of rationals is countable. If the set of irrationals were countable, then the union of rationals and irrationals would be countable as well (since the union of two countable sets is countable). The reals would be countable. That is not true. Hence, the set of irrationals is uncountable.",
        "id": "college_math_232401"
    },
    {
        "informal_statement": "Theorem 7.8 (Transition function and jump rates) For any $x, y \\in \\Psi$ the transition function $t \\rightarrow {p}_{t}\\left( {x, y}\\right)$ as a function of $t \\geq 0$ is differentiable and the derivative is continuous. The derivative at 0 is given by\\n\\n$$ A\\left( {x, y}\\right) = {\\left. \\frac{d}{dt}{p}_{t}\\left( x, y\\right) \\right| }_{t = 0} = \\left\\{ \\begin{array}{ll} - \\lambda \\left( x\\right) & \\text{ if }x = y, \\\\ \\lambda \\left( x\\right) q\\left( {x, y}\\right) & \\text{ if }x \\neq y. \\end{array}\\right. \\tag{7.37} $$",
        "informal_proof": "Furthermore, for any $t \\geq 0$ we have\\n\\n$$ \\frac{d}{dt}{p}_{t}\\left( {x, y}\\right) = \\mathop{\\sum }\\limits_{{z \\in \\Psi }}A\\left( {x, z}\\right) {p}_{t}\\left( {z, y}\\right) = \\mathop{\\sum }\\limits_{{z \\in \\Psi }}{p}_{t}\\left( {x, z}\\right) A\\left( {z, y}\\right) . \\tag{7.38} $$",
        "id": "college_math_129990"
    },
    {
        "informal_statement": "Theorem 4.16 Let $\\\\bar{x}$ be an optimal solution to $\\\\left( P\\\\right)$ . Then there are multipliers ${\\\\lambda }_{0} \\\\geq 0$ and $\\\\lambda = \\\\left( {{\\\\lambda }_{1},\\\\ldots ,{\\\\lambda }_{m}}\\\\right) \\\\in {\\\\mathbb{R}}^{m}$, not equal to zero simultaneously, such that ${\\\\lambda }_{i} \\\\geq 0$ as $i = 0,\\\\ldots, m$ ,\\n\\n$$ 0 \\\\in {\\\\lambda }_{0}\\\\partial f\\\\left( \\\\bar{x}\\\\right) + \\\\mathop{\\\\sum }\\\\limits_{{i = 1}}^{m}{\\\\lambda }_{i}\\\\partial {g}_{i}\\\\left( \\\\bar{x}\\\\right) + N\\\\left( {\\\\bar{x};\\\\Omega }\\\\right) , $$\\n\\nand ${\\\\lambda }_{i}{g}_{i}\\\\left( \\\\bar{x}\\\\right) = 0$ for all $i = 1,\\\\ldots, m$ .",
        "informal_proof": "Proof. Consider the finite convex function\\n\\n$$ \\\\varphi \\\\left( x\\\\right) \\\\mathrel{\\\\text{:=}} \\\\max \\\\left\\{ {f\\\\left( x\\\\right) - f\\\\left( \\\\bar{x}\\\\right) ,{g}_{i}\\\\left( x\\\\right) \\\\mid i = 1,\\\\ldots, m}\\\\right\\} $$\\n\\nand observe that $\\\\bar{x}$ solves the following problem with no functional constraints:\\n\\n$$ \\\\text{minimize}\\\\varphi \\\\left( x\\\\right) \\\\text{subject to}x \\\\in \\\\Omega \\\\text{.} $$\\n\\nSince $\\\\varphi$ is finite and hence locally Lipschitz continuous, we get from Theorem 4.14 that $0 \\\\in$ $\\\\partial \\\\varphi \\\\left( \\\\bar{x}\\\\right) + N\\\\left( {\\\\bar{x};\\\\Omega }\\\\right)$ . The subdifferential maximum rule from Proposition 2.54 yields\\n\\n$$ 0 \\\\in \\\\operatorname{co}\\\\left\\\\lbrack {\\\\partial f\\\\left( \\\\bar{x}\\\\right) \\\\cup \\\\left\\\\lbrack {\\\\mathop{\\\\bigcup }\\\\limits_{{i \\\\in I\\\\left( \\\\bar{x}\\\\right) }}\\\\partial {g}_{i}\\\\left( \\\\bar{x}\\\\right) }\\\\right\\\\rbrack }\\\\right\\\\rbrack + N\\\\left( {\\\\bar{x};\\\\Omega }\\\\right) . $$\\n\\nThis gives us ${\\\\lambda }_{0} \\\\geq 0$ and ${\\\\lambda }_{i} \\\\geq 0$ for $i \\\\in I\\\\left( \\\\bar{x}\\\\right)$ such that ${\\\\lambda }_{0} + \\\\mathop{\\\\sum }\\\\limits_{{i \\\\in I\\\\left( \\\\bar{x}\\\\right) }}{\\\\lambda }_{i} = 1$ and\\n\\n$$ 0 \\\\in {\\\\lambda }_{0}\\\\partial f\\\\left( \\\\bar{x}\\\\right) + \\\\mathop{\\\\sum }\\\\limits_{{i \\\\in I\\\\left( \\\\bar{x}\\\\right) }}{\\\\lambda }_{i}\\\\partial {g}_{i}\\\\left( \\\\bar{x}\\\\right) + N\\\\left( {\\\\bar{x};\\\\Omega }\\\\right) . \\\\tag{4.7} $$\\n\\nLetting ${\\\\lambda }_{i} \\\\mathrel{\\\\text{:=}} 0$ for $i \\\\notin I\\\\left( \\\\bar{x}\\\\right)$, we get ${\\\\lambda }_{i}{g}_{i}\\\\left( \\\\bar{x}\\\\right) = 0$ for all $i = 1,\\\\ldots, m$ with $\\\\left( {{\\\\lambda }_{0},\\\\lambda }\\\\right) \\\\neq 0$ .",
        "id": "college_math_17623"
    },
    {
        "informal_statement": "Theorem 3.1.23 (Ado) Every finite-dimensional Lie algebra over a field of characteristic 0 has a faithful finite-dimensional representation.",
        "informal_proof": "Proof Suppose that we are able to construct a finite-dimensional representation \\( \\rho \\) from \\( L \\) to \\( {gl}\\left( V\\right) \\) such that the restriction \\( {\\left. \\rho \\right| }_{Z\\left( L\\right) } \\) of \\( \\rho \\) on \\( Z\\left( L\\right) \\) is faithful. Then the representation \\( \\widehat{\\rho } = \\rho \\oplus {ad} \\) from \\( L \\) to \\( {gl}\\left( {V \\oplus L}\\right) \\) is a finite-dimensional representation of \\( L \\) such that\\n\\n\\[ \\operatorname{Ker}\\widehat{\\rho } = \\{ x \\in L \\mid 0 = \\widehat{\\rho }\\left( x\\right) = \\rho \\left( x\\right) \\oplus \\operatorname{ad}\\left( x\\right) \\} = \\{ x \\in L \\mid x \\in Z\\left( L\\right) \\text{ and }\\rho \\left( x\\right) = 0\\} = \\{ 0\\} . \\]\\n\\nThis means that \\( \\widehat{\\rho } \\) is a faithful finite-dimensional representation of \\( L \\) .\\n\\nThus, it is sufficient to construct a finite-dimensional representation \\( \\rho \\) of \\( L \\) which is faithful on \\( Z\\left( L\\right) \\) . Since \\( Z\\left( L\\right) \\subseteq N\\left( L\\right) \\), we have an ascending chain\\n\\n\\[ Z\\left( L\\right) = {N}_{1}\\left( L\\right) \\subset {N}_{2}\\left( L\\right) \\subset \\cdots \\subset {N}_{m}\\left( L\\right) = N\\left( L\\right) \\]\\n\\nsuch that \\( {N}_{i}\\left( L\\right) \\) is an ideal of \\( {N}_{i + 1}\\left( L\\right) \\) and \\( \\operatorname{Dim}{N}_{i + 1}\\left( L\\right) /{N}_{i}\\left( L\\right) = 1 \\) for each \\( i \\) . Suppose that the Dim \\( Z\\left( L\\right) = r \\) . Let \\( {W}_{1} = \\left\\{ {{x}_{1},{x}_{2},\\ldots ,{x}_{r},{x}_{r + 1}}\\right\\} > \\) be an \\( r + 1 \\) - dimensional vector space over \\( F \\) . Consider the linear transformation \\( {\\eta }_{1} \\) on \\( {W}_{1} \\) given by \\( {\\eta }_{1}\\left( {x}_{i}\\right) = {x}_{i + 1}, i \\leq r \\) and \\( {\\eta }_{1}\\left( {x}_{r + 1}\\right) = 0 \\) . Evidently, \\( {\\eta }_{1}^{r} \\neq 0 \\) but \\( {\\eta }_{1}^{r + 1} = 0 \\) . Clearly, \\( \\left\\{ {{\\eta }_{1},{\\eta }_{1}^{2},\\ldots {\\eta }_{1}^{r}}\\right\\} \\) is a linearly independent subset of \\( {End}{W}_{1} \\) . Let \\( {U}_{1} \\) denote the subspace of \\( \\operatorname{End}{W}_{1} \\) with basis \\( \\left\\{ {{\\eta }_{1},{\\eta }_{1}^{2},\\ldots {\\eta }_{1}^{r}}\\right\\} \\) . Evidently, \\( {U}_{1} \\) is an abelian Lie subalgebra of \\( {gl}\\left( {W}_{1}\\right) \\) which is isomorphic to the Lie subalgebra \\( Z\\left( L\\right) \\) of \\( L \\) . Thus, we get a faithful finite-dimensional representation \\( {\\rho }_{1} \\) of \\( Z\\left( L\\right) \\) on \\( {W}_{1} \\) . Since \\( {N}_{2}\\left( L\\right) \\) is nilpotent and \\( {N}_{2}\\left( L\\right) = Z\\left( L\\right) \\succ F{u}_{2} \\) for some \\( {u}_{2} \\in {N}_{2}\\left( L\\right) \\), from the previous theorem, we obtain a finite-dimensional representation \\( {\\rho }_{2} \\) of \\( {N}_{2}\\left( L\\right) \\) such that \\( {\\rho }_{2} \\) is faithful on \\( Z\\left( L\\right) \\) . Proceeding inductively, we obtain a finite-dimensional representation \\( {\\rho }_{r} \\) of \\( N\\left( L\\right) \\) such that \\( {\\left. \\rho \\right| }_{Z\\left( L\\right) } \\) is faithful. Again, we have an ascending chain\\n\\n\\[ N\\left( L\\right) = {R}_{1}\\left( L\\right) \\subseteq {R}_{2}\\left( L\\right) \\subseteq \\cdots \\subseteq {R}_{s}\\left( L\\right) = R\\left( L\\right) ,\\]\\n\\nwhere \\( {R}_{j}\\left( L\\right) \\) is an ideal of \\( {R}_{j + 1}\\left( L\\right) \\) and \\( \\operatorname{Dim}{R}_{j + 1}\\left( L\\right) /{R}_{j}\\left( L\\right) = 1 \\) for each \\( j \\) . Indeed, \\( N\\left( L\\right) = N\\left( {{R}_{j}\\left( L\\right) }\\right) \\) for each \\( j \\) . Starting with \\( {\\rho }_{r} \\) and using the previous theorem again and again, by the induction, we get a finite-dimensional representation \\( \\rho \\) of \\( R\\left( L\\right) \\) which agrees with \\( {\\rho }_{r} \\) on \\( N\\left( L\\right) \\) and so it is faithful when restricted to \\( Z\\left( L\\right) \\) . Further, by the Radical splitting theorem due to Levi (Theorem 1.5.16), \\( L = R\\left( L\\right) \\succ \\) \\( {L}_{1} \\), where \\( {L}_{1} ",
        "id": "college_math_212043"
    },
    {
        "informal_statement": "Theorem 2.13. Let \\( A \\) be an \\( m \\times n \\) matrix and \\( B \\) be an \\( n \\times p \\) matrix. For each \\( j\\left( {1 \\leq j \\leq p}\\right) \\) let \\( {u}_{j} \\) and \\( {v}_{j} \\) denote the \\( j \\) th columns of \\( {AB} \\) and \\( B \\) , respectively. Then\\n\\n(a) \\( {u}_{j} = A{v}_{j} \\)\\n\\n(b) \\( {v}_{j} = B{e}_{j} \\), where \\( {e}_{j} \\) is the \\( j \\) th standard vector of \\( {\\mathrm{F}}^{p} \\) .",
        "informal_proof": "Proof. (a) We have\\n\\n\\[ \\n{u}_{j} = \\left( \\begin{matrix} {\\left( AB\\right) }_{1j} \\\\ {\\left( AB\\right) }_{2j} \\\\ \\vdots \\\\ {\\left( AB\\right) }_{mj} \\end{matrix}\\right) = \\left( \\begin{matrix} \\mathop{\\sum }\\limits_{{k = 1}}^{n}{A}_{1k}{B}_{kj} \\\\ \\mathop{\\sum }\\limits_{{k = 1}}^{n}{A}_{2k}{B}_{kj} \\\\ \\vdots \\\\ \\mathop{\\sum }\\limits_{{k = 1}}^{n}{A}_{mk}{B}_{kj} \\end{matrix}\\right) = A\\left( \\begin{matrix} {B}_{1j} \\\\ {B}_{2j} \\\\ \\vdots \\\\ {B}_{nj} \\end{matrix}\\right) = A{v}_{j}. \\n\\]\\n\\nHence (a) is proved. The proof of (b) is left as an exercise. (See Exercise 6.)",
        "id": "college_math_171738"
    },
    {
        "informal_statement": "Theorem 71. Let $f \\\\in {L}^{2}\\\\left( {0,\\\\infty }\\\\right)$ , $$ \\\\operatorname{Re}\\\\left( {\\\\alpha + \\\\mu + 1}\\\\right) > 0,\\\\;\\\\operatorname{Re}\\\\left( {\\\\alpha + \\\\frac{\\\\mu - v}{2}}\\\\right) < 0. $$ Then for the transmutation operator ${T}_{v,\\\\mu }^{\\\\left( \\\\alpha \\\\right) }$ obtained by the ITCM and such that $$ {T}_{v,\\\\mu }^{\\\\left( \\\\alpha \\\\right) }{B}_{v} = {B}_{\\\\mu }{T}_{v,\\\\mu }^{\\\\left( \\\\alpha \\\\right) } $$ the following integral representation is true: $$ \\\\left( {{T}_{v,\\\\mu }^{\\\\left( \\\\alpha \\\\right) }f}\\\\right) \\\\left( x\\\\right) = C \\\\cdot \\\\frac{{2}^{\\\\alpha + 3}\\\\Gamma \\\\left( \\\\frac{\\\\alpha + \\\\mu + 1}{2}\\\\right) }{\\\\Gamma \\\\left( \\\\frac{\\\\mu + 1}{2}\\\\right) } \\\\times $$ $$ \\\\left\\\\lbrack {\\\\frac{{x}^{-1 - \\\\mu - \\\\alpha }}{\\\\Gamma \\\\left( {-\\\\frac{\\\\alpha }{2}}\\\\right) }{\\\\int }_{0}^{x}f{\\\\left( y\\\\right) }_{2}{F}_{1}\\\\left( {\\\\frac{\\\\alpha + \\\\mu + 1}{2},\\\\frac{\\\\alpha }{2} + 1;\\\\frac{v + 1}{2};\\\\frac{{y}^{2}}{{x}^{2}}}\\\\right) {y}^{v}{dy} + }\\\\right. $$ $$ \\\\frac{\\\\Gamma \\\\left( \\\\frac{v + 1}{2}\\\\right) }{\\\\Gamma \\\\left( \\\\frac{\\\\mu + 1}{2}\\\\right) \\\\Gamma \\\\left( \\\\frac{v - \\\\mu - \\\\alpha }{2}\\\\right) }{\\\\int }_{x}^{\\\\infty }f\\\\left( y\\\\right) \\\\times $$ $$ \\\\left. {{}_{2}{F}_{1}\\\\left( {\\\\frac{\\\\alpha + \\\\mu + 1}{2},\\\\frac{\\\\alpha + \\\\mu - v}{2} + 1;\\\\frac{\\\\mu + 1}{2};\\\\frac{{x}^{2}}{{y}^{2}}}\\\\right) {y}^{v - \\\\mu - \\\\alpha - 1}{dy}}\\\\right\\\\rbrack , $$ where ${}_{2}{F}_{1}$ is the Gauss hypergeometric function.",
        "informal_proof": "Proof. We have $$ \\\\left( {{T}_{v,\\\\mu }^{\\\\left( \\\\alpha \\\\right) }f}\\\\right) \\\\left( x\\\\right) = C \\\\cdot {H}_{\\\\mu }^{-1}\\\\left\\\\lbrack {{t}^{\\\\alpha }{H}_{v}\\\\left\\\\lbrack f\\\\right\\\\rbrack \\\\left( t\\\\right) }\\\\right\\\\rbrack \\\\left( x\\\\right) = $$ $$ C \\\\cdot \\\\frac{{2}^{1 - \\\\mu }}{{\\\\Gamma }^{2}\\\\left( \\\\frac{\\\\mu + 1}{2}\\\\right) }{\\\\int }_{0}^{\\\\infty }{j}_{\\\\frac{\\\\mu - 1}{2}}\\\\left( {xt}\\\\right) {t}^{\\\\mu + \\\\alpha }{dt}{\\\\int }_{0}^{\\\\infty }{j}_{\\\\frac{v - 1}{2}}\\\\left( {ty}\\\\right) f\\\\left( y\\\\right) {y}^{v}{dy} = $$ $$ C \\\\cdot \\\\frac{{2}^{\\\\frac{v - \\\\mu }{2} + 2}\\\\Gamma \\\\left( \\\\frac{v + 1}{2}\\\\right) }{\\\\Gamma \\\\left( \\\\frac{\\\\mu + 1}{2}\\\\right) }{\\\\int }_{0}^{\\\\infty }{\\\\left( xt\\\\right) }^{\\\\frac{1 - \\\\mu }{2}}{J}_{\\\\frac{\\\\mu - 1}{2}}\\\\left( {xt}\\\\right) {t}^{\\\\mu + \\\\alpha }{dt}{\\\\int }_{0}^{\\\\infty }{\\\\left( ty\\\\right) }^{\\\\frac{1 - v}{2}}{J}_{\\\\frac{v - 1}{2}}\\\\left( {ty}\\\\right) f\\\\left( y\\\\right) {y}^{v}{dy} = $$ $$ C \\\\cdot \\\\frac{{2}^{\\\\frac{v - \\\\mu }{2} + 2}\\\\Gamma \\\\left( \\\\frac{v + 1}{2}\\\\right) }{\\\\Gamma \\\\left( \\\\frac{\\\\mu + 1}{2}\\\\right) }{x}^{\\\\frac{1 - \\\\mu }{2}{\\\\int }_{0}^{\\\\infty }{y}^{\\\\frac{v + 1}{2}}f\\\\left( y\\\\right) {dy}{\\\\int }_{0}^{\\\\infty }{t}^{\\\\alpha + 1 + \\\\frac{\\\\mu - v}{2}}{J}_{\\\\frac{\\\\mu - 1}{2}}\\\\left( {xt}\\\\right) {J}_{\\\\frac{v - 1}{2}}\\\\left( {ty}\\\\right) {dt} = $$ $$ C \\\\cdot \\\\frac{{2}^{\\\\frac{v - \\\\mu }{2} + 2}\\\\Gamma \\\\left( \\\\frac{v + 1}{2}\\\\right) }{\\\\Gamma \\\\left( \\\\frac{\\\\mu + 1}{2}\\\\right) }{x}^{\\\\frac{1 - \\\\mu }{2}{\\\\int }_{0}^{x}{y}^{\\\\frac{v + 1}{2}}f\\\\left( y\\\\right) {dy}{\\\\int }_{0}^{\\\\infty }{t}^{\\\\alpha + 1 + \\\\frac{\\\\mu - v}{2}}{J}_{\\\\frac{\\\\mu - 1}{2}}\\\\left( {xt}\\\\right) {J}_{\\\\frac{v - 1}{2}}\\\\left( {ty}\\\\right) {dt} + $$ $$ C \\\\cdot \\\\frac{{2}^{\\\\frac{v - \\\\mu }{2} + 2}\\\\Gamma \\\\left( \\\\frac{v + 1}{2}\\\\right) }{\\\\Gamma \\\\left( \\\\frac{\\\\mu + 1}{2}\\\\right) }{x}^{\\\\frac{1 - \\\\mu }{2}{\\\\int }_{x}^{\\\\infty }{y}^{\\\\frac{v + 1}{2}}f\\\\left( y\\\\right) {dy}{\\\\int }_{0}^{\\\\infty }{t}^{\\\\alpha + 1 + \\\\frac{\\\\mu - v}{2}}{J}_{\\\\frac{\\\\mu - 1}{2}}\\\\left( {xt}\\\\right) {J}_{\\\\frac{v - 1}{2}}\\\\left( {ty}\\\\right) {dt}. $$ Using formula (2.12.31.1) from [456], p. 209, of the form $$ {\\\\int }_{0}^{\\\\infty }{t}^{\\\\beta - 1}{J}_{\\\\rho }\\\\left( {xt}\\\\right) {J}_{\\\\gamma }\\\\left( {yt}\\\\right) {dt} = $$ $$ \\\\left\\\\{ \\\\begin{array}{ll} {2}^{\\\\beta - 1}{x}^{-\\\\gamma - \\\\beta }{y}^{\\\\gamma }\\\\frac{\\\\Gamma \\\\left( \\\\frac{\\\\gamma + \\\\rho + \\\\beta }{2}\\\\right) }{\\\\Gamma \\\\left( {\\\\gamma + 1}\\\\right) \\\\Gamma \\\\left( {\\\\frac{\\\\rho - \\\\gamma - \\\\beta }{2} + 1}\\\\right) }{}_{2}{F}_{1}\\\\left( {\\\\frac{\\\\gamma + \\\\rho + \\\\beta }{2},\\\\frac{\\\\gamma - \\\\rho + \\\\beta }{2};\\\\gamma + 1;\\\\frac{{y}^{2}}{{x}^{2}}}\\\\right) & 0 < y < x, \\\\\\\\ {2}^{\\\\beta - 1}{x}^{\\\\rho }{y}^{-\\\\rho - \\\\beta }\\\\frac{\\\\Gamma \\\\left( \\\\frac{\\\\gamma + \\\\rho + \\\\beta }{2}\\\\right) }{\\\\Gamma \\\\left",
        "id": "college_math_142096"
    },
    {
        "informal_statement": "Lemma 2.1.6. Let \\( M = \\langle X, Y, S,\\delta ,\\lambda \\rangle \\) be weakly invertible with delay \\( r - 1 \\) and \\( \\left| X\\right| = \\left| Y\\right| = q \\) . Then \\( {w}_{r, M}^{\\prime \\prime } = {q}^{r}/{w}_{r, M} \\) .",
        "informal_proof": "Proof. The proof of this lemma is similar to Lemma 2.1.3 but replacing Lemma 2.1.2 and \\( {w}_{r, M}^{\\prime } \\) by Lemma 2.1.5 and \\( {w}_{r, M}^{\\prime \\prime } \\), respectively.",
        "id": "college_math_156547"
    },
    {
        "informal_statement": "How many cubic meters of blood does the heart pump in a 75-year lifetime, assuming the average flow rate is 5.00 Umin?",
        "informal_proof": "Strategy\\n\\nTime and flow rate $Q$ are given, and so the volume $V$ can be calculated from the definition of flow rate.\\n\\nSolution\\n\\nSolving $Q = V/t$ for volume gives\\n\\n$$ V = {Qt} \\\\tag{12.2} $$\\n\\nSubstituting known values yields\\n\\n$$ V = \\\\left( \\\\frac{{5.00}\\\\mathrm{\\\\;L}}{1\\\\mathrm{\\\\;{min}}}\\\\right) \\\\left( {{75}\\\\mathrm{y}}\\\\right) \\\\left( \\\\frac{1{\\\\mathrm{\\\\;m}}^{3}}{{10}^{3}\\\\mathrm{\\\\;L}}\\\\right) \\\\left( {{5.26} \\\\times {10}^{5}\\\\frac{\\\\mathrm{{min}}}{\\\\mathrm{y}}}\\\\right) \\\\tag{12.3} $$\\n\\n$$ = {2.0} \\\\times {10}^{5}{\\\\mathrm{\\\\;m}}^{3}\\\\text{.} $$",
        "id": "college_math_79001"
    },
    {
        "informal_statement": "Proposition 6.19. Suppose \\( \\Omega \\subset \\mathcal{M} \\) is an Ahlfors regular domain with outward unit conormal \\( v \\) and surface measure \\( \\sigma = {\\mathcal{H}}^{n - 1}\\lfloor \\partial \\Omega \\), and fix an arbitrary degree \\( l \\in \\{ 0,1,\\ldots, n\\} \\) along with some \\( p \\in \\left( {1,\\infty }\\right) \\) . In this setting, let \\( u, v \\) be two differential forms satisfying (with \\( {p}^{\\prime } \\in \\left( {1,\\infty }\\right) \\) such that \\( 1/p + 1/{p}^{\\prime } = 1 \\) )",
        "informal_proof": "Proof. Let \\( u \\) and \\( v \\) be as in (6.3.58) and (6.3.59), respectively, and such that the nontangential traces mentioned in the statement exist \\( \\sigma \\) -a.e. on \\( \\partial \\Omega \\) . From these, we manufacture a vector field \\( \\overrightarrow{F} : \\Omega \\rightarrow T\\mathcal{M} \\) by asking that for all \\( x \\in \\Omega \\) and all \\( \\xi \\in {T}_{x}^{ * }\\mathcal{M} \\) we have",
        "id": "college_math_194831"
    },
    {
        "informal_statement": "Theorem 18.7 Let $f$ and $g$ be real-valued functions of a real variable. Let $\\\\mathrm{A} \\\\subseteq \\\\mathbf{R}$ be the domain of $f$ and $\\\\mathrm{B} \\\\subseteq \\\\mathbf{R}$ the domain of $g$, such that $f\\\\left( \\\\mathrm{\\\\;A}\\\\right) \\\\subseteq \\\\mathrm{B}$ . Let $c \\\\in \\\\mathbf{R}$ be an accumulation point for $A$ . If\\n\\n$$ \\n\\\\mathop{\\\\lim }\\\\limits_{{x \\\\rightarrow c}}f\\\\left( x\\\\right) = \\\\ell \\\\tag{18.12}\\n$$ \\n\\n$\\\\ell \\\\in \\\\mathbf{R}$, and there exists a neighborhood $H$ of $c$, such that for every $x \\\\in \\\\mathrm{H} \\\\cap \\\\mathrm{A}, x \\\\neq$ c, it results\\n\\n$$ \\nf\\\\left( x\\\\right) \\\\neq \\\\ell \\\\tag{18.13}\\n$$ \\n\\nIf\\n\\n$$ \\n\\\\mathop{\\\\lim }\\\\limits_{{y \\\\rightarrow \\\\ell }}g\\\\left( y\\\\right) = h \\\\tag{18.14}\\n$$ \\n\\n$h \\\\in \\\\mathbf{R}$, then\\n\\n$$ \\n\\\\mathop{\\\\lim }\\\\limits_{{x \\\\rightarrow c}}g\\\\left( {f\\\\left( x\\\\right) }\\\\right) = h \\\\tag{18.15}\\n$$",
        "informal_proof": "Proof Let us first observe that the composite function $g\\\\left( {f\\\\left( x\\\\right) }\\\\right)$ has meaning (Sect. 5.4). Let $\\\\mathrm{M}$ be a neighborhood of $h$ . Then, by (18.14), there exists a neighborhood $\\\\mathrm{J}$ of $\\\\ell$ such that for every $y$ belonging to $\\\\left( {\\\\mathrm{J} \\\\cap \\\\mathrm{B}}\\\\right) - \\\\{ \\\\ell \\\\}$, it is $g\\\\left( y\\\\right) \\\\in \\\\mathrm{M}$ ; then assigned the neighborhood J, from (18.12) there exists a neighborhood ${\\\\mathrm{I}}^{\\\\prime }$ of $c$, such that for every $x \\\\in \\\\left( {\\\\mathrm{A} \\\\cap {\\\\mathrm{I}}^{\\\\prime }}\\\\right) - \\\\{ c\\\\}$ it is $f\\\\left( x\\\\right) \\\\in \\\\mathrm{J}$ . Since (18.13) holds for every $x \\\\in (\\\\mathrm{H} \\\\cap$ A) $- \\\\{ c\\\\}$, set $\\\\mathrm{I} = {\\\\mathrm{I}}^{\\\\prime } \\\\cap \\\\mathrm{H}$, for every $x \\\\in \\\\left( {\\\\mathrm{A} \\\\cap D}\\\\right) - \\\\{ \\\\mathrm{c}\\\\}$ we have $f\\\\left( x\\\\right) \\\\in \\\\left( {\\\\mathrm{J} \\\\cap f\\\\left( \\\\mathrm{\\\\;A}\\\\right) }\\\\right) - \\\\{ \\\\ell \\\\}$ , so $g\\\\left( {f\\\\left( x\\\\right) }\\\\right) \\\\in \\\\mathrm{M}$ and (18.15) holds. (Observe that the intersection of two neighborhoods of $c,\\\\mathrm{I} = {\\\\mathrm{I}}^{\\\\prime } \\\\cap \\\\mathrm{H}$, is a neighborhood of $c$ ; and that if $x \\\\in \\\\left( {\\\\mathrm{H} \\\\cap \\\\mathrm{A}}\\\\right) - \\\\{ c\\\\}$, then $0 < \\\\left| {f\\\\left( x\\\\right) - \\\\ell }\\\\right| )$ .",
        "id": "college_math_38378"
    },
    {
        "informal_statement": "Example 6 (Reversible Jump - MCMC). As a illustrative toy example, we define a target density on the union of two spaces $\\\\mathcal{E} = {\\\\mathcal{E}}_{1} \\\\cup {\\\\mathcal{E}}_{2}$ where\\n\\n$${\\\\mathcal{E}}_{1} = \\\\{ \\\\{ 1\\\\} \\\\times r\\\\} \\\\;{\\\\mathcal{E}}_{2} = \\\\{ \\\\{ 2\\\\} \\\\times \\\\left( {x, y}\\\\right) \\\\}$\\n\\nwith parameters corresponding to each space as\\n\\n$${\\\\theta }_{1} \\\\equiv r\\\\;{\\\\theta }_{2} \\\\equiv \\\\left( {x, y}\\\\right)$\\n\\nWe choose, for $k = 1$, a target proportional to a truncated exponential distribution. For $k = 2$, our target is a two dimensional Gaussian density truncated on an elliptical region\\n\\n$${\\\\phi }_{1}\\\\left( r\\\\right) = {e}^{-{\\\\lambda r}}\\\\left\\\\lbrack {0 \\\\leq r \\\\leq 1}\\\\right\\\\rbrack$\\n\\n$${\\\\phi }_{2}\\\\left( {x, y}\\\\right) = \\\\exp \\\\left( {-\\\\frac{{\\\\left( x - {\\\\mu }_{x}\\\\right) }^{2} + {\\\\left( y - {\\\\mu }_{y}\\\\right) }^{2}}{2{\\\\sigma }^{2}}}\\\\right) \\\\left\\\\lbrack {{\\\\left( \\\\frac{x}{a}\\\\right) }^{2} + {\\\\left( \\\\frac{y}{b}\\\\right) }^{2} \\\\leq 1}\\\\right\\\\rbrack$",
        "informal_proof": "Algorithm 6. Reversible Jump Markov Chain Monte Carlo\\n\\n1:Initialise $\\\\left( {{k}^{\\\\left( 1\\\\right) },{\\\\theta }_{\\\\left( 1\\\\right) }^{\\\\left( 1\\\\right) }}\\\\right)$ arbitrarily\\n\\nfor $\\\\tau = 2,3,\\\\ldots$ do\\n\\nLet $\\\\left( {k,{\\\\theta }_{k}}\\\\right) = \\\\left( {{k}^{\\\\left( \\\\tau - 1\\\\right) },{\\\\theta }_{{k}^{\\\\left( \\\\tau - 1\\\\right) }}^{\\\\left( \\\\tau - 1\\\\right) }}\\\\right)$\\n\\nPropose a move type: ${k}^{\\\\text{new }} \\\\sim {q}_{k \\\\rightarrow {k}^{\\\\prime }}$\\n\\nif $k = {k}^{\\\\text{new }}$, a ordinary move in the same space then\\n\\n${\\\\theta }^{\\\\text{new }} \\\\sim {q}_{k}\\\\left( {{\\\\theta }_{k} \\\\mid {\\\\theta }_{k}^{\\\\left( \\\\tau - 1\\\\right) }}\\\\right)$\\n\\nend if\\n\\nif $k < {k}^{\\\\text{new }}$, a transdimensional move to a larger space then\\n\\nPropose $u \\\\sim {q}_{k}\\\\left( {u}_{k}\\\\right)$\\n\\n${\\\\theta }^{\\\\text{new }} \\\\leftarrow {g}_{k \\\\rightarrow {k}^{\\\\text{new }}}\\\\left( {{\\\\theta }_{k}, u}\\\\right)$\\n\\nCompute the acceptance probability\\n\\n$\\\\alpha \\\\left( {\\\\left( {k,{\\\\theta }_{k}}\\\\right) \\\\rightarrow \\\\left( {{k}^{\\\\text{new }},{\\\\theta }^{\\\\text{new }}}\\\\right) }\\\\right) = \\\\min \\\\left\\\\{ {1,\\\\frac{{\\\\phi }_{{k}^{\\\\text{new }}}\\\\left( {\\\\theta }_{{k}^{\\\\text{new }}}\\\\right) {p}_{{k}^{\\\\text{new }}}}{{\\\\phi }_{k}\\\\left( {\\\\theta }_{k}\\\\right) {p}_{k}}\\\\frac{{q}_{{k}^{\\\\text{new }} \\\\rightarrow k}}{{q}_{k}\\\\left( u\\\\right) {q}_{k \\\\rightarrow {k}^{\\\\text{new }}}}\\\\left| {J}_{k \\\\rightarrow {k}^{\\\\text{new }}}\\\\right| }\\\\right\\\\}$\\n\\nend if\\n\\nif $k > {k}^{\\\\text{new }}$, a transdimensional move to a smaller space then\\n\\n$\\\\left( {{\\\\theta }^{\\\\text{new }}, u}\\\\right) \\\\leftarrow {g}_{k \\\\rightarrow {k}^{\\\\text{new }}}\\\\left( {\\\\theta }_{k}\\\\right)$\\n\\nCompute the acceptance probability\\n\\n$\\\\alpha \\\\left( {\\\\left( {k,{\\\\theta }_{k}}\\\\right) \\\\rightarrow \\\\left( {{k}^{\\\\text{new }},{\\\\theta }^{\\\\text{new }}}\\\\right) }\\\\right) = \\\\min \\\\left\\\\{ {1,\\\\frac{{\\\\phi }_{k}\\\\operatorname{new}\\\\left( {{\\\\theta }_{k}\\\\text{ new }}\\\\right) {p}_{k}\\\\text{ new }}{{\\\\phi }_{k}\\\\left( {\\\\theta }_{k}\\\\right) {p}_{k}}\\\\frac{{q}_{k}\\\\operatorname{new}\\\\left( u\\\\right) {q}_{k}\\\\text{ new } \\\\rightarrow k}{{q}_{k \\\\rightarrow k}}\\\\left| {J}_{k \\\\rightarrow {k}^{\\\\text{new }}}\\\\right| }\\\\right\\\\}$\\n\\nend if\\n\\nSample from uniform distribution: $a \\\\sim \\\\mathcal{U}\\\\left( {0,1}\\\\right)$\\n\\nif $a < \\\\alpha \\\\left( {\\\\left( {k,{\\\\theta }_{k}}\\\\right) \\\\rightarrow \\\\left( {{k}^{\\\\text{new }},{\\\\theta }^{\\\\text{new }}}\\\\right) }\\\\right)$ then\\n\\nAccept candidate: $\\\\left( {{k}^{\\\\left( \\\\tau \\\\right) },{\\\\theta }_{\\\\left( \\\\tau \\\\right) }^{\\\\left( \\\\tau \\\\right) }}\\\\right) \\\\leftarrow \\\\left( {{k}^{\\\\text{new }},{\\\\theta }^{\\\\text{new }}}\\\\right)$\\n\\nelse\\n\\nReject candidate and stay put: $\\\\left( {{k}^{(\\\\left( \\\\tau \\\\right) },{\\\\theta }_{{k}^{\\\\left( \\\\tau \\\\right) }}^{\\\\left( \\\\tau \\\\right) }}\\\\right) \\\\leftarrow \\\\left( {{k}^{\\\\left( \\\\tau - 1\\\\right) },{\\\\theta }_{{k}^{\\\\left( \\\\tau - 1\\\\right) }}^{\\\\left( \\\\tau - 1\\\\right) }}\\\\right)$\\n\\nend if\\n\\n23: end for",
        "id": "college_math_114677"
    },
    {
        "informal_statement": "Lemma 3.17. If $\\sum {u}_{n}$ and $\\sum {v}_{n}$ are absolutely convergent series and\\n\\n$$ \\n{w}_{n} = \\mathop{\\sum }\\limits_{{p = 0}}^{n}{u}_{p}{v}_{n - p}\\n$$ \\n\\nthen $\\sum {w}_{n}$ is absolutely convergent and $\\sum {w}_{n} = \\left( {\\sum {u}_{n}}\\right) \\left( {\\sum {v}_{n}}\\right)$ .",
        "informal_proof": "Proof. Let ${\\alpha }_{p} = \\mathop{\\sum }\\limits_{{n \\geq p}}\\left| {u}_{n}\\right|$ and ${\\beta }_{p} = \\mathop{\\sum }\\limits_{{n \\geq p}}\\left| {v}_{n}\\right|$ . Then\\n\\n$$ \\n\\mathop{\\lim }\\limits_{p}{\\alpha }_{p} = 0 = \\mathop{\\lim }\\limits_{p}{\\beta }_{p} \\tag{3.2}\\n$$ \\n\\nAlso\\n\\n$$ \\n\\mathop{\\sum }\\limits_{{n = 0}}^{N}\\left| {w}_{n}\\right| \\leq \\mathop{\\sum }\\limits_{{n = 0}}^{\\infty }\\left| {u}_{n}\\right| \\mathop{\\sum }\\limits_{{n = 0}}^{\\infty }\\left| {v}_{n}\\right| = {\\alpha }_{0}{\\beta }_{0} < + \\infty ,\\n$$ \\n\\nand therefore $\\mathop{\\sum }\\limits_{{n = 0}}^{\\infty }\\left| {w}_{n}\\right| < + \\infty$ . Thus we have proven the absolute convergence of the new series $\\sum {w}_{n}$ .\\n\\nTo show the required equality, choose $m$ and $n$ with $m \\geq {2n}$ and consider\\n\\n$$ \\n\\left| {\\mathop{\\sum }\\limits_{{k = 0}}^{m}{w}_{k} - \\left( {\\mathop{\\sum }\\limits_{{k = 0}}^{n}{u}_{k}}\\right) \\left( {\\mathop{\\sum }\\limits_{{k = 0}}^{n}{v}_{k}}\\right) }\\right| = \\mathfrak{L}.\\n$$ \\n\\nWe have to show that $\\mathfrak{L} \\rightarrow 0$ as $n \\rightarrow \\infty$ (we already know each of the above series converges). We rewrite\\n\\n$$ \\n\\mathfrak{L} = \\left| {\\mathop{\\sum }\\limits_{{k = 0}}^{m}\\mathop{\\sum }\\limits_{{i = 0}}^{k}{u}_{i}{v}_{k - i} - \\mathop{\\sum }\\limits_{{j = 0}}^{n}\\mathop{\\sum }\\limits_{{i = 0}}^{n}{u}_{i}{v}_{j}}\\right| .\\n$$ \\n\\nBy looking at the diagrams in the $\\left( {i, k}\\right)$ plane shown in Fig. 3.1, we see that\\n\\n$$ \\n\\mathop{\\sum }\\limits_{{k = 0}}^{m}\\mathop{\\sum }\\limits_{{i = 0}}^{k}{u}_{i}{v}_{k - i} = \\mathop{\\sum }\\limits_{{i = 0}}^{m}\\mathop{\\sum }\\limits_{{k = i}}^{m}{u}_{i}{v}_{k - i} \\tag{3.3}\\n$$ \\n\\n$$ \\n= \\mathop{\\sum }\\limits_{{i = 0}}^{m}{u}_{i}\\mathop{\\sum }\\limits_{{k = i}}^{m}{v}_{k - i} = \\mathop{\\sum }\\limits_{{i = 0}}^{m}{u}_{i}\\mathop{\\sum }\\limits_{{j = 0}}^{{m - i}}{v}_{j} = \\mathop{\\sum }\\limits_{{i = 0}}^{m}\\mathop{\\sum }\\limits_{{j = 0}}^{{m - i}}{u}_{i}{v}_{j}.\\n$$ \\n\\n![images/67.jpg?x=148&y=180&w=1047&h=506](images/67.jpg?x=148&y=180&w=1047&h=506)\\n\\nFig. 3.1 The $\\left( {i, k}\\right)$ plane. (a) The first sum in (3.3). (b) The second sum\\n\\nThus we can estimate\\n\\n$$ \\n\\mathfrak{L} = \\left| {\\mathop{\\sum }\\limits_{{i = 0}}^{n}\\left\\lbrack {\\mathop{\\sum }\\limits_{{j = 0}}^{{m - i}}{u}_{i}{v}_{j} - \\mathop{\\sum }\\limits_{{j = 0}}^{n}{u}_{i}{v}_{j}}\\right\\rbrack + \\mathop{\\sum }\\limits_{{i = n + 1}}^{m}\\mathop{\\sum }\\limits_{{j = 0}}^{{m - i}}{u}_{i}{v}_{j}}\\right| \\n$$ \\n\\n$$ \\n\\leq \\left| {\\mathop{\\sum }\\limits_{{i = 0}}^{n}\\mathop{\\sum }\\limits_{{j = n + 1}}^{{m - i}}{u}_{i}{v}_{j}}\\right| + \\left| {\\mathop{\\sum }\\limits_{{i = n + 1}}^{m}\\mathop{\\sum }\\limits_{{j = 0}}^{{m - i}}{u}_{i}{v}_{j}}\\right| \\n$$ \\n\\n$$ \\n\\leq \\mathop{\\sum }\\limits_{{i = 0}}^{\\infty }\\mathop{\\sum }\\limits_{{j = n + 1}}^{\\infty }\\left| {u}_{i}\\right| \\left| {v}_{j}\\right| + \\mathop{\\sum }\\limits_{{i = n + 1}}^{\\infty }\\mathop{\\sum }\\limits_{{j = 0}}^{\\infty }\\left| {u}_{i}\\right| \\left| {v}_{j}\\right| \\n$$ \\n\\n$$ \\n= {\\alpha }_{0}{\\beta }_{n + 1} + {\\beta }_{0}{\\alpha }_{n + 1} \\n$$ \\n\\nand the last expression approaches 0 as $n$ goes to $\\infty$, by (3.2).",
        "id": "college_math_46246"
    },
    {
        "informal_statement": "Example 8.4 A spherical shell of radius $R$ carries a uniformly distributed surface charge $\\sigma$ . The sphere is spinning at constant angular velocity $\\omega$ . Find the vector potential $\\mathbf{A}$ and magnetic field $\\mathbf{B}$ both inside and outside the sphere.",
        "informal_proof": "Solution To solve this problem we use a trick. Although it seems natural to align the $z$ axis along $\\mathbf{\\omega }$, as in Figure 8.4(a), the integral is easier to evaluate if we let ${\\mathbf{r}}_{1}$ lie on the $z$ axis; then $\\mathbf{\\omega }$ is tilted at an angle $\\psi$ on the ${xz}$ plane: see Figure 8.4(b). We explain the reasoning as follows. Because with such coordinates we have\\n\\n$${\\mathbf{r}}_{1} = {z}_{1}\\widehat{\\mathbf{z}},\\;{\\mathbf{r}}_{2} = R\\sin {\\theta }_{2}\\cos {\\phi }_{2}\\widehat{\\mathbf{x}} + R\\sin {\\theta }_{2}\\sin {\\phi }_{2}\\widehat{\\mathbf{y}} + R\\cos {\\theta }_{2}\\widehat{\\mathbf{z}},$$\\n\\nthen\\n\\n$${r}_{12} = \\sqrt{{R}^{2} + {z}_{1}^{2} - {2R}{z}_{1}\\cos {\\theta }_{2}}$$\\n\\nThis length is familiar as it appears repeatedly in integrals related to Coulomb's law. According to that experience, we choose to perform the integration in the (b) configuration.\\n\\nThe vector for angular velocity is\\n\\n$$\\mathbf{\\omega } = \\omega \\sin \\psi \\widehat{\\mathbf{x}} + \\omega \\cos \\psi \\widehat{\\mathbf{z}}$$\\n\\nhence the velocity ${\\mathbf{v}}_{2}$ is\\n\\n$${\\mathbf{v}}_{2} = \\mathbf{\\omega } \\times {\\mathbf{r}}_{2}$$\\n\\n$$= - {R\\omega }\\cos \\psi \\sin {\\theta }_{2}\\sin {\\phi }_{2}\\widehat{\\mathbf{x}} + {R\\omega }\\left( {\\cos \\psi \\sin {\\theta }_{2}\\cos {\\phi }_{2} - \\sin \\psi \\cos {\\theta }_{2}}\\right) \\widehat{\\mathbf{y}}$$\\n\\n$$+ {R\\omega }\\sin \\psi \\sin {\\theta }_{2}\\sin {\\phi }_{2}\\widehat{\\mathbf{z}}$$\\n\\nAgain, only ${A}_{y}$ is nonvanishing because the integrand for ${A}_{x}$ and ${A}_{z}$ contains $\\sin {\\phi }_{2}$ ,\\n\\n$${A}_{y} = \\frac{{\\mu }_{0}}{4\\pi }{\\int }_{0}^{2\\pi }d{\\phi }_{2}{\\int }_{0}^{\\pi }d{\\theta }_{2}\\frac{{\\sigma R\\omega }\\left( {\\cos \\psi \\sin {\\theta }_{2}\\cos {\\phi }_{2} - \\sin \\psi \\cos {\\theta }_{2}}\\right) {R}^{2}\\sin {\\theta }_{2}}{\\sqrt{{R}^{2} + {z}_{1}^{2} - {2R}{z}_{1}\\cos {\\theta }_{2}}}. \\tag{8.27}$$\\n\\nAlthough this integral appears formidable, Maple achieves the integration and delivers elegant results, depending on whether ${\\mathbf{r}}_{1}$ is inside or outside the sphere:\\n\\n$${A}_{y} = \\left\\{ \\begin{array}{ll} - \\frac{{\\mu }_{0}{R\\omega \\sigma }}{3}{z}_{1}\\sin \\psi , & r \\leq R, \\\\ - \\frac{{\\mu }_{0}{R}^{4}{\\omega \\sigma }}{3}\\frac{\\sin \\psi }{{z}_{2}^{2}}, & r \\geq R. \\end{array}\\right. \\tag{8.28}$$\\n\\nWorksheet 8.4 Like many preceding worksheets, we define the symbols in a systematic fashion and follow the formulas provided to perform the calculations. The Norm command is used as before, and the surface current density $\\mathrm{k}$ is obtained from the CrossProduct and ScalarMultiply commands. The int command serves to evaluate those formidable integrals.",
        "id": "college_math_93961"
    },
    {
        "informal_statement": "Theorem 11.2 Let \\( {\\delta }_{3} < {\\mu }_{3} \\) and\\n\\n\\[ \\n{\\delta }_{1} < {\\mu }_{1},\\;{\\delta }_{2} < \\frac{\\beta {S}^{ * }\\beta {I}^{ * }}{b{\\left( {S}^{ * }\\right) }^{-1} + \\beta {S}^{ * }} \\tag{11.31} \\n\\]\\n\\nor\\n\\n\\[ \\n{\\mu }_{1} \\leq {\\delta }_{1} < {\\mu }_{1} + \\beta {I}^{ * }\\frac{\\sqrt{4{I}^{ * }{\\left( {S}^{ * }\\right) }^{-1} + 1} - 1}{\\sqrt{4{I}^{ * }{\\left( {S}^{ * }\\right) }^{-1} + 1} + 1}, \\tag{11.32} \\n\\]\\n\\n\\[ \\n{\\delta }_{2} < \\frac{\\beta {S}^{ * }\\beta {I}^{ * }}{b{\\left( {S}^{ * }\\right) }^{-1} + \\beta {S}^{ * }}\\left( {1 - \\frac{\\beta {S}^{ * }\\left( {{\\delta }_{1} - {\\mu }_{1}}\\right) }{{\\left( b{\\left( {S}^{ * }\\right) }^{-1} - {\\delta }_{1}\\right) }^{2}}}\\right) .\\n\\]\\n\\nThen the trivial solution of (11.28) is asymptotically mean-square stable.",
        "informal_proof": "Proof Let \\( L \\) be the generator of the system of two first equations in (11.28). Following the procedure of constructing Lyapunov functionals, we will construct a Lya-punov functional \\( V \\) for this system in the form \\( V = {V}_{1} + {V}_{2} \\), where\\n\\n\\[ \\n{V}_{1} = {p}_{11}{y}_{1}^{2}\\left( t\\right) + 2{p}_{12}{y}_{1}\\left( t\\right) {y}_{2}\\left( t\\right) + {p}_{22}{y}_{2}^{2}\\left( t\\right) \\tag{11.33} \\n\\]\\n\\nand the parameters \\( {p}_{11},{p}_{12},{p}_{22} \\) by (1.29) are\\n\\n\\[ \\n{p}_{11} = \\frac{{S}^{ * }}{2b}q + \\frac{\\beta {I}^{ * }{S}^{ * }}{b}{p}_{12},\\;{p}_{22} = \\frac{1}{{2\\beta }{S}^{ * }},\\;{p}_{12} = \\frac{{I}^{ * }{\\left( {S}^{ * }\\right) }^{-1}}{2\\left( {b{\\left( {S}^{ * }\\right) }^{-1} + \\beta {S}^{ * }}\\right) }. \\tag{11.34} \\n\\]\\n\\nThen by (11.33), (11.28) we have\\n\\n\\[ \\nL{V}_{1} = 2\\left( {{p}_{11}{y}_{1}\\left( t\\right) + {p}_{12}{y}_{2}\\left( t\\right) }\\right) \\left( {-b{\\left( {S}^{ * }\\right) }^{-1}{y}_{1}\\left( t\\right) - \\beta {S}^{ * }J\\left( {y}_{2t}\\right) }\\right) + {p}_{11}{\\sigma }_{1}^{2}{y}_{1}^{2}\\left( t\\right) \\n\\]\\n\\n\\[ \\n+ 2\\left( {{p}_{12}{y}_{1}\\left( t\\right) + {p}_{22}{y}_{2}\\left( t\\right) }\\right) \\left( {\\beta {I}^{ * }{y}_{1}\\left( t\\right) - \\beta {S}^{ * }{y}_{2}\\left( t\\right) }\\right. \\n\\]\\n\\n\\[ \\n\\left. {+\\beta {S}^{ * }J\\left( {y}_{2t}\\right) }\\right) + {p}_{22}{\\sigma }_{2}^{2}{y}_{2}^{2}\\left( t\\right) \\n\\]\\n\\n\\[ \\n= 2\\left( {\\left( {{\\delta }_{1} - b{\\left( {S}^{ * }\\right) }^{-1}}\\right) {p}_{11} + \\beta {I}^{ * }{p}_{12}}\\right) {y}_{1}^{2}\\left( t\\right) + 2\\left( {{\\delta }_{2} - \\beta {S}^{ * }}\\right) {p}_{22}{y}_{2}^{2}\\left( t\\right) \\n\\]\\n\\n\\[ \\n+ 2\\left\\lbrack {\\beta {I}^{ * }{p}_{22} - \\left( {b{\\left( {S}^{ * }\\right) }^{-1} + \\beta {S}^{ * }}\\right) {p}_{12}}\\right\\rbrack {y}_{1}\\left( t\\right) {y}_{2}\\left( t\\right) \\n\\]\\n\\n\\[ \\n+ {2\\beta }{S}^{ * }\\left\\lbrack {\\left( {{p}_{12} - {p}_{11}}\\right) {y}_{1}\\left( t\\right) + \\left( {{p}_{22} - {p}_{12}}\\right) {y}_{2}\\left( t\\right) }\\right\\rbrack J\\left( {y}_{2t}\\right) .\\n\\]\\n\\nNote that, by (11.2) and (11.7), \\( 2{y}_{2}\\left( t\\right) J\\left( {y}_{2t}\\right) \\leq {y}_{2}^{2}\\left( t\\right) + J\\left( {y}_{2t}^{2}\\right) \\) and \\( 2{y}_{1}\\left( t\\right) J\\left( {y}_{2t}\\right) \\leq \\) \\( {\\alpha }^{-1}{y}_{1}^{2}\\left( t\\right) + {\\alpha J}\\left( {y}_{2t}^{2}\\right) \\) for some \\( \\alpha > 0 \\) . Besides, by (11.34) and (11.4), \\n\\n\\[ \\n\\left( {{\\delta }_{1} - b{\\left( {S}^{ * }\\right) }^{-1}}\\right) {p}_{11} + \\beta {I}^{ * }{p}_{12} = \\left( {{\\delta }_{1} - {\\mu }_{1}}\\right) {p}_{11} + \\beta {I}^{ * }\\left( {{p}_{12} - {p}_{11}}\\right) , \\n\\]\\n\\n\\[ \\n\\beta {I}^{ * }{p}_{22} - \\left( {b{\\left( {S}^{ * }\\right) }^{-1} + \\beta {S}^{ * }}\\right) {p}_{12} = 0, \\n\\]\\n\\n\\[ \\n{p}_{22} - {p}_{12} = \\frac{b{\\left( {S}^{ * }\\right) }^{-1} + \\beta {S}^{ * } - \\beta {I}^{ * }}{{2\\beta }{S}^{ * }\\left( {b{\\left( {S}^{ * }\\right) }^{-1} + \\beta {S}^{ * }}\\right) } = \\frac{{\\mu }_{1} + \\beta {S}^{ * }}{{2\\beta }{S}^{ * }\\left( {b{\\left( {S}^{ * }\\right) }^{-1} + \\beta {S}^{ * }}\\right) } > 0. \\n\\]\\n\\nThus, \\n\\n\\[ \\nL{V}_{1} \\leq \\left\\lbrack {2\\left( {\\left( {{\\delta }_{1} - {\\mu }_{1}}\\right) {p}_{11} + \\beta {I}^{ * }\\left( {{p}_{12} - {p}_{11}}\\right) }\\right) + {\\alpha }^{-1}\\beta {S}^{ * }\\left| {{p}_{12} - {p}_{11}}\\right| }\\right\\r",
        "id": "college_math_194965"
    },
    {
        "informal_statement": "Theorem 15.6. (Wilson). If $p$ is prime then $\\left( {p - 1}\\right) ! \\equiv - 1\\left( {\\;\\operatorname{mod}\\;p}\\right)$ .",
        "informal_proof": "Proof. We may assume $p \\neq 2$ . By Exercise 13.7 the only elements of the set\\n\\n$$ \\{ 1,2,\\ldots, p - 1\\} $$\\n\\nwhich are equal to their own inverse ${\\;\\operatorname{mod}\\;p}$ are 1 and $p - 1$ . So one can write\\n\\n$$ \\{ 1,2,\\ldots, p - 1\\} = \\left\\{ {1, p - 1,{a}_{1},{a}_{1}^{\\prime },{a}_{2},{a}_{2}^{\\prime },\\ldots }\\right\\} $$\\n\\nwhere ${a}_{1}^{\\prime } = {i}_{p}\\left( {a}_{1}\\right)$ is the inverse of ${a}_{1}{\\;\\operatorname{mod}\\;p}$, etc. Then\\n\\n$$ \\left( {p - 1}\\right) ! \\equiv 1 \\times \\left( {p - 1}\\right) \\times {a}_{1} \\times {a}_{1}^{\\prime } \\times {a}_{2} \\times {a}_{2}^{\\prime } \\times \\ldots \\equiv p - 1 \\equiv - 1\\left( {\\;\\operatorname{mod}\\;p}\\right) . $$",
        "id": "college_math_28801"
    },
    {
        "informal_statement": "Given that the function \\( f : \\left\\lbrack {a, b}\\right\\rbrack \\rightarrow \\mathbb{R} \\) is continuous on \\( \\left\\lbrack {a, b}\\right\\rbrack \\), and differentiable on \\( \\left( {a, b}\\right) \\) . 1. \\( f : \\left\\lbrack {a, b}\\right\\rbrack \\rightarrow \\mathbb{R} \\) is an increasing function if and only if \\( {f}^{\\prime }\\left( x\\right) \\geq 0 \\) for all \\( x \\in \\left( {a, b}\\right) \\) .",
        "informal_proof": "Again, let us consider the first statement. If \\( {f}^{\\prime }\\left( x\\right) \\geq 0 \\) for all \\( x \\in \\left( {a, b}\\right) \\) , the proof that \\( f \\) is increasing is almost verbatim the proof in Theorem 3.15, with \\( > \\) replaced by \\( \\geq \\) . For the converse, if \\( f \\) is increasing on \\( \\left\\lbrack {a, b}\\right\\rbrack \\), we want to show that \\( {f}^{\\prime }\\left( {x}_{0}\\right) \\geq 0 \\) for any \\( {x}_{0} \\) in \\( \\left( {a, b}\\right) \\) . This follows from the fact that \\[ \\frac{f\\left( x\\right) - f\\left( {x}_{0}\\right) }{x - {x}_{0}} \\geq 0 \\] for any \\( x \\) in \\( \\left( {a, b}\\right) \\smallsetminus \\{ 0\\} \\) since \\( f \\) is increasing. Taking limit gives \\( {f}^{\\prime }\\left( {x}_{0}\\right) \\geq 0 \\) .",
        "id": "college_math_306273"
    },
    {
        "informal_statement": "Theorem 5.3.1 The following q-Taylor formula [97, 2.5, p. 988], [236, 7.2, p. 856], [234, 2.11, p. 91] applies:\\n\\n$$ f\\\\left( x\\\\right) = \\\\mathop{\\\\sum }\\\\limits_{{k = 0}}^{\\\\infty }\\\\frac{{\\\\bigtriangleup }_{\\\\mathrm{{CG}}, q}^{k}f\\\\left( 0\\\\right) }{\\\\{ k\\\\}_{q}!}\\\\left\\\\{ x - k + 1\\\\right\\\\}_{k, q}. \\\\tag{5.73} $$",
        "informal_proof": "Proof Apply ${\\\\bigtriangleup }_{\\\\mathrm{{CG}}, q}^{s}$ to both members and finally put $x = 0$ .",
        "id": "college_math_124891"
    },
    {
        "informal_statement": "Theorem 9.3.1 (Hahn-Kolmogorov extension theorem-uniqueness of extension of a \\( \\sigma \\) -finite measure) Let \\( \\mathcal{A} \\) be an algebra of subsets of a non-empty set \\( X \\) and \\( m : \\mathcal{A} \\mapsto \\left\\lbrack {0,\\infty }\\right\\rbrack \\) a \\( \\sigma \\) -finite measure on \\( \\mathcal{A} \\) . There exists a unique extension of \\( m \\) to a measure \\( {m}^{\\prime } \\) defined on the \\( \\sigma \\) -algebra \\( \\mathcal{S}\\left( \\mathcal{A}\\right) \\) .",
        "informal_proof": "Proof Suppose \\( {m}_{1},{m}_{2} \\) are \\( \\sigma \\) -finite measures extending \\( m \\) to \\( \\mathcal{S}\\left( \\mathcal{A}\\right) \\), i.e. \\( {m}_{1}\\left( E\\right) = \\) \\( m\\left( E\\right) = {m}_{2}\\left( E\\right) \\) for every \\( E \\in \\mathcal{A} \\) . We need to show \\( {m}_{1}\\left( E\\right) = {m}_{2}\\left( E\\right) \\) for every \\( E \\in \\) \\( \\mathcal{S}\\left( \\mathcal{A}\\right) \\) . First, \\( {m}_{1}\\left( \\varnothing \\right) = m\\left( \\varnothing \\right) = {m}_{2}\\left( \\varnothing \\right) = 0 \\) . Without loss of generality assume, to begin with, \\( {m}_{1},{m}_{2} \\) totally finite over \\( \\mathcal{S}\\left( \\mathcal{A}\\right) \\) , i.e. \\( {m}_{1}\\left( X\\right) ,{m}_{2}\\left( X\\right) < \\infty \\) . Set \\[ \\mathcal{M} = \\left\\{ {E \\in \\mathcal{S}\\left( \\mathcal{A}\\right) : {m}_{1}\\left( E\\right) = {m}_{2}\\left( E\\right) }\\right\\} . \\] In \\( \\mathcal{M} \\) belong only elements of \\( \\mathcal{S}\\left( \\mathcal{A}\\right) \\) satisfying \\( {m}_{1}\\left( E\\right) = {m}_{2}\\left( E\\right) \\), so \\( \\mathcal{M} \\subseteq \\mathcal{S}\\left( \\mathcal{A}\\right) \\) . So we just need the opposite inequality. By assumption \\( m,{m}_{1},{m}_{2} \\) agree on any \\( E \\in \\mathcal{A} \\), so \\( \\mathcal{M} \\) is an algebra. By Proposition 7.1.2 we just need that \\( \\mathcal{M} \\) is monotone. Then take an increasing sequence \\( {\\left\\{ {E}_{i}\\right\\} }_{i \\geq 1} \\) in \\( \\mathcal{M} \\) with \\( E = \\mathop{\\bigcup }\\limits_{{i = 1}}^{\\infty }{E}_{i} \\) . Now, \\( {m}_{1},{m}_{2} \\) are measures on \\( \\mathcal{S}\\left( \\mathcal{A}\\right) \\) and hence they are completely additive on \\( \\mathcal{M} \\) . By Theorem 8.2.3 they are continuous from below as well, so \\[ {m}_{1}\\left( E\\right) = \\mathop{\\lim }\\limits_{{i \\rightarrow \\infty }}{m}_{1}\\left( {E}_{i}\\right) = \\mathop{\\lim }\\limits_{{i \\rightarrow \\infty }}{m}_{2}\\left( {E}_{i}\\right) = {m}_{2}\\left( E\\right) . \\] Therefore \\( E = \\left( {\\mathop{\\bigcup }\\limits_{{i = 1}}^{\\infty }{E}_{i}}\\right) \\in \\mathcal{M} \\) . Let now \\( {\\left\\{ {E}_{i}\\right\\} }_{i \\geq 1} \\) be a decreasing sequence in \\( \\mathcal{M} \\) , with \\( E = \\left( {\\mathop{\\bigcap }\\limits_{{i = 1}}^{\\infty }{E}_{i}}\\right) \\) . As before, \\( {m}_{1},{m}_{2} \\) are completely additive on \\( \\mathcal{M} \\) and totally finite by assumption. Theorem 8.2.4 ensures their continuity from above, \\[ {m}_{1}\\left( E\\right) = \\mathop{\\lim }\\limits_{{i \\rightarrow \\infty }}{m}_{1}\\left( {E}_{i}\\right) = \\mathop{\\lim }\\limits_{{i \\rightarrow \\infty }}{m}_{2}\\left( {E}_{i}\\right) = {m}_{2}\\left( E\\right) . \\] Hence \\( E = \\left( {\\mathop{\\bigcap }\\limits_{{i = 1}}^{\\infty }{E}_{i}}\\right) \\in \\mathcal{M} \\) . All in all \\( \\mathcal{M} \\) is a monotone collection containing \\( \\mathcal{A} \\) . Further, \\( \\mathcal{M}\\left( \\mathcal{A}\\right) \\) is the smallest monotone family containing \\( \\mathcal{A} \\), and Theorem 7.1.3 says \\( \\mathcal{S}\\left( \\mathcal{A}\\right) = \\mathcal{M}\\left( \\mathcal{A}\\right) \\subseteq \\mathcal{M} \\) . This proves the opposite inclusion for totally finite measures. For the general case, take a sequence \\( {\\left\\{ {A}_{i}\\right\\} }_{i \\geq 1} \\) in \\( \\mathcal{A} \\) such that \\( {m}_{1}\\left( {A}_{i}\\right) < \\infty ,{m}_{2}\\left( {A}_{i}\\right) \\) \\( < \\infty \\) for every \\( i \\in \\mathbb{N} \\) and \\( X = \\mathop{\\bigcup }\\limits_{{i = 1}}^{\\infty }{A}_{i} \\) . Given any \\( E \\in \\mathcal{S}\\left( \\mathcal{A}\\right) \\), call \\[ {m}_{1 \\mid {A}_{i}}\\left( E\\right) = {m}_{1}\\left( {E \\cap {A}_{i}}\\right) ,{m}_{2 \\mid {A}_{i}}\\lef",
        "id": "college_math_305550"
    },
    {
        "informal_statement": "Lemma 6.4.8. Let \\( \\omega \\in u{\\Omega }^{2}\\left( {{2r};\\underline{n}}\\right) \\) be a Hamiltonian form, \\( u \\in \\mathcal{U}\\left( {{2r};\\underline{n}}\\right) \\), and let \\( \\theta = \\mathop{\\sum }\\limits_{{i = 1}}^{{2r}}{f}_{i}d{x}_{i} \\in {\\Omega }^{1}\\left( {{2r};\\underline{n}}\\right) \\) be such that for some \\( k \\geq 2 \\n\\[ \\n{f}_{i} = {O}^{2}\\left( k\\right) \\text{ for all }i. \\n\\]\\nThen there is \\( \\varphi \\in {\\mathcal{A}}^{2}\\left( {{2r};\\underline{n}}\\right) \\) satisfying \\( \\varphi \\left( \\omega \\right) = \\omega + d\\left( {u\\theta }\\right) + u{O}^{2}\\left( {k + 2}\\right) \\) . If, moreover, \\( \\omega = d\\left( {u\\eta }\\right) \\) for some \\( \\eta \\in {\\Omega }^{1}{\\left( 2r;\\underline{n}\\right) }_{\\left( 2\\right) } \\), then there is \\( \\varphi \\in {\\mathcal{A}}^{2}\\left( {{2r};\\underline{n}}\\right) \\) satisfying \\( \\varphi \\left( \\omega \\right) = \\omega + d\\left( {u\\theta }\\right) + d\\left( {u{O}^{2}\\left( {k + 2}\\right) }\\right) . \\)",
        "informal_proof": "Proof. (a) Suppose first that \\( {\\omega }_{2} = {\\omega }_{H} \\) . Define \\( \\varphi \\in {\\mathcal{A}}^{2}\\left( {{2r};\\underline{n}}\\right) \\) by setting\\n\\n\\[ \\n\\varphi \\left( {x}_{i}\\right) \\mathrel{\\text{:=}} {x}_{i} + \\sigma \\left( i\\right) {f}_{{i}^{\\prime }}\\;\\text{ for all }i. \\n\\]\\n\\nLemma 6.4.7 yields \\( \\varphi \\left( u\\right) = u\\left( {1 + {O}^{2}\\left( k\\right) }\\right) \\) . Write \\( \\omega = u{\\omega }^{\\prime } \\) with \\( {\\omega }^{\\prime } \\in {\\Omega }^{2}\\left( {{2r};\\underline{n}}\\right) \\) , observe that \\( {\\omega }_{2}^{\\prime } = {\\omega }_{2} = {\\omega }_{H} \\) (as \\( u\\left( 0\\right) = 1 \\) ). Decompose \\( {\\omega }^{\\prime } = {\\omega }_{H} + {\\omega }_{\\left( 3\\right) }^{\\prime } \\) where \\( {\\omega }_{\\left( 3\\right) }^{\\prime } = O\\left( 3\\right) \\) . Note that \\( \\varphi \\left( {\\omega }_{\\left( 3\\right) }^{\\prime }\\right) = {\\omega }_{\\left( 3\\right) }^{\\prime } + {O}^{2}\\left( {k + 2}\\right) \\) and\\n\\n\\[ \\n\\varphi \\left( {\\omega }_{H}\\right) = \\mathop{\\sum }\\limits_{{i = 1}}^{r}d\\left( {{x}_{i} + {f}_{i + r}}\\right) \\land d\\left( {{x}_{i + r} - {f}_{i}}\\right) \\n\\]\\n\\n\\[ \\n= {\\omega }_{H} + d\\left( {\\mathop{\\sum }\\limits_{{i = 1}}^{{2r}}{f}_{i}d{x}_{i}}\\right) + {O}^{2}\\left( {2k}\\right) . \\n\\]\\n\\nThus\\n\\n\\[ \\n\\varphi \\left( \\omega \\right) = \\varphi \\left( u\\right) \\varphi \\left( {\\omega }^{\\prime }\\right) = u\\left( {1 + {O}^{2}\\left( k\\right) }\\right) \\left( {{\\omega }_{H} + {d\\theta } + {O}^{2}\\left( {2k}\\right) + {\\omega }_{\\left( 3\\right) }^{\\prime } + {O}^{2}\\left( {k + 2}\\right) }\\right) \\n\\]\\n\\n\\[ \\n= \\omega + u\\left( {{d\\theta } + {O}^{2}\\left( {k + 2}\\right) }\\right) . \\n\\]\\n\\nNext recall that by definition there is \\( \\lambda \\in {\\Omega }^{1}\\left( {{2r};\\underline{n}}\\right) \\) satisfying \\( {du} = {u\\lambda } \\) . Then \\( {ud\\theta } = d\\left( {u\\theta }\\right) - {u\\lambda } \\land \\theta = d\\left( {u\\theta }\\right) + u{O}^{2}\\left( {k + 2}\\right) \\) . This proves the claim in the particular case.\\n\\n(b) To treat the general case we recall that due to Lemma 6.4.2 there are \\( \\pi \\) and \\( \\psi : {\\operatorname{Aut}}_{c}\\mathcal{O}\\left( {{2r};\\underline{n}}\\right) \\rightarrow {\\operatorname{Aut}}_{c}\\mathcal{O}\\left( {{2r};{\\underline{n}}^{\\pi }}\\right) \\) such that \\( \\psi {\\left( \\omega \\right) }_{2} = {\\omega }_{H} \\) . Choose according to (a) \\( \\varphi \\in {\\mathcal{A}}^{2}\\left( {{2r};{\\underline{n}}^{\\pi }}\\right) \\) satisfying\\n\\n\\[ \\n\\varphi \\left( {\\psi \\left( \\omega \\right) }\\right) = \\psi \\left( \\omega \\right) + d\\left( {\\psi \\left( u\\right) \\psi \\left( \\theta \\right) }\\right) + \\psi \\left( u\\right) {O}^{2}\\left( {k + 2}\\right) . \\n\\]\\n\\nThen \\( {\\psi }^{-1} \\circ \\varphi \\circ \\psi \\) is the required homomorphism.",
        "id": "college_math_170422"
    },
    {
        "informal_statement": "Problem 3.3.27. If $p$ and ${p}^{2} + 8$ are both prime numbers, prove that ${p}^{3} + 4$ is also prime.",
        "informal_proof": "Solution 3.3.27. Referring to Problem 3.3.8, if $p > 3$ is prime, it is of the form $\\left( {{6k} + 1}\\right)$ or $\\left( {{6k} + 5}\\right)$ . So for $p = {6k} + 1$ or ${6k} + 5$, we have ${p}^{2} + 8 =$ ${36}{k}^{2} + {12k} + 9\\;$ or $\\;{p}^{2} + 8 = {36}{k}^{2} + {60k} + {33}\\;$ respectively. $\\;$ But $\\;3\\left| \\left( {{36}{k}^{2} + {12k} + 9}\\right) \\right.$ and $3 \\mid \\left( {{36}{k}^{2} + {60k} + {33}}\\right)$ . So ${p}^{2} + 8$ is not prime, provided $p > 3$ . By the problem, both $p$ and ${p}^{2} + 8$ are primes. Thus the only possibility is $p = 3$, which yields ${p}^{2} + 8 = {17}$ . Hence ${p}^{3} + 4 = {31}$.",
        "id": "college_math_84658"
    },
    {
        "informal_statement": "Corollary 5 If $\\\\left( {\\\\xi }_{n}\\\\right)$ is a real sequence such that, for each positive integer $m$ ,\\n\\n$$ \\n{N}^{-1}\\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{N}e\\\\left( {{\\\\xi }_{n + m} - {\\\\xi }_{n}}\\\\right) \\\\rightarrow 0\\\\;\\\\text{ as }N \\\\rightarrow \\\\infty ,\\n$$ \\n\\nthen \\n\\n$$ \\n{N}^{-1}\\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{N}e\\\\left( {\\\\xi }_{n}\\\\right) \\\\rightarrow 0\\\\;\\\\text{ as }N \\\\rightarrow \\\\infty .\\n$$",
        "informal_proof": "Proof By taking ${\\\\zeta }_{n} = e\\\\left( {\\\\xi }_{n}\\\\right)$ in Lemma 4 we obtain, for $1 \\\\leq M \\\\leq N$ ,\\n\\n$$ \\n{N}^{-2}{\\\\left| \\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{N}e\\\\left( {\\\\xi }_{n}\\\\right) \\\\right| }^{2} \\\\leq 2\\\\left( {M + N - 1}\\\\right) {M}^{-2}{N}^{-2}\\\\mathop{\\\\sum }\\\\limits_{{m = 1}}^{{M - 1}}\\\\left( {M - m}\\\\right) \\\\left| {\\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{{N - m}}e\\\\left( {{\\\\xi }_{n + m} - {\\\\xi }_{n}}\\\\right) }\\\\right| \\n$$ \\n\\n$$ \\n+ \\\\left( {M + N - 1}\\\\right) {M}^{-1}{N}^{-1}.\\\\text{.}\\n$$ \\n\\nKeeping $M$ fixed and letting $N \\\\rightarrow \\\\infty$, we get \\n\\n$$ \\n\\\\mathop{\\\\lim }\\\\limits_{{N \\\\rightarrow \\\\infty }}{N}^{-2}{\\\\left| \\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{N}e\\\\left( {\\\\xi }_{n}\\\\right) \\\\right| }^{2} \\\\leq {M}^{-1} \\n$$ \\n\\nBut $M$ can be chosen as large as we please.",
        "id": "college_math_86240"
    },
    {
        "informal_statement": "Proposition 10.45. Let \\( 0 \\leq {t}_{1} < {t}_{2} \\), and put \\( \\tau = {t}_{2} - {t}_{1} \\) . Then the process \\( \\left\\{ {{\\widetilde{B}}_{t},0 \\leq t \\leq \\tau }\\right\\} \\) is independent of \\( \\sigma \\left\\{ {{B}_{s}, s \\notin \\left( {{t}_{1},{t}_{2}}\\right) }\\right\\} \\) . Moreover, \\( {\\widetilde{B}}_{\\tau } = \\) \\( {\\widetilde{B}}_{0} = 0 \\), and \\( \\widetilde{B} \\) is a mean zero Gaussian process with covariance function \\( \\Gamma \\left( {s, t}\\right) = E\\left\\{ {{\\widetilde{B}}_{s}{\\widetilde{B}}_{t}}\\right\\} = s \\land t - {st}/\\tau \\) . In particular, \\( \\operatorname{Var}\\left\\{ {\\widetilde{B}}_{t}\\right\\} = t\\left( {\\tau - t}\\right) /\\tau \\) .",
        "informal_proof": "Proof. Brownian motion is a Gaussian process, so the variables are independent if they are uncorrelated, and we need only check the covariances. If \\( s \\notin \\left( {{t}_{1},{t}_{2}}\\right) \\), use the fact that \\( E\\left\\{ {{B}_{s}{B}_{t}}\\right\\} = s \\land t \\) to see that\\n\\n\\[ E\\left\\{ {{B}_{s}{\\widetilde{B}}_{t}}\\right\\} = E\\left\\{ {{B}_{s}\\left( {{B}_{t + {t}_{1}} - {B}_{{t}_{1}} - \\frac{t}{\\tau }\\left( {{B}_{{t}_{2}} - {B}_{{t}_{1}}}\\right) }\\right) }\\right\\} \\]\\n\\n\\[ = s \\land \\left( {t + {t}_{1}}\\right) - s \\land {t}_{1} - \\frac{t}{\\tau }\\left( {s \\land {t}_{2}}\\right) + \\frac{t}{\\tau }\\left( {s \\land {t}_{1}}\\right) . \\]\\n\\nThis expression vanishes if either \\( s \\leq {t}_{1} \\) or \\( s \\geq {t}_{2} \\) . This proves the independence.\\n\\nIt is clear that \\( {\\widetilde{B}}_{\\tau } = {\\widetilde{B}}_{0} = 0 \\), and that \\( \\widetilde{B} \\) has mean zero. Its covariance function is\\n\\n\\[ \\Gamma \\left( {s, t}\\right) = E\\left\\{ {{\\widetilde{B}}_{s}{\\widetilde{B}}_{t}}\\right\\} \\]\\n\\n\\[ = E\\left\\{ {\\left( {{B}_{s + {t}_{1}} - {B}_{t} - \\frac{s}{\\tau }\\left( {{B}_{{t}_{2}} - {B}_{{t}_{1}}}\\right) }\\right) \\left( {{B}_{t + {t}_{1}} - {B}_{{t}_{1}} - \\frac{t}{\\tau }\\left( {{B}_{{t}_{2}} - {B}_{{t}_{1}}}\\right) }\\right) }\\right\\} \\]\\n\\n\\[ = s \\land t - \\frac{st}{\\tau } \\]\\n\\nand the rest is immediate.",
        "id": "college_math_351457"
    },
    {
        "informal_statement": "Theorem 2.7.7 If\\n\\n$${\\\\int }_{\\\\sigma }^{\\\\infty }\\\\frac{1}{{r}_{2}\\\\left( t\\\\right) }{dt} = {\\\\int }_{\\\\sigma }^{\\\\infty }\\\\frac{1}{{r}_{1}\\\\left( t\\\\right) }{dt} = \\\\infty ,\\\\;{\\\\int }_{\\\\sigma }^{\\\\infty }{r}_{3}\\\\left( t\\\\right) {\\\\int }_{\\\\sigma }^{t}\\\\frac{1}{{r}_{1}\\\\left( s\\\\right) }{dsdt} = \\\\infty ,\\n$$\\n\\nthen (2.117) is oscillatory.",
        "informal_proof": "Proof If (2.117) is nonoscillatory, then by [8], there exists a nonoscillatory solution $x\\\\left( t\\\\right)$ of (2.117) such that $x\\\\left( t\\\\right)$ satisfies (2.135) for large $t$ . Without any loss of generality, we may assume that $x\\\\left( t\\\\right) > 0,{L}_{1}x\\\\left( t\\\\right) > 0$ and ${L}_{2}x\\\\left( t\\\\right) > 0$ for some $t \\\\geq {t}_{0} \\\\geq \\\\sigma$ , where ${L}_{i}x\\\\left( t\\\\right)$ is defined in (2.133). Integrating (2.117) from ${t}_{0}$ to $t$, we obtain\\n\\n$${\\\\left. {r}_{2}\\\\left( t\\\\right) {\\\\left( {r}_{1}\\\\left( t\\\\right) {x}^{\\\\prime }\\\\left( t\\\\right) \\\\right) }^{\\\\prime } - {r}_{2}\\\\left( t\\\\right) {\\\\left( {r}_{1}\\\\left( t\\\\right) {x}^{\\\\prime }\\\\left( t\\\\right) \\\\right) }^{\\\\prime }\\\\right| }_{t = {t}_{0}} + {\\\\int }_{{t}_{0}}^{t}{r}_{3}\\\\left( s\\\\right) x\\\\left( s\\\\right) {ds} = 0.\\n$$\\n\\nSince ${L}_{2}x\\\\left( t\\\\right) > 0$, we have\\n\\n$${\\\\left. {r}_{2}\\\\left( t\\\\right) {\\\\left( {r}_{1}\\\\left( t\\\\right) {x}^{\\\\prime }\\\\left( t\\\\right) \\\\right) }^{\\\\prime }\\\\right| }_{t = {t}_{0}} > {\\\\int }_{{t}_{0}}^{t}{r}_{3}\\\\left( s\\\\right) x\\\\left( s\\\\right) {ds}. \\\\tag{2.136}\\n$$\\n\\nSince ${L}_{1}x\\\\left( t\\\\right)$ is a positive increasing function, we have\\n\\n$$x\\\\left( t\\\\right) > x\\\\left( {t}_{0}\\\\right) + {r}_{1}\\\\left( {t}_{0}\\\\right) {x}^{\\\\prime }\\\\left( {t}_{0}\\\\right) {\\\\int }_{{t}_{0}}^{t}\\\\frac{1}{{r}_{1}\\\\left( u\\\\right) }{du}.\\n$$\\n\\nHence (2.136) gives\\n\\n$${\\\\left. {r}_{2}\\\\left( t\\\\right) {\\\\left( {r}_{1}\\\\left( t\\\\right) {x}^{\\\\prime }\\\\left( t\\\\right) \\\\right) }^{\\\\prime }\\\\right| }_{t = {t}_{0}} > {\\\\int }_{{t}_{0}}^{t}{r}_{3}\\\\left( s\\\\right) \\\\left( {x\\\\left( {t}_{0}\\\\right) + {r}_{1}\\\\left( {t}_{0}\\\\right) {x}^{\\\\prime }\\\\left( {t}_{0}\\\\right) {\\\\int }_{{t}_{0}}^{s}\\\\frac{1}{{r}_{1}\\\\left( u\\\\right) }{du}}\\\\right) {ds}\\n$$\\n\\n$$> {r}_{1}\\\\left( {t}_{0}\\\\right) {x}^{\\\\prime }\\\\left( {t}_{0}\\\\right) {\\\\int }_{{t}_{0}}^{t}{r}_{3}\\\\left( s\\\\right) {\\\\int }_{{t}_{0}}^{s}\\\\frac{1}{{r}_{1}\\\\left( u\\\\right) }{duds}.\\n$$\\n\\nThe above inequality yields a contradiction by taking the limit as $t \\\\rightarrow \\\\infty$ . The theorem is proved.",
        "id": "college_math_130095"
    },
    {
        "informal_statement": "Proposition 3.1 Let $f$ be a non-degenerate quadratic form in $n$ variables over a quadratically closed ${}^{2}$ field. If the characteristic of $k$ is two, then $f$ is projectively equivalent to either\\n\\n1. ${x}_{1}{x}_{2} + {x}_{3}{x}_{4} + \\cdots + {x}_{n - 2}{x}_{n - 1} + {x}_{n}^{2}$ if $n$ is odd, or\\n\\n2. ${x}_{1}{x}_{2} + {x}_{3}{x}_{4} + \\cdots + {x}_{n - 3}{x}_{n - 2} + {x}_{n - 1}{x}_{n}$ if $n$ is even.\\n\\n---",
        "informal_proof": "Proof The only quadratic form in one variable is ${x}_{1}^{2}$ . Likewise, the two-variable case is trivial: a degree two form in two variables must factor into two linear forms over a quadratically closed field, so in suitable coordinates, the form is either ${x}_{1}{x}_{2}$ or ${x}_{1}^{2}$ (which is degenerate).\\n\\n## Case of Characteristic Not Two\\n\\nIt is straightforward to check (even without closure assumptions on $k$ ) that a suitable choice of linear change of coordinates puts $f$ in the form ${\\lambda }_{1}{x}_{1}^{2} + \\cdots + {\\lambda }_{n}{x}_{n}^{2}$, where the ${\\lambda }_{i}$ are nonzero (e.g., see [13]). So, if the ground field is quadratically closed, the change of coordinates taking each ${x}_{i} \\mapsto \\frac{1}{\\sqrt{{\\lambda }_{i}}}{x}_{i}$ normalizes the form to ${x}_{1}^{2} + \\cdots + {x}_{n}^{2}$ .\\n\\n## Case of Characteristic Two\\n\\nSay that $n \\geq 3$ . Since $f$ is non-degenerate, it is not the square of a linear form. Thus some square-free term, which we can assume to be ${x}_{1}{x}_{2}$, appears with nonzero coefficient. Scaling, we may assume the coefficient of ${x}_{1}{x}_{2}$ is 1 .\\n\\nNow write $f$ in the form\\n\\n$${L}^{2} + {x}_{1}{x}_{2} + \\mathop{\\sum }\\limits_{{j = 3}}^{n}{a}_{1j}{x}_{1}{x}_{j} + \\mathop{\\sum }\\limits_{{j = 3}}^{n}{a}_{2j}{x}_{2}{x}_{j} + {h}_{1}\\left( {{x}_{3},\\ldots ,{x}_{n}}\\right) \\tag{2}$$\\n\\nwhere $L$ is a (possibly zero) linear form in ${x}_{1},{x}_{2}$, and ${h}_{1}$ is a quadratic form in ${x}_{3},\\ldots ,{x}_{n}$ . Apply the linear change of coordinates sending ${x}_{2}$ to ${x}_{2} + \\mathop{\\sum }\\limits_{{j = 3}}^{n}{a}_{1j}{x}_{j}$ , fixing the other variables. This transforms (2) into an expression which can be written\\n\\n$${L}^{2} + {x}_{1}{x}_{2} + \\mathop{\\sum }\\limits_{{j = 3}}^{n}{a}_{2j}^{\\prime }{x}_{2}{x}_{j} + {h}_{2}\\left( {{x}_{3},\\ldots ,{x}_{n}}\\right) , \\tag{3}$$\\n\\nwhere again ${h}_{2}$ is a quadratic form in ${x}_{3},\\ldots ,{x}_{n}$ .",
        "id": "college_math_144508"
    },
    {
        "informal_statement": "Theorem 26. A logic with theorems is fully Fregean if and only if it is both Fregean and fully selfextensional.",
        "informal_proof": "Proof. Trivially, if a logic is fully Fregean, then it is both Fregean and fully selfextensional. For the converse, suppose that a logic has these two properties. By Theorem 14, the logic will be truth-equational, and then we can apply Theorem 22, which tells us that, since it is assumed to be fully selfex-tensional, it is in fact fully Fregean.",
        "id": "college_math_225273"
    },
    {
        "informal_statement": "Example 1.22 Consider the differential equation\\n\\n\\\\[ \\n\\\\frac{{d}^{2}x}{d{t}^{2}} - 4\\\\frac{dx}{dt} + {5x} = \\\\cos t,\\\\;t \\\\in \\\\mathbb{R}.\\n\\\\]\\n\\nLet \\\\( a \\\\) and \\\\( b \\\\) be two real numbers. How many solutions \\\\( x = \\\\varphi \\\\left( t\\\\right) \\\\) of the differential equation satisfy \\\\( \\\\varphi \\\\left( 0\\\\right) = a \\\\) and \\\\( \\\\varphi \\\\left( {2\\\\pi }\\\\right) = b \\\\) ?",
        "informal_proof": "A. Linear inhomogeneous differential equation of second order and of constant coefficients.\\n\\nD. Start by finding the roots of the characteristic polynomial, and find the complete solution of the homogeneous equation. Since \\\\( \\\\cos t = \\\\operatorname{Re}{e}^{it} \\\\), we first guess (in a complex form) of a particular solution of the form \\\\( x = c \\\\cdot {e}^{it} \\\\) . Finally, we apply the boundary conditions on the complete solution and analyze.\\n\\nI. The characteristic polynomial \\\\( {R}^{2} - {4R} + 5 \\\\) has the simple roots \\\\( R = 2 \\\\pm i \\\\), hence the homogeneous equation has the complete solution\\n\\n\\\\[ \\n{c}_{1}{e}^{2t}\\\\cos t + {c}_{2}{e}^{2t}\\\\sin t\\n\\\\]\\n\\nIf we put \\\\( x = c \\\\cdot {e}^{it} \\\\) into the left hand side of the equation we get\\n\\n\\\\[ \\n\\\\frac{{d}^{2}x}{d{t}^{2}} - 4\\\\frac{dx}{dt} + {5x} = c\\\\left( {-1 - {4i} + 5}\\\\right) {e}^{it} = {4c}\\\\left( {1 - i}\\\\right) {e}^{ot},\\n\\\\]\\n\\nwhich is equal to \\\\( {e}^{it} \\\\) for \\\\( c = \\\\frac{1}{4} \\\\cdot \\\\frac{1}{1 - i} = \\\\frac{1}{8}\\\\left( {1 + i}\\\\right) \\\\) . Thus, a particular solution is\\n\\n\\\\[ \\n\\\\operatorname{Re}\\\\left\\\\{ {\\\\frac{1}{8}\\\\left( {1 + i}\\\\right) {e}^{it}}\\\\right\\\\} = \\\\frac{1}{8}\\\\cos t - \\\\frac{1}{8}\\\\sin t.\\n\\\\]\\n\\nThe complete solution is\\n\\n\\\\[ \\n\\\\varphi \\\\left( t\\\\right) = \\\\frac{1}{8}\\\\cos t - \\\\frac{1}{8}\\\\sin t + {c}_{1}{e}^{2t}\\\\cos t + {c}_{2}{e}^{2t}\\\\sin t.\\n\\\\]\\n\\nFrom the given boundary value condition follows that\\n\\n\\\\[ \\n\\\\varphi \\\\left( 0\\\\right) = a = \\\\frac{1}{8} + {c}_{1},\\\\;\\\\varphi \\\\left( {2\\\\pi }\\\\right) = b = \\\\frac{1}{8} + {c}_{1}{e}^{4\\\\pi }.\\n\\\\]\\n\\nThis gives\\n\\n\\\\[ \\n{c}_{1} = a - \\\\frac{1}{8}\\\\;\\\\text{ og }\\\\;{c}_{1} = {e}^{-{4\\\\pi }}\\\\left( {b - \\\\frac{1}{8}}\\\\right) .\\n\\\\]\\n\\nHere we have two possibilities:\\n\\n\\\\[ \\n\\\\text{1) If}a - \\\\frac{1}{8} \\\\neq {e}^{-{4\\\\pi }}\\\\left( {b - \\\\frac{1}{8}}\\\\right) \\\\text{, i.e.}\\n\\\\]\\n\\n\\\\[ \\n{e}^{4\\\\pi }a \\\\neq b + \\\\frac{1}{8}\\\\left\\\\{ {{e}^{4\\\\pi } - 1}\\\\right\\\\}\\n\\\\]\\n\\nthen there is no solution.\\n\\n2) If instead \\\\( {e}^{4\\\\pi }a = b + \\\\frac{1}{8}\\\\left\\\\{ {{e}^{4\\\\pi } - 1}\\\\right\\\\} \\\\), then\\n\\n\\\\[ \\n{c}_{1} = a - \\\\frac{1}{8}\\\\;\\\\left( { = {e}^{-{4\\\\pi }}\\\\left( {b - \\\\frac{1}{8}}\\\\right) }\\\\right) ,\\n\\\\]\\n\\nand we can choose \\\\( {c}_{2} \\\\) arbitrarily, and we conclude that we have infinitely many solutions.",
        "id": "college_math_356221"
    },
    {
        "informal_statement": "Proposition 5.7. If $f$ is approach prime then there exists an ultrafilter $\\mathcal{U} \\supset \\mathcal{F}$ such that $f = \\lambda \\mathcal{U}$ (consequently $\\mathcal{U}$ is a Cauchy filter).",
        "informal_proof": "Proof. It is clear that $f \\leq \\lambda \\mathcal{U}$ for all ultrafilters $\\mathcal{U}$ finer than $\\mathcal{F}$ . Suppose that for all those ultrafilters $f \\neq \\lambda \\mathcal{U}$ then $$ \\forall \\mathcal{U} \\supset \\mathcal{F},\\exists {x}_{\\mathcal{U}} \\in X : f\\left( {x}_{\\mathcal{U}}\\right) < \\lambda \\mathcal{U}\\left( {x}_{\\mathcal{U}}\\right) = \\mathop{\\sup }\\limits_{{U \\in \\mathcal{U}}}\\delta \\left( {{x}_{\\mathcal{U}}, U}\\right) $$ and hence $$ \\forall \\mathcal{U} \\supset \\mathcal{F},\\exists {x}_{\\mathcal{U}} \\in X,\\exists {U}_{\\mathcal{U}} \\in \\mathcal{U} : f\\left( {x}_{\\mathcal{U}}\\right) < \\delta \\left( {{x}_{\\mathcal{U}},{U}_{\\mathcal{U}}}\\right) $$ which implies that $\\forall \\mathcal{U}$ finer than $\\mathcal{F} : {\\delta }_{{U}_{\\mathcal{U}}} \\nleqslant f$ . Now there exist ${\\mathcal{U}}_{1},\\ldots ,{\\mathcal{U}}_{n}$ finer than $\\mathcal{F}$ and ${U}_{{\\mathcal{U}}_{1}} \\in {\\mathcal{U}}_{1},\\ldots ,{U}_{{\\mathcal{U}}_{n}} \\in {\\mathcal{U}}_{n}$ such that $\\mathop{\\bigcup }\\limits_{{i = 1}}^{n}{U}_{{\\mathcal{U}}_{i}} \\in \\mathcal{F}$ . However then $$ \\mathop{\\min }\\limits_{{i = 1}}^{n}{\\delta }_{{U}_{{\\mathcal{U}}_{i}}} = {\\delta }_{\\mathop{\\bigcup }\\limits_{{i = 1}}^{n}{U}_{{\\mathcal{U}}_{i}}} \\nleqslant f $$ since $f$ is prime. We know however that $f = \\alpha \\mathcal{F} = \\mathop{\\sup }\\limits_{{F \\in \\mathcal{F}}}{\\delta }_{F}$ so this gives a contradiction. Thus there exists an ultrafilter $\\mathcal{U}$ finer than $\\mathcal{F}$ such that $f = \\lambda \\mathcal{U}.\\mathcal{U}$ is also Cauchy since $\\mathop{\\inf }\\limits_{{x \\in X}}f\\left( x\\right) = 0$ .",
        "id": "college_math_139385"
    },
    {
        "informal_statement": "Proposition 8. If $\\\\mathcal{M}$ is a reachable automaton with behavior $L$, then\\n\\n$$ C{C}_{\\\\mathcal{M}} = {RS}{C}_{L} $$",
        "informal_proof": "Proof. Analogous to that of Proposition 7.",
        "id": "college_math_94982"
    },
    {
        "informal_statement": "Proposition 5.3.18 Let $f, g \\\\in {S}_{k + 2}\\\\left( {\\\\Gamma ,\\\\mathbb{C}}\\\\right)$ . Then\\n\\n$$ \\n\\\\left( {{\\\\phi }_{f}^{1},{\\\\phi }_{g}^{1}}\\\\right) = 0 \\\\tag{5.3.15} \\n$$ \\n\\n$$ \\n\\\\left( {{\\\\phi }_{f}^{3},{\\\\phi }_{g}^{3}}\\\\right) = 0 \\\\tag{5.3.16} \\n$$ \\n\\n$$ \\n\\\\left( {{\\\\phi }_{f}^{1},{\\\\phi }_{g}^{3}}\\\\right) = - {\\\\left( 2i\\\\right) }^{k - 1}{\\\\left( f,{g}_{\\\\rho }\\\\right) }_{\\\\Gamma }. \\\\tag{5.3.17} \\n$$",
        "informal_proof": "Proof This follows immediately from Proposition 5.3.18 and the definition of ${\\\\phi }_{3}$ .",
        "id": "college_math_131856"
    },
    {
        "informal_statement": "Theorem 5.1 Let \\( f : \\mathcal{D} \\subset {\\mathbb{C}}^{ + } \\rightarrow \\mathbb{C} \\) be a meromorphic function of bounded type in the upper half plane and \\( {\\zeta }_{0} \\in \\mathcal{D} \\) . Then \\( f \\) has a realization \\( \\left( {A,{\\Gamma }_{{\\zeta }_{0}}}\\right) \\) admitting \\( \\varrho \\left( A\\right) \\cap {\\mathbb{C}}^{ + } = \\mathcal{D} \\) . In other words, \\( f \\) can be expressed by formula (2.1), i.e.\\n\\n\\[ f\\left( \\zeta \\right) = \\overline{f\\left( {\\zeta }_{0}\\right) } + \\left( {\\zeta - \\overline{{\\zeta }_{0}}}\\right) {\\Gamma }_{{\\zeta }_{0}}^{ + }\\left( {I + \\left( {\\zeta - {\\zeta }_{0}}\\right) {\\left( A - \\zeta \\right) }^{-1}}\\right) {\\Gamma }_{{\\zeta }_{0}}\\;\\forall \\zeta \\in \\mathcal{D}. \\]",
        "informal_proof": "Proof Since \\( f \\) is of bounded type we can write \\( f = \\frac{{f}_{1}}{{f}_{2}} \\), where \\( {f}_{1} \\) and \\( {f}_{2} \\) are in \\( {H}^{\\infty }\\left( {\\mathbb{C}}^{ + }\\right) \\) and do not have common zeros. We denote by \\( \\mathcal{P}\\left( f\\right) \\) the poles of \\( f \\) and note that \\( \\mathcal{P}\\left( f\\right) = \\mathcal{Z}\\left( {f}_{2}\\right) \\) . We rewrite\\n\\n\\[ f = \\frac{{f}_{1}}{{f}_{2}} = \\frac{{f}_{1} + 2{\\begin{Vmatrix}{f}_{1}\\end{Vmatrix}}_{\\infty } - 2{\\begin{Vmatrix}{f}_{1}\\end{Vmatrix}}_{\\infty }}{{f}_{2}} = \\frac{{f}_{1} + 2{\\begin{Vmatrix}{f}_{1}\\end{Vmatrix}}_{\\infty }}{{f}_{2}} + \\frac{-2{\\begin{Vmatrix}{f}_{1}\\end{Vmatrix}}_{\\infty }}{{f}_{2}} \\]\\n\\nand define\\n\\n\\[ {g}_{1} \\mathrel{\\text{:=}} \\frac{{f}_{2}}{{f}_{1} + 2{\\begin{Vmatrix}{f}_{1}\\end{Vmatrix}}_{\\infty }},\\;{g}_{2} \\mathrel{\\text{:=}} \\frac{-{f}_{2}}{2{\\begin{Vmatrix}{f}_{1}\\end{Vmatrix}}_{\\infty }}. \\]\\n\\nIt is obvious that \\( {g}_{2} \\in {H}^{\\infty }\\left( {\\mathbb{C}}^{ + }\\right) \\) and that \\( \\mathcal{Z}\\left( {g}_{2}\\right) = \\mathcal{Z}\\left( {f}_{2}\\right) = \\mathcal{P}\\left( f\\right) \\) . For \\( {g}_{1} \\) we note that\\n\\n\\[ \\left| {{f}_{1}\\left( z\\right) + 2{\\begin{Vmatrix}{f}_{1}\\end{Vmatrix}}_{\\infty }}\\right| \\geq {\\begin{Vmatrix}{f}_{1}\\end{Vmatrix}}_{\\infty }\\;\\forall z \\in {\\mathbb{C}}^{ + }. \\]\\n\\nThus, \\( {g}_{1} \\in {H}^{\\infty }\\left( {\\mathbb{C}}^{ + }\\right) \\) and \\( \\mathcal{Z}\\left( {g}_{1}\\right) = \\mathcal{Z}\\left( {f}_{2}\\right) = \\mathcal{P}\\left( f\\right) \\) . Consequently, the functions \\( {g}_{1} \\) and \\( {g}_{2} \\) are real Quasi-Herglotz functions (Proposition 4.4) with\\n\\n\\[ f = \\frac{1}{{g}_{1}} + \\frac{1}{{g}_{2}}\\;\\text{ and }\\;\\mathcal{Z}\\left( {g}_{1}\\right) = \\mathcal{Z}\\left( {g}_{2}\\right) = \\mathcal{P}\\left( f\\right) . \\]\\n\\nSince \\( {\\zeta }_{0} \\in \\mathcal{D} = {\\mathbb{C}}^{ + } \\smallsetminus \\mathcal{P}\\left( f\\right) \\) we have by Corollary 4.5 minimal realizations\\n\\n\\[ \\left( {{A}^{1},{\\Gamma }_{{\\zeta }_{0}}^{1}}\\right) \\text{ for }\\frac{1}{{g}_{1}}\\;\\text{ and }\\;\\left( {{A}^{2},{\\Gamma }_{{\\zeta }_{0}}^{2}}\\right) \\text{ for }\\frac{1}{{g}_{2}} \\]\\n\\nwith the resolvent sets admitting\\n\\n\\[ {\\mathbb{C}}^{ + } \\smallsetminus \\mathcal{P}\\left( f\\right) \\subset \\varrho \\left( {A}^{1}\\right) \\;\\text{ and }\\;{\\mathbb{C}}^{ + } \\smallsetminus \\mathcal{P}\\left( f\\right) \\subset \\varrho \\left( {A}^{2}\\right) . \\]\\n\\nIt is straightforward to verify that \\( \\left( {A,{\\Gamma }_{{\\zeta }_{0}}}\\right) \\) defined on the Krein space \\( \\mathcal{K} \\) given by\\n\\n\\[ \\mathcal{K} \\mathrel{\\text{:=}} {\\mathcal{K}}_{1} \\oplus {\\mathcal{K}}_{2} \\]\\n\\n\\[ {\\left\\lbrack \\left( {x}_{1},{x}_{2}\\right) ,\\left( {y}_{1},{y}_{2}\\right) \\right\\rbrack }_{\\mathcal{K}} \\mathrel{\\text{:=}} {\\left\\lbrack {x}_{1},{y}_{1}\\right\\rbrack }_{{\\mathcal{K}}_{1}} + {\\left\\lbrack {x}_{2},{y}_{2}\\right\\rbrack }_{{\\mathcal{K}}_{2}}\\forall \\left( {{x}_{1},{x}_{2}}\\right) ,\\left( {{y}_{1},{y}_{2}}\\right) \\in \\mathcal{K} \\]\\n\\n\\[ {\\left( A - {\\zeta }_{0}\\right) }^{-1} = \\left( \\begin{matrix} {\\left( {A}^{1} - {\\zeta }_{0}\\right) }^{-1} & 0 \\\\ 0 & {\\left( {A}^{2} - {\\zeta }_{0}\\right) }^{-1} \\end{matrix}\\right) ,\\;{\\Gamma }_{{\\zeta }_{0}} = \\left( \\begin{matrix} {\\Gamma }_{{\\zeta }_{0}}^{1} \\\\ {\\Gamma }_{{\\zeta }_{0}}^{2} \\end{matrix}\\right) \\]\\n\\nis a r",
        "id": "college_math_278611"
    },
    {
        "informal_statement": "Example 11.3 (Langevin equation). Consider the dynamics of an inertial particle moving in a potential \\( U \\), subject to friction and random noise:\\n\\n(11.72)\\n\\n\\[ \\left\\{ \\begin{array}{l} {\\dot{\\mathbf{X}}}_{t} = {\\mathbf{V}}_{t}, \\\\ m{\\dot{\\mathbf{V}}}_{t} = - \\gamma {\\mathbf{V}}_{t} - \\nabla U\\left( {\\mathbf{X}}_{t}\\right) + \\sqrt{2\\sigma }{\\dot{\\mathbf{W}}}_{t}. \\end{array}\\right. \\]",
        "informal_proof": "In the absence of the external potential, we have\\n\\n\\[ m{\\dot{\\mathbf{V}}}_{t} = - \\gamma {\\mathbf{V}}_{t} + \\sqrt{2\\sigma }{\\dot{\\mathbf{W}}}_{t} \\]\\n\\nThis is the Ornstein-Uhlenbeck process for \\( {\\mathbf{V}}_{t} \\) . In the limit \\( t \\rightarrow \\infty \\), we have\\n\\n\\[ {\\left\\langle \\frac{1}{2}m{\\mathbf{V}}^{2}\\right\\rangle }_{eq} = \\frac{3\\sigma }{2\\gamma }. \\]\\n\\nFrom thermodynamics, the average kinetic energy is related to the temperature \\( T \\) by\\n\\n\\[ {\\left\\langle \\frac{1}{2}m{\\mathbf{V}}^{2}\\right\\rangle }_{eq} = \\frac{3{k}_{B}T}{2}. \\]\\n\\nThus we obtain the simplest version of the fluctuation-dissipation relation:\\n\\n(11.73)\\n\\n\\[ \\sigma = {k}_{B}{T\\gamma } \\]\\n\\nwhich connects the strength of the fluctuation \\( \\sigma \\) and the friction coefficient \\( \\gamma \\) . It can be proved that in this case the diffusion coefficient\\n\\n(11.74)\\n\\n\\[ D \\mathrel{\\text{:=}} \\mathop{\\lim }\\limits_{{t \\rightarrow \\infty }}\\frac{\\left\\langle {\\left( {\\mathbf{X}}_{t} - {\\mathbf{X}}_{0}\\right) }^{2}\\right\\rangle }{6t} = \\frac{{k}_{B}T}{\\gamma }. \\]\\n\\nThis is called the Einstein relation.\\n\\nEquation (11.73) can also be proved by requiring that the probability measure with density \\( \\rho = {Z}^{-1}{e}^{-{\\beta H}}, H\\left( {\\mathbf{x},\\mathbf{v}}\\right) = m{\\mathbf{v}}^{2}/2 + U\\left( \\mathbf{x}\\right) \\), is invariant under the dynamics (11.72).",
        "id": "college_math_188712"
    },
    {
        "informal_statement": "Proposition 19.18. Let \\( A \\) be a ring and let \\( I \\subset A \\) be an ideal that can be generated by a completely intersecting sequence \\( \\mathbf{f} \\) of length \\( r \\) . Let \\( {g}_{1},\\ldots ,{g}_{r} \\in I \\) be elements that generate I. Then the sequence \\( \\left( {{g}_{1},\\ldots ,{g}_{r}}\\right) \\) is completely intersecting.",
        "informal_proof": "Proof. As \\( I/{I}^{2} \\) is a free \\( A/I \\) -module of rank \\( r \\) (Corollary 19.15), the image of \\( \\mathbf{g} \\) in \\( I/{I}^{2} \\) is automatically a basis (Corollary B.4). Write \\( {g}_{j} = \\sum {a}_{ij}{f}_{i} \\) for \\( {a}_{ij} \\in A \\) and let \\( w : {A}^{r} \\rightarrow {A}^{r} \\) be the \\( A \\) -linear map given by the matrix \\( \\left( {a}_{ij}\\right) \\) . Then \\( w \\otimes {\\operatorname{id}}_{A/I} \\) is an isomorphism. By functoriality, \\( w \\) yields a homomorphism \\( {K}_{ \\bullet }\\left( \\mathbf{f}\\right) \\rightarrow {K}_{ \\bullet }\\left( \\mathbf{g}\\right) \\) (Remark 19.5 (1)) which induces an isomorphism \\( {H}_{ \\bullet }\\left( \\mathbf{f}\\right) \\overset{ \\sim }{ \\rightarrow }{H}_{ \\bullet }\\left( \\mathbf{g}\\right) \\) since the homology is annihilated by \\( I \\) (Remark 19.6).",
        "id": "college_math_346417"
    },
    {
        "informal_statement": "Example 17 Solve \\( \\frac{dy}{dx} = {e}^{x - y} + {x}^{2}{e}^{-y} \\) .",
        "informal_proof": "Sol. Here, \\( \\;\\frac{dy}{dx} = \\frac{{e}^{x}}{{e}^{y}} + \\frac{{x}^{2}}{{e}^{y}} \\Rightarrow {e}^{y}{dy} = \\left( {{x}^{2} + {e}^{x}}\\right) {dx} \\)\\n\\nThis is variable-separable form,\\n\\n\\( \\therefore \\) Integrating both the sides,\\n\\n\\[ \\n\\int {e}^{y}{dy} = \\int \\left( {{x}^{2} + {e}^{x}}\\right) {dx} \\Rightarrow {e}^{y} = \\frac{{x}^{3}}{3} + {e}^{x} + C \\n\\]\\n\\nWhich is the general solution of the given differential equation, where \\( C \\) is an arbitrary constant.",
        "id": "college_math_201273"
    },
    {
        "informal_statement": "Exercise 8.4 (Normalisation constant of a Gaussian). The normalisation constant of a Gaussian distribution is related to the integral\\n\\n\\[ I = {\\int }_{-\\infty }^{\\infty }{e}^{-\\frac{1}{2}{x}^{2}}{dx} \\tag{8.11.3} \\]",
        "informal_proof": "By considering\\n\\n\\[ {I}^{2} = {\\int }_{-\\infty }^{\\infty }{e}^{-\\frac{1}{2}{x}^{2}}{dx}{\\int }_{-\\infty }^{\\infty }{e}^{-\\frac{1}{2}{y}^{2}}{dy} = {\\int }_{-\\infty }^{\\infty }{\\int }_{-\\infty }^{\\infty }{e}^{-\\frac{1}{2}{x}^{2} + {y}^{2}}{dxdy} \\tag{8.11.4} \\]\\n\\nand transforming to polar coordinates, show that\\n\\n1. \\( I = \\sqrt{2\\pi } \\)\\n\\n2. \\( {\\int }_{-\\infty }^{\\infty }{e}^{-\\frac{1}{2{\\sigma }^{2}}{\\left( x - \\mu \\right) }^{2}}{dx} = \\sqrt{{2\\pi }{\\sigma }^{2}} \\)",
        "id": "college_math_207355"
    },
    {
        "informal_statement": "Theorem 4.4 (Superposition-decomposition) Let $\\\\mathcal{I}$ be a countable set.\\n\\n(a) Let ${\\\\left( {N}_{t}^{i}\\\\right) }_{t \\\\geq 0}$ be independent Poisson processes with intensities ${\\\\lambda }_{i} > 0$ for $i$ in\\n\\n$\\\\mathcal{I}$ satisfying $\\\\lambda \\\\mathrel{\\\\text{:=}} \\\\mathop{\\\\sum }\\\\limits_{{i \\\\in \\\\mathcal{I}}}{\\\\lambda }_{i} < \\\\infty$ . Then ${\\\\left( {N}_{t}\\\\right) }_{t \\\\geq 0}$ defined by\\n\\n$$ {N}_{t} \\\\mathrel{\\\\text{:=}} \\\\mathop{\\\\sum }\\\\limits_{{i \\\\in \\\\mathcal{I}}}{N}_{t}^{i} $$\\n\\nis a Poisson process with intensity $\\\\lambda$, and is called the superposition, or the sum, of the ${\\\\left( {N}_{t}^{i}\\\\right) }_{t \\\\geq 0}$ .\\n\\nMoreover, for $n \\\\geq 1$, let ${T}_{n}$ be the nth jump instant of ${\\\\left( {N}_{t}\\\\right) }_{t \\\\geq 0}$, and ${Y}_{n}$ the $\\\\mathcal{I}$ -valued r.v. given by\\n\\n$$ \\\\left\\\\{ {{Y}_{n} = i}\\\\right\\\\} \\\\mathrel{\\\\text{:=}} \\\\left\\\\{ {{N}_{{T}_{n}}^{i} \\\\neq {N}_{{T}_{n} - }^{i}}\\\\right\\\\} ,\\\\;i \\\\in \\\\mathcal{I}. $$\\n\\nThen ${\\\\left( {Y}_{n}\\\\right) }_{n \\\\geq 1}$ is a sequence of i.i.d. r.v.’s independent of ${\\\\left( {N}_{t}\\\\right) }_{t \\\\geq 0}$, and\\n\\n$$ \\\\mathbb{P}\\\\left( {{Y}_{1} = i}\\\\right) = \\\\frac{{\\\\lambda }_{i}}{\\\\lambda } = \\\\frac{{\\\\lambda }_{i}}{\\\\mathop{\\\\sum }\\\\limits_{{j \\\\in \\\\mathcal{J}}}{\\\\lambda }_{j}},\\\\;i \\\\in \\\\mathcal{I}. $$\\n\\n(b) Let ${\\\\left( {N}_{t}\\\\right) }_{t \\\\geq 0}$ be a Poisson process with intensity $\\\\lambda > 0$ and ${\\\\left( {T}_{n}\\\\right) }_{n \\\\geq 1}$ its jump instants. Let ${\\\\left( {Y}_{n}\\\\right) }_{n \\\\geq 1}$ be a sequence of i.i.d. r.v.’s with values in $\\\\mathcal{I}$, independent of ${\\\\left( {N}_{t}\\\\right) }_{t \\\\geq 0}$ and such that $\\\\mathbb{P}\\\\left( {{Y}_{1} = i}\\\\right) = {p}_{i} > 0$ for $i$ in $\\\\mathcal{I}$ . Then the ${\\\\left( {N}_{t}^{i}\\\\right) }_{t \\\\geq 0}$ defined by\\n\\n$$ {N}_{t}^{i} \\\\mathrel{\\\\text{:=}} \\\\mathop{\\\\sum }\\\\limits_{{n \\\\geq 1}}{\\\\mathbb{1}}_{\\\\left\\\\{ {T}_{n} \\\\leq t,{Y}_{n} = i\\\\right\\\\} },\\\\;i \\\\in \\\\mathcal{I}, $$\\n\\nare independent Poisson processes with intensities ${\\\\lambda }_{i} = \\\\lambda {p}_{i}$, and are called the decomposition, or marking, of ${\\\\left( {N}_{t}\\\\right) }_{t \\\\geq 0}$ according to ${\\\\left( {Y}_{n}\\\\right) }_{n \\\\geq 1}$ .\\n\\nNotably, the sampling or erasing property holds: retaining among the instants of a Poisson process with intensity $\\\\lambda > 0$ only those marked in i.i.d. fashion with probability $p > 0$ yields a Poisson process with intensity ${\\\\lambda p}$ .",
        "informal_proof": "Proof (a) Let us first assume that $\\\\mathcal{I}$ is finite. Recall Definitions 4.1 and 4.2. The ${\\\\left( {N}_{t}^{i}\\\\right) }_{t \\\\in {\\\\mathbb{R}}_{ + }}$ are point processes with independent and stationary increments (Theorem 4.1) and are independent, hence their finite sum ${\\\\left( {N}_{t}\\\\right) }_{t \\\\in {\\\\mathbb{R}}_{ + }}$ is also such a process; the only fact which is not immediate is that it has unit jumps, i.e., that ${N}_{t} - {N}_{t - } \\\\in \\\\{ 0,1\\\\}$, which we now proceed to prove. For $\\\\varepsilon > 0$ and $k \\\\geq 1$ and $i \\\\neq j$ , independence implies that\\n\\n$$ \\\\mathbb{P}\\\\left( {{N}_{k\\\\varepsilon }^{i} - {N}_{\\\\left( {k - 1}\\\\right) \\\\varepsilon }^{i} \\\\geq 1,{N}_{k\\\\varepsilon }^{j} - {N}_{\\\\left( {k - 1}\\\\right) \\\\varepsilon }^{j} \\\\geq 1}\\\\right) = \\\\left( {1 - {\\\\mathrm{e}}^{{\\\\lambda }_{i}\\\\varepsilon }}\\\\right) \\\\left( {1 - {\\\\mathrm{e}}^{{\\\\lambda }_{j}\\\\varepsilon }}\\\\right) \\\\leq {\\\\lambda }_{i}{\\\\lambda }_{j}{\\\\varepsilon }^{2} $$\\n\\nwhich with (4.2) applied to the ${N}^{i}$ yields\\n\\n$$ \\\\mathbb{P}\\\\left( {{N}_{k\\\\varepsilon } - {N}_{\\\\left( {k - 1}\\\\right) \\\\varepsilon } \\\\geq 2}\\\\right) \\\\leq \\\\frac{1}{2}\\\\mathop{\\\\sum }\\\\limits_{{i \\\\in \\\\mathcal{I}}}{\\\\lambda }_{i}^{2}{\\\\varepsilon }^{2} + \\\\frac{1}{2}\\\\mathop{\\\\sum }\\\\limits_{{i \\\\neq j}}{\\\\lambda }_{i}{\\\\lambda }_{j}{\\\\varepsilon }^{2} = \\\\frac{1}{2}{\\\\lambda }^{2}{\\\\varepsilon }^{2}, $$\\n\\n$$ \\\\mathbb{P}\\\\left( {\\\\exists t \\\\in \\\\left\\\\lbrack {0, T}\\\\right\\\\rbrack : {N}_{t} - {N}_{t - } \\\\geq 2}\\\\right) \\\\leq \\\\frac{1}{2}\\\\lceil T/\\\\varepsilon \\\\rceil {\\\\lambda }^{2}{\\\\varepsilon }^{2}\\\\underset{\\\\varepsilon \\\\rightarrow 0}{ \\\\rightarrow }0,\\\\;\\\\forall T > 0. $$\\n\\nThen, Theorem 4.1 yields that ${\\\\left( {N}_{t}\\\\right) }_{t \\\\in {\\\\mathbb{R}}_{ + }}$ is a Poisson process with intensity $\\\\lambda$.",
        "id": "college_math_123653"
    },
    {
        "informal_statement": "Proposition 4.13 There are exactly \\( q \\) elements \\( \\alpha \\in {\\mathbb{F}}_{{q}^{m}} \\) satisfying \\( {\\alpha }^{q} = \\alpha \\) and these form the subfield \\( {\\mathbb{F}}_{q} \\) of \\( {\\mathbb{F}}_{{q}^{m}} \\) .",
        "informal_proof": "Proof We've already argued that the roots are closed under addition and multiplication. Thus the roots of \\( {x}^{q} - x \\) form a field. It suffices to argue that the cardinality of this field is \\( q \\) .\\n\\nIt is easy to see there are at most \\( q \\) elements \\( \\alpha \\) in \\( {\\mathbb{F}}_{{q}^{m}} \\) satisfying \\( {\\alpha }^{q} - \\alpha = 0 \\), since these are roots of the degree \\( q \\) polynomial \\( {x}^{q} - x \\) . To see that this polynomial has \\( q \\) roots, note that every element of \\( {\\mathbb{F}}_{{q}^{m}} \\) is a root of \\( {x}^{{q}^{m}} - x \\) . Furthermore we have \\( {x}^{{q}^{m}} - x = \\left( {{x}^{q} - x}\\right) \\cdot h\\left( x\\right) \\), where \\( h\\left( x\\right) = {x}^{{q}^{m} - q} + {x}^{{q}^{m} - {2q}} + \\cdots + {x}^{q} + 1 \\) is a polynomial of degree \\( {q}^{m} - q \\) . Thus every element of \\( {\\mathbb{F}}_{{q}^{m}} \\) is a either a root of the polynomial \\( {x}^{q} - x \\) or of \\( h\\left( x\\right) \\) . Since at most \\( {q}^{m} - q \\) elements can be roots of \\( h\\left( x\\right) \\), we have that \\( h\\left( x\\right) \\) has exactly \\( {q}^{m} - q \\) roots, and \\( {x}^{q} - x \\) has exactly \\( q \\) roots. \\( \\blacksquare \\)",
        "id": "college_math_205031"
    },
    {
        "informal_statement": "Theorem 10.1.7. Let \\( \\mu \\) be a fuzzy subgroup of \\( G \\) . Then \\( \\bar{\\mu } \\) is the largest fuzzy subgroup of \\( G \\) such that \\( \\bar{\\mu } \\subseteq \\mu \\) and \\( \\bar{\\mu } \\) is commutative, where \\( \\bar{\\mu } \\) is defined by \\( \\bar{\\mu }\\left( x\\right) = \\land \\left\\{ {\\mu \\left( {{zx}{z}^{-1}}\\right) \\mid z \\in G}\\right\\} \\forall x \\in G.",
        "informal_proof": "The following theorem follows from the observations made above and is easily proved directly as well from the fact that \\( \\sigma \\left( {\\left( {x, y}\\right) \\left( {{x}^{\\prime },{y}^{\\prime }}\\right) }\\right) = \\sigma \\left( {x{x}^{\\prime }, y{y}^{\\prime }}\\right) \\geq \\) \\( \\sigma \\left( {x{x}^{\\prime }, y{x}^{\\prime }}\\right) \\land \\sigma \\left( {y{x}^{\\prime }, y{y}^{\\prime }}\\right) = \\sigma \\left( {x, y}\\right) \\land \\sigma \\left( {{x}^{\\prime },{y}^{\\prime }}\\right) \\forall x, y,{x}^{\\prime },{y}^{\\prime } \\in G.",
        "id": "college_math_237008"
    },
    {
        "informal_statement": "Proposition 19. Let $\\\\partial : A \\\\rightarrow G$ and $\\\\delta : B \\\\rightarrow G$ be two pro- $\\\\mathcal{C}$ crossed modules and let $\\\\left( {\\\\phi ,{Id}}\\\\right) : \\\\left( {A, G,\\\\partial }\\\\right) \\\\rightarrow \\\\left( {B, G,\\\\delta }\\\\right)$ be a morphism in Pro-C.CMod/G. Then defining a continuous $B$ -action on $A$ by ${}^{b}a = {}^{\\\\delta \\\\left( b\\\\right) }a$, we have $\\\\left( {A, B,\\\\phi }\\\\right)$ is a pro- $\\\\mathcal{C}$ crossed module.",
        "informal_proof": "The proof is an easy exercise in using the two crossed module axioms.",
        "id": "college_math_98363"
    },
    {
        "informal_statement": "Example 2. It also follows from Corollary 2.11.2 that the function $f\\\\left( z\\\\right) = {\\\\left| z\\\\right| }^{2}$ , whose components are\\n\\n$$ u\\\\left( {x, y}\\\\right) = {x}^{2} + {y}^{2}\\\\text{ and }v\\\\left( {x, y}\\\\right) = 0, $$\\n\\nhas a derivative at $z = 0$ .",
        "informal_proof": "In fact, ${f}^{\\\\prime }\\\\left( 0\\\\right) = 0 + {i0} = 0$ . We saw in Example 2, Sec. 2.10, that this function cannot have a derivative at any nonzero point since the Cauchy-Riemann equations are not satisfied at such points.",
        "id": "college_math_147515"
    },
    {
        "informal_statement": "Theorem 5.3.11 (Change of basis for vectors). Let \\( V \\) be an \\( n \\) -dimensional vector space over a field \\( \\mathbb{F} \\), and let \\( \\mathcal{B} \\) and \\( \\mathcal{C} \\) be any two bases of \\( V \\) . Then there is an \\( n \\) -by- \\( n \\) matrix \\( {P}_{\\mathcal{C} \\leftarrow \\mathcal{B}} \\) with the property that\\n\\n\\[{\\left\\lbrack v\\right\\rbrack }_{\\mathcal{C}} = {P}_{\\mathcal{C} \\leftarrow \\mathcal{B}}{\\left\\lbrack v\\right\\rbrack }_{\\mathcal{B}}\\;\\text{ for every }v \\in V.\\]\\n\\nLet \\( \\mathcal{B} = \\left\\{ {{v}_{1},\\ldots ,{v}_{n}}\\right\\} \\) . Then\\n\\n\\[{P}_{\\mathcal{C} \\leftarrow \\mathcal{B}} = \\left\\lbrack {{\\left\\lbrack {v}_{1}\\right\\rbrack }_{\\mathcal{C}}\\left| {\\left\\lbrack {v}_{2}\\right\\rbrack }_{\\mathcal{C}}\\right| \\ldots \\mid {\\left\\lbrack {v}_{n}\\right\\rbrack }_{\\mathcal{C}}}\\right\\rbrack .",
        "informal_proof": "Proof. We know, by Lemma 5.3.3, that there is an isomorphism \\( {\\mathcal{T}}_{1} : V \\rightarrow {\\mathbb{F}}^{n} \\) given by \\( {\\mathcal{T}}_{1}\\left( v\\right) = {\\left\\lbrack v\\right\\rbrack }_{\\mathcal{B}} \\), and similarly that there is an isomorphism \\( {\\mathcal{T}}_{2} : V \\rightarrow {\\mathbb{F}}^{n} \\) given by \\( {\\mathcal{T}}_{2}\\left( v\\right) = {\\left\\lbrack v\\right\\rbrack }_{\\mathcal{C}} \\) . Let \\( \\mathcal{T} \\) be the composition \\( \\mathcal{T} = {\\mathcal{T}}_{2}{\\mathcal{T}}_{1}^{-1} : {\\mathbb{F}}^{n} \\rightarrow {\\mathbb{F}}^{n} \\) . Then \\( \\mathcal{T} \\) is a linear transformation with\\n\\n\\[ \\mathcal{T}\\left( {\\left\\lbrack v\\right\\rbrack }_{\\mathcal{B}}\\right) = {\\mathcal{T}}_{2}{\\mathcal{T}}_{1}^{-1}\\left( {\\left\\lbrack v\\right\\rbrack }_{\\mathcal{B}}\\right) = {\\mathcal{T}}_{2}\\left( {{\\mathcal{T}}_{1}^{-1}\\left( {\\left\\lbrack v\\right\\rbrack }_{\\mathcal{B}}\\right) }\\right) = {\\mathcal{T}}_{2}\\left( v\\right) = {\\left\\lbrack v\\right\\rbrack }_{\\mathcal{C}}.\\]\\n\\nBut we know that any linear transformation \\( \\mathcal{T} : {\\mathbb{F}}^{n} \\rightarrow {\\mathbb{F}}^{n} \\) is \\( \\mathcal{T} = {\\mathcal{T}}_{P} \\) for some (unique) matrix \\( P \\) . So we let \\( {P}_{\\mathcal{C} \\leftarrow \\mathcal{B}} \\) be this matrix \\( P \\) .\\n\\nWe know that\\n\\n\\[{P}_{\\mathcal{C} \\leftarrow \\mathcal{B}} = \\left\\lbrack {{P}_{\\mathcal{C} \\leftarrow \\mathcal{B}}{e}_{1}\\left| {{P}_{\\mathcal{C} \\leftarrow \\mathcal{B}}{e}_{2}}\\right| \\ldots \\mid {P}_{\\mathcal{C} \\leftarrow \\mathcal{B}}{e}_{n}}\\right\\rbrack .\\]\\n\\nBut remember that \\( {\\left\\lbrack {v}_{i}\\right\\rbrack }_{\\mathcal{B}} = {e}_{i} \\) . Then we see\\n\\n\\[{P}_{\\mathcal{C} \\leftarrow \\mathcal{B}}{e}_{i} = {P}_{\\mathcal{C} \\leftarrow \\mathcal{B}}{\\left\\lbrack {v}_{i}\\right\\rbrack }_{\\mathcal{B}} = {\\left\\lbrack {v}_{i}\\right\\rbrack }_{\\mathcal{C}}\\]",
        "id": "college_math_262577"
    },
    {
        "informal_statement": "Theorem 16.23. Given a measure space \\( \\left( {X,\\mathfrak{A},\\mu }\\right) \\) . Let \\( p \\in \\lbrack 1,\\infty ) \\) . Then \\( \\parallel \\cdot {\\parallel }_{p} \\) is a norm on the linear space \\( {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) over the field of scalars \\( \\mathbb{C} \\) and \\( {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) is a Banach space with respect to the norm \\( \\parallel \\cdot {\\parallel }_{p} \\) . We call \\( \\parallel \\cdot {\\parallel }_{p} \\) the \\( {L}^{p} \\) norm on \\( {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) .",
        "informal_proof": "Proof. 1. It is easily verified that the function \\( \\parallel \\cdot {\\parallel }_{p} \\) on \\( {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) satisfies the defining conditions of a norm:\\n\\n\\( {1}^{ \\circ }\\;\\parallel f{\\parallel }_{p} \\in \\lbrack 0,\\infty ) \\) for \\( f \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) ,\\)\\n\\n\\( {2}^{ \\circ }\\;\\parallel f{\\parallel }_{p} = 0 \\) if and only if \\( f = 0 \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) ,\\)\\n\\n\\( {3}^{ \\circ }\\;\\parallel {\\alpha f}{\\parallel }_{p} = \\left| \\alpha \\right| \\parallel f{\\parallel }_{p} \\) for \\( f \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) and \\( \\alpha \\in \\mathbb{C},\\)\\n\\n\\( {4}^{ \\circ }\\;\\parallel f + g{\\parallel }_{p} \\leq \\parallel f{\\parallel }_{p} + \\parallel g{\\parallel }_{p} \\) for \\( f, g \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) .\\)\\n\\nIn particular the fact that \\( \\parallel f{\\parallel }_{p} < \\infty \\) for every \\( f \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) is from the definition of \\( {\\mathcal{L}}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) as the collection of all extended complex-valued \\( \\mathfrak{A} \\) -measurable functions \\( f \\) on \\( X \\) with \\( {\\int }_{X}{\\left| f\\right| }^{p}{d\\mu } < \\infty \\) . Regarding \\( {2}^{ \\circ } \\), note that if \\( \\parallel f{\\parallel }_{p} = 0 \\) for some \\( f \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) then \\( f \\) is the equivalence class of all extended complex-valued \\( \\mathfrak{A} \\) -measurable functions which vanish a.e. on \\( X \\), that is, \\( f = 0 \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) . Condition \\( {4}^{ \\circ } \\) is from Theorem 16.17 (Minkowski’s Inequality).\\n\\n2. Let us show that \\( {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) is complete with respect to the norm \\( \\parallel \\cdot {\\parallel }_{p} \\) . According to Theorem 15.18, it suffices to show that for every sequence \\( \\left( {{f}_{n} : n \\in \\mathbb{N}}\\right) \\) in \\( {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) with \\( \\mathop{\\sum }\\limits_{{n \\in \\mathbb{N}}}{\\begin{Vmatrix}{f}_{n}\\end{Vmatrix}}_{p} < \\infty \\), the series \\( \\mathop{\\sum }\\limits_{{n \\in \\mathbb{N}}}{f}_{n} \\) converges in the norm, that is, there exists \\( g \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) such that \\( \\mathop{\\lim }\\limits_{{n \\rightarrow \\infty }}{\\begin{Vmatrix}{g}_{n} - g\\end{Vmatrix}}_{p} = 0 \\) where \\( {g}_{n} = \\mathop{\\sum }\\limits_{{k = 1}}^{n}{f}_{k} \\) for \\( n \\in \\mathbb{N} \\) .\\n\\nLet \\( \\left( {{f}_{n} : n \\in \\mathbb{N}}\\right) \\) be a sequence in \\( {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) with \\( B \\mathrel{\\text{:=}} \\mathop{\\sum }\\limits_{{n \\in \\mathbb{N}}}{\\begin{Vmatrix}{f}_{n}\\end{Vmatrix}}_{p} < \\infty \\) . Consider the sequence \\( \\left( {\\left| {f}_{n}\\right| : n \\in \\mathbb{N}}\\right) \\) . Now \\( {f}_{n} \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) implies that \\( \\left| {f}_{n}\\right| \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) and \\( {\\begin{Vmatrix}\\left| {f}_{n}\\right| \\end{Vmatrix}}_{p} = {\\begin{Vmatrix}{f}_{n}\\end{Vmatrix}}_{p} \\) . Thus \\( \\mathop{\\sum }\\limits_{{n \\in \\mathbf{N}}}{\\begin{Vmatrix}\\left| {f}_{n}\\right| \\end{Vmatrix}}_{p} = \\mathop{\\sum }\\limits_{{n \\in \\mathbf{N}}}{\\begin{Vmatrix}{f}_{n}\\end{Vmatrix}}_{p} = B < \\infty \\) . Let us show that there exists \\( h \\in {L}^{p}\\left( {X,\\mathfrak{A},\\mu }\\right) \\) such that \\( \\mathop{\\lim }\\limits_{{n \\rightarrow \\infty }}{\\begin{Vmatrix}{h}_{n} - h\\end{V",
        "id": "college_math_256314"
    },
    {
        "informal_statement": "Proposition 2.1 For any complex manifold \\( M \\), we have\\n\\n\\[ \\n{d}_{M}\\left( {p, q}\\right) \\geqq {c}_{M}\\left( {p, q}\\right) \\;\\text{ for }p, q \\in M.\\n\\]",
        "informal_proof": "Proof As in the definition of \\( {d}_{M}\\left( {p, q}\\right) \\), choose points \\( p = {p}_{0},{p}_{1},\\ldots \\) , \\( {p}_{k - 1},{p}_{k} = q \\) of \\( M \\) and points \\( {a}_{1},\\ldots ,{a}_{k},{b}_{1},\\ldots ,{b}_{k} \\) of \\( D \\) and also mappings \\( {f}_{1},\\ldots ,{f}_{k} \\) of \\( D \\) into \\( M \\) such that \\( {f}_{i}\\left( {a}_{i}\\right) = {p}_{i - 1} \\) and \\( {f}_{i}\\left( {b}_{i}\\right) = {p}_{i} \\) . Let \\( f \\) be a holomorphic mapping of \\( M \\) into \\( D \\) . Then\\n\\n\\[ \\n\\mathop{\\sum }\\limits_{{i = 1}}^{k}\\varrho \\left( {{a}_{i},{b}_{i}}\\right) \\geqq \\mathop{\\sum }\\limits_{{i = 1}}^{k}\\varrho \\left( {f \\circ {f}_{i}\\left( {a}_{i}\\right), f \\circ {f}_{i}\\left( {b}_{i}\\right) }\\right)\\n\\]\\n\\n\\[ \\n\\geqq \\varrho \\left( {f \\circ {f}_{1}\\left( {a}_{1}\\right), f \\circ {f}_{k}\\left( {b}_{k}\\right) }\\right)\\n\\]\\n\\n\\[ \\n= \\varrho \\left( {f\\left( p\\right), f\\left( q\\right) }\\right) ,\\n\\]\\n\\nwhere the first inequality follows from the Schwarz lemma and the second inequality is a consequence of the triangular axiom. Hence,\\n\\n\\[ \\n{d}_{M}\\left( {p, q}\\right) = \\inf \\mathop{\\sum }\\limits_{{i = 1}}^{k}\\varrho \\left( {{a}_{i},{b}_{i}}\\right) \\geqq \\sup \\varrho \\left( {f\\left( p\\right), f\\left( q\\right) }\\right) = {c}_{M}\\left( {p, q}\\right) .\\n\\]",
        "id": "college_math_178054"
    },
    {
        "informal_statement": "Theorem 2.2.9 (Carleman continuity theorem). Let ${X}_{n}$ be a sequence of uniformly sub-Gaussian real random variables, and let $X$ be another sub-Gaussian random variable. Then the following statements are equivalent:\\n\\n(i) For every $k = 0,1,2,\\ldots ,\\mathbf{E}{X}_{n}^{k}$ converges pointwise to $\\mathbf{E}{X}^{k}$ .\\n\\n(ii) ${X}_{n}$ converges in distribution to $X$ .",
        "informal_proof": "Proof. We first show how (ii) implies (i). Let $N > 0$ be a truncation parameter, and let $\\varphi : \\mathbf{R} \\rightarrow \\mathbf{R}$ be a smooth function that equals 1 on $\\left\\lbrack {-1,1}\\right\\rbrack$ and vanishes outside of $\\left\\lbrack {-2,2}\\right\\rbrack$ . Then for any $k$, the convergence in distribution implies that $\\mathbf{E}{X}_{n}^{k}\\varphi \\left( {{X}_{n}/N}\\right)$ converges to $\\mathbf{E}{X}^{k}\\varphi \\left( {X/N}\\right)$ . On the other hand, from the uniform sub-Gaussian hypothesis, one can make $\\mathbf{E}{X}_{n}^{k}\\left( {1 - \\varphi \\left( {{X}_{n}/N}\\right) }\\right)$ and $\\mathbf{E}{X}^{k}\\left( {1 - \\varphi \\left( {X/N}\\right) }\\right)$ arbitrarily small for fixed $k$ by making $N$ large enough. Summing, and then letting $N$ go to infinity, we obtain (i).\\n\\nConversely, suppose (i) is true. From the uniform sub-Gaussian hypothesis, the ${X}_{n}$ have ${\\left( k + 1\\right) }^{\\text{st }}$ moment bounded by ${\\left( Ck\\right) }^{k/2}$ for all $k \\geq 1$ and some $C$ independent of $k$ (see Exercise 1.1.4). From Taylor’s theorem with remainder (and Stirling's formula, Section 1.2) we conclude\\n\\n$${F}_{{X}_{n}}\\left( t\\right) = \\mathop{\\sum }\\limits_{{j = 0}}^{k}\\frac{{\\left( it\\right) }^{j}}{j!}\\mathbf{E}{X}_{n}^{j} + O\\left( {{\\left( Ck\\right) }^{-k/2}{\\left| t\\right| }^{k + 1}}\\right)$$\\n\\nuniformly in $t$ and $n$ . Similarly for $X$ . Taking limits using (i) we see that\\n\\n$$\\mathop{\\limsup }\\limits_{{n \\rightarrow \\infty }}\\left| {{F}_{{X}_{n}}\\left( t\\right) - {F}_{X}\\left( t\\right) }\\right| = O\\left( {{\\left( Ck\\right) }^{-k/2}{\\left| t\\right| }^{k + 1}}\\right) .$$\\n\\nThen letting $k \\rightarrow \\infty$, keeping $t$ fixed, we see that ${F}_{{X}_{n}}\\left( t\\right)$ converges pointwise to ${F}_{X}\\left( t\\right)$ for each $t$, and the claim now follows from the Lévy continuity theorem.",
        "id": "college_math_135906"
    },
    {
        "informal_statement": "Corollary 3.1.6 (Random laws seen as degenerate Young measures on $\\\\left. {{\\\\mathcal{M}}^{+,1}\\\\left( \\\\mathbb{T}\\\\right) }\\\\right)$ Assume that $\\\\mathbb{T}$ is cosmic regular. Then ${\\\\tau }_{\\\\text{prob }}\\\\left( {\\\\mathcal{Y}}_{\\\\text{dis }}^{1}\\\\right)$ is the restriction to $x\\\\left( {{\\\\mathcal{M}}^{+,1}\\\\left( \\\\mathbb{T}\\\\right) }\\\\right)$ of ${\\\\tau }_{{\\\\mathcal{Y}}^{1}\\\\left( {{\\\\mathcal{M}}^{+,1}\\\\left( \\\\mathbb{T}\\\\right) }\\\\right) }^{\\\\mathrm{W}}$ .",
        "informal_proof": "Proof. From Proposition 1.3.2, ${\\\\mathcal{M}}^{+,1}\\\\left( \\\\mathbb{T}\\\\right)$ is cosmic regular, thus it is separable, hereditarily Lindelöf and completely regular. The result follows by applying Part 3 of Theorem 3.1.2 to the space ${\\\\mathcal{M}}^{+,1}\\\\left( \\\\mathbb{T}\\\\right)$ .",
        "id": "college_math_144587"
    },
    {
        "informal_statement": "Lemma 1. The inequality $\\\\left( {{c}_{\\\\mu },{\\\\gamma }_{\\\\mu }}\\\\right)$ is valid for $\\\\operatorname{GTSP}\\\\left( n\\\\right)$ .",
        "informal_proof": "Proof. Since ${c}_{\\\\mu }$ is metric, the validity of $\\\\left( {{c}_{\\\\mu },{\\\\gamma }_{\\\\mu }}\\\\right)$ for $\\\\operatorname{GTSP}\\\\left( n\\\\right)$ follows from its validity for $\\\\operatorname{STSP}\\\\left( n\\\\right)$, which is given because it is a sum of valid inequalities.",
        "id": "college_math_95036"
    },
    {
        "informal_statement": "Proposition 5.27. The dimension of ${\\left( 0,1\\right) }^{d}$ is $d$ . The dimension of a cell semi-algebraically homeomorphic to ${\\left( 0,1\\right) }^{d}$ is $d$ .",
        "informal_proof": "Proof: There is no injective semi-algebraic map from ${\\left( 0,1\\right) }^{e}$ to ${\\left( 0,1\\right) }^{d}$ if $e > d$ . Otherwise, the composition of such a map with the embedding of ${\\left( 0,1\\right) }^{d}$ in ${\\mathrm{R}}^{e} = {\\mathrm{R}}^{d} \\times {\\mathrm{R}}^{e - d}$ as ${\\left( 0,1\\right) }^{d} \\times \\{ 0\\}$ would contradict Lemma 5.26. This shows the first part of the corollary. The second part follows, using the fact that the dimension, according to its definition, is invariant under semi-algebraic bijection.",
        "id": "college_math_15114"
    },
    {
        "informal_statement": "Proposition 6.2 The calculus \\( {\\mathbf{{TK}}}_{{\\omega }_{1}} \\) does not admit cut-elimination. For instance, the sequent\\n\\n\\[ \\n\\Delta \\mathrel{\\text{:=}} \\left\\{ {\\mathop{\\bigvee }\\limits_{n}\\neg {\\square }_{i}{q}_{n}^{k} \\mid k \\geq 0}\\right\\} ,{\\square }_{i}\\mathop{\\bigwedge }\\limits_{k}{q}_{k}^{k} \\n\\]\\n\\nis derivable, but not cut-free derivable, in \\( {\\mathbf{{TK}}}_{{\\omega }_{1}} \\) .",
        "informal_proof": "Proof \\( {\\mathbf{{TK}}}_{{\\omega }_{1}}{ \\nvdash }_{0}\\Delta \\) by Fact 6.1, since \\( \\Delta \\equiv {\\Delta }_{\\Phi } \\) with \\( \\Phi = \\varnothing \\) .\\n\\nOn the other side, \\( \\Delta \\) can be derived as follows by making use of a CUT with an appropriate instance of \\( B{F}_{{\\omega }_{1}} \\) (which we know being derivable in \\( {\\mathbf{{TK}}}_{{\\omega }_{1}} \\) ):\\n\\n![01918df4-9532-7444-aa32-6ad6777303c7_321_199840.jpg](images/01918df4-9532-7444-aa32-6ad6777303c7_321_199840.jpg)",
        "id": "college_math_258120"
    },
    {
        "informal_statement": "Theorem 1.5.1. Let $\\\\Gamma$ be a full lattice in $K$ .\\n\\n(a) There is some $m \\\\in \\\\mathbb{Q}$ and $\\\\gamma \\\\in K$ such that $\\\\Gamma = \\\\langle m,{m\\\\gamma }{\\\\rangle }_{\\\\mathbb{Z}}$ .\\n\\n(b) Let $a, b, c \\\\in \\\\mathbb{Z},\\\\gcd \\\\left( {a, b, c}\\\\right) = 1, a > 0$, such that $a{\\\\gamma }^{2} + {b\\\\gamma } + c = 0$ . Then ${a\\\\gamma } = h + {k\\\\omega } \\\\in {\\\\mathbb{Z}}_{K}$ and\\n\\n$$ O\\\\left( \\\\Gamma \\\\right) \\\\mathrel{\\\\text{:=}} \\\\{ x \\\\in K \\\\mid {x\\\\Gamma } \\\\subseteq \\\\Gamma \\\\} = \\\\langle 1,{a\\\\gamma }{\\\\rangle }_{\\\\mathbb{Z}} = \\\\langle 1,{k\\\\omega }{\\\\rangle }_{\\\\mathbb{Z}}. $$",
        "informal_proof": "Proof. (a) Is just the Hermite normal form for integral matrices: If $\\\\Gamma = \\\\langle \\\\alpha ,\\\\beta {\\\\rangle }_{\\\\mathbb{Z}}$, then there are $x, y \\\\in \\\\mathbb{Q}$ such that $1 = {x\\\\alpha } + {y\\\\beta }$ . Choose $m \\\\in \\\\mathbb{Q}$ such that $u \\\\mathrel{\\\\text{:=}} {mx}$ and $v \\\\mathrel{\\\\text{:=}} {my}$ both lie in $\\\\mathbb{Z}$ and $\\\\gcd \\\\left( {u, v}\\\\right) = 1$ . Then there are $r, s \\\\in \\\\mathbb{Z}$ such that $1 = {us} - {rv}$ . Put\\n\\n$$ \\\\gamma \\\\mathrel{\\\\text{:=}} \\\\frac{{r\\\\alpha } + {s\\\\beta }}{m},\\\\text{ then }\\\\Gamma = \\\\langle m,{m\\\\gamma }{\\\\rangle }_{\\\\mathbb{Z}}. $$\\n\\n(b) Clearly $O\\\\left( {\\\\langle m,{m\\\\gamma }{\\\\rangle }_{\\\\mathbb{Z}}}\\\\right) = O\\\\left( {\\\\langle 1,{1\\\\gamma }{\\\\rangle }_{\\\\mathbb{Z}}}\\\\right)$, so wlog assume that $m = 1$ . Then $O\\\\left( \\\\Gamma \\\\right)$ contains ${a\\\\gamma }$ , since both, ${a\\\\gamma }$ and $a{\\\\gamma }^{2} = - {b\\\\gamma } - c$ lie in $\\\\Gamma$ . On the other hand let $x + {y\\\\gamma } = : \\\\delta \\\\in O\\\\left( \\\\Gamma \\\\right)$ . Then $x + {y\\\\gamma } \\\\in \\\\Gamma$, so $x, y \\\\in \\\\mathbb{Z}$ and ${y\\\\gamma } \\\\in O\\\\left( \\\\Gamma \\\\right)$, so $y{\\\\gamma }^{2} \\\\in \\\\Gamma$ implying that $y$ is divisible by $a$ .",
        "id": "college_math_8765"
    },
    {
        "informal_statement": "Theorem 7 The objective in (67) admits the two-parameter topological expansion\\n\\n$$ J\\\\left( {\\\\omega ,\\\\varepsilon ,{x}_{0},\\\\alpha }\\\\right) = {J}_{0} + \\\\operatorname{Re}\\\\left\\\\{ {{\\\\varepsilon }^{2}{J}_{1}\\\\left( {\\\\omega ,{x}_{0},\\\\alpha }\\\\right) + {J}_{2}^{\\\\left( \\\\varepsilon ,\\\\alpha \\\\right) } + {J}_{3}^{\\\\left( \\\\varepsilon ,\\\\alpha \\\\right) } + {J}_{4}^{\\\\left( \\\\varepsilon ,\\\\alpha \\\\right) }}\\\\right\\\\}\\n$$\\n\\n$$ + \\\\mathrm{O}\\\\left( {{\\\\left| 1 - \\\\operatorname{Re}\\\\left( \\\\alpha \\\\right) \\\\right| }^{2}{\\\\varepsilon }^{4}\\\\left| {\\\\ln \\\\varepsilon }\\\\right| }\\\\right) \\\\tag{84} $$\\n\\nwith the first-order asymptotic term implying the topological derivative:\\n\\n$$ {J}_{1}\\\\left( {\\\\omega ,{x}_{0},\\\\alpha }\\\\right) \\\\mathrel{\\\\text{:=}} - \\\\nabla {u}^{0}{\\\\left( {x}_{0}\\\\right) }^{\\\\top }{A}_{\\\\left( \\\\omega ,\\\\operatorname{Re}\\\\left( \\\\alpha \\\\right) \\\\right) }\\\\nabla \\\\overline{{v}^{0}}\\\\left( {x}_{0}\\\\right) + \\\\left( {1 - \\\\alpha }\\\\right) {k}^{2}\\\\left| \\\\omega \\\\right| {u}^{0}\\\\left( {x}_{0}\\\\right) \\\\overline{{v}^{0}}\\\\left( {x}_{0}\\\\right) \\\\tag{85} $$\\n\\nand the high-order asymptotic terms expressed by the following formulas:\\n\\n$$ {J}_{2}^{\\\\left( \\\\varepsilon ,\\\\alpha \\\\right) } \\\\mathrel{\\\\text{:=}} {\\\\varepsilon }^{3}\\\\sqrt{\\\\left| \\\\ln \\\\varepsilon \\\\right| }{\\\\int }_{-\\\\pi }^{\\\\pi }\\\\nabla {u}^{0}\\\\left( {x}_{0}\\\\right) \\\\cdot \\\\left( {\\\\frac{\\\\partial {u}^{1}}{\\\\partial \\\\rho }\\\\overline{{v}^{0}}\\\\left( {x}_{0}\\\\right) - {u}^{1}\\\\left( {\\\\nabla \\\\overline{{v}^{0}}\\\\left( {x}_{0}\\\\right) \\\\cdot \\\\widehat{x}}\\\\right) }\\\\right) {d\\\\theta }\\n$$\\n\\n$$ = \\\\mathrm{O}\\\\left( {\\\\left| {1 - \\\\operatorname{Re}\\\\left( \\\\alpha \\\\right) }\\\\right| {\\\\varepsilon }^{3}\\\\sqrt{\\\\left| \\\\ln \\\\varepsilon \\\\right| }}\\\\right) , \\\\tag{86} $$\\n\\n$$ {J}_{3}^{\\\\left( \\\\varepsilon ,\\\\alpha \\\\right) } \\\\mathrel{\\\\text{:=}} - {\\\\int }_{{\\\\omega }_{\\\\varepsilon }\\\\left( {x}_{0}\\\\right) }\\\\left( {\\\\left( {1 - \\\\operatorname{Re}\\\\left( \\\\alpha \\\\right) }\\\\right) \\\\nabla {q}^{\\\\varepsilon } \\\\cdot \\\\nabla \\\\overline{{v}^{0}} - \\\\left( {1 - \\\\alpha }\\\\right) {k}^{2}{q}^{\\\\varepsilon }\\\\overline{{v}^{0}}}\\\\right) {dx}\\n$$\\n\\n$$ - \\\\left( {1 - \\\\operatorname{Re}\\\\left( \\\\alpha \\\\right) }\\\\right) {\\\\int }_{\\\\partial {\\\\omega }_{\\\\varepsilon }\\\\left( {x}_{0}\\\\right) }n \\\\cdot \\\\left( {{b}_{u}^{0} + \\\\nabla {U}_{1}^{0}}\\\\right) \\\\overline{{V}_{0}^{0}}d{S}_{x} - \\\\mathrm{i}\\\\operatorname{Im}\\\\left( \\\\alpha \\\\right) {\\\\int }_{{\\\\omega }_{\\\\varepsilon }\\\\left( {x}_{0}\\\\right) }{k}^{2}\\\\left( {{U}_{0}^{0}\\\\overline{{v}^{0}}\\\\left( {x}_{0}\\\\right) }\\\\right.\\n$$\\n\\n$$ \\\\left. {+{u}^{0}\\\\left( {x}_{0}\\\\right) \\\\overline{{V}_{0}^{0}}}\\\\right) {dx} + {\\\\varepsilon }^{2}{\\\\int }_{-\\\\pi }^{\\\\pi }\\\\nabla {u}^{0}\\\\left( {x}_{0}\\\\right) \\\\cdot \\\\left( {\\\\frac{\\\\partial {W}^{\\\\varepsilon }}{\\\\partial \\\\rho }\\\\overline{{V}_{1}^{0}} - {W}^{\\\\varepsilon }\\\\frac{\\\\partial \\\\overline{{V}_{1}^{0}}}{\\\\partial \\\\rho }}\\\\right) {d\\\\theta }\\n$$\\n\\n$$ - \\\\varepsilon {\\\\int }_{{B}_{\\\\varepsilon }\\\\left( {x}_{0}\\\\right) }{\\\\chi }_{{\\\\omega }_{\\\\varepsilon }\\\\left( {x}_{0}\\\\right) }^{\\\\alpha }{k}^{2}\\\\nabla {u}^{0}\\\\left( {x}_{0}\\\\right) \\\\cdot {w}^{\\\\varepsilon }\\\\overline{{v}^{0}}{dx} = \\\\mathrm{O}\\\\left\\\\{ {\\\\left( {\\\\left| {1 - \\\\operatorname{Re}\\\\left( \\\\alpha \\\\right) }\\\\right| \\\\left( {\\\\left| {1 - \\\\alpha }\\\\right| + \\\\operatorname{Re}\\\\left( \\\\alpha \\\\right) }\\\\right) }\\\\right) {\\\\varepsilon }^{3}}\\\\right\\\\} \\\\tag{87} $$\\n\\n$$ {J}_{4}^{\\\\left( \\\\varepsilon ,\\\\alpha \\\\right) } \\\\mathrel{\\\\text{:=}} {\\\\varepsilon }^{3}\\\\sqrt{\\\\left| \\\\ln \\\\varepsilon \\\\right| }{\\\\int }_{-\\\\pi }^{\\\\pi }\\\\nabla {u}^{0}\\\\left( {x}_{0}\\\\right) \\\\cdot \\\\left\\\\{ {\\\\frac{\\\\partial {u}^{1}}{\\\\partial \\\\rho }\\\\varepsilon \\\\left( {\\\\nabla \\\\overline{{v}^{0}}\\\\left( {x}_{0}\\\\right) \\\\cdot \\\\widehat{x}}\\\\right) }\\\\right.\\n$$\\n\\n$$ \\\\left. {-{u}^{1}\\\\left( {\\\\overline{{v}^{0}}\\\\left( {x}_{0}\\\\right) k{a}_{0}^{\\\\prime } + \\\\frac{\\\\partial \\\\overline{{V}_{1}^{0}}}{\\\\partial \\\\rho }}\\\\right) }\\\\right\\\\} {d\\\\theta} = \\\\mathrm{O}\\\\left( {\\\\left| {1 - \\\\operatorname{Re}\\\\left( \\\\alpha \\\\right) }\\\\right| {\\\\varepsilon }^{4}\\\\sqrt{\\\\left| \\\\ln \\\\varepsilon \\\\right| }}\\\\right) . \\\\tag{88} $$",
        "informal_proof": "Proof Due to ${u}^{1} = \\\\mathrm{O}\\\\left( \\\\left| {1 - \\\\operatorname{Re}\\\\left( \\\\alpha \\\\right) }\\\\right| \\\\right)$ in (59), the asymptotic rate of ${J}_{\\\\left(",
        "id": "college_math_99908"
    },
    {
        "informal_statement": "Example 4.6. \\( P = {P}_{n, n} \\) . The incomparability graph is the complete graph. Each proper coloring \\( c \\) is an injective map which can be associated with a permutation \\( \\sigma \\in {\\mathfrak{S}}_{n} \\) for which \\( \\operatorname{inv}\\left( \\sigma \\right) = \\operatorname{asc}\\left( c\\right) \\) .",
        "informal_proof": "It follows that\\n\\n\\[ \\n{X}_{\\mathrm{{inc}}\\left( {P}_{n, n}\\right) }\\left( {\\mathbf{x}, t}\\right) = {e}_{n}\\mathop{\\sum }\\limits_{{\\sigma \\in {\\mathfrak{S}}_{n}}}{t}^{\\mathrm{{inv}}\\left( \\sigma \\right) } = {\\left\\lbrack n\\right\\rbrack }_{t}!{e}_{n}. \\tag{4.3} \\n\\]",
        "id": "college_math_188493"
    },
    {
        "informal_statement": "Lemma 3.31. For $\\vartriangleleft \\in {\\mathcal{P}}_{n}$ and $\\vartriangleleft \\in {\\mathrm{{WOEP}}}_{n}$, we have\\n\\n- ${ \\vartriangleleft }^{\\text{WOEPid }} = \\blacktriangleleft \\Leftrightarrow$ for all $\\left( {a, c}\\right) \\in {\\left( \\vartriangleleft \\smallsetminus \\blacktriangleleft \\right) }^{\\text{Inc }}$ there exists $a < b < c$ such that $a \\blacktriangleright b \\blacktriangleright c$ ,\\n\\n- ${ \\vartriangleleft }^{\\text{WOEPdd }} = \\blacktriangleleft \\Leftrightarrow$ for all $\\left( {c, a}\\right) \\in {\\left( \\vartriangleleft \\smallsetminus \\blacktriangleleft \\right) }^{\\text{Dec }}$ there exists $a < b < c$ such that $a \\blacktriangleleft b \\blacktriangleleft c$ .",
        "informal_proof": "Proof. We only prove the first statement, the second is symmetric. Assume ${ \\vartriangleleft }^{\\text{WOEPid }} = \\blacktriangleleft$ and $\\left( {a, c}\\right) \\in {\\left( \\vartriangleleft \\smallsetminus \\blacktriangleleft \\right) }^{\\text{Inc }}$ . Since $\\left( {a, c}\\right)$ is deleted in ${ \\vartriangleleft }^{\\text{WOEPid }} =$ ${\\left( { \\vartriangleleft }^{\\text{IWOIPid }}\\right) }^{\\text{maxle }}$, it is already deleted in ${ \\vartriangleleft }^{\\text{IWOIPid }}$ . Therefore, there exists $a < {b}_{1} <$ $\\cdots < {b}_{k} < c$ such that $a \\ntriangleleft {b}_{1} \\ntriangleleft \\cdots \\ntriangleleft {b}_{k} \\ntriangleleft c$ . By definition of $\\blacktriangleleft = \\left( { \\vartriangleleft }^{\\text{IWOIPid }}\\right.$ ${)}^{\\text{maxle }}$, this implies that $a \\blacktriangleright {b}_{1} \\blacktriangleright \\ldots \\blacktriangleright {b}_{k} \\blacktriangleright c$ .\\n\\nConversely, assume that for all $\\left( {a, c}\\right) \\in {\\left( \\vartriangleleft \\smallsetminus \\blacktriangleleft \\right) }^{\\text{lnc }}$ there exists $a < b < c$ such that $a \\blacktriangleright b \\blacktriangleright c$ . Assume by contradiction that ${ \\vartriangleleft }^{\\text{WOEPid }} \\neq \\blacktriangleleft$ . By definition of ${ \\vartriangleleft }^{\\text{WOEPid }} = {\\left( { \\vartriangleleft }^{\\text{IWOIPid }}\\right) }^{\\text{maxle }}$, this implies that there exists $\\left( {a, c}\\right) \\in \\left( {{ \\vartriangleleft }^{\\text{IWOIPid }} \\smallsetminus }\\right.$ $\\vartriangleleft {)}^{\\text{lnc }}$ . Choose such an $\\left( {a, c}\\right)$ with $c - a$ minimal. Since $\\left( {a, c}\\right) \\in {\\left( { \\vartriangleleft }^{\\text{IWOIPid }} \\smallsetminus \\blacktriangleleft \\right) }^{\\text{lnc }} \\subseteq$ ${\\left( \\vartriangleleft \\smallsetminus \\blacktriangleleft \\right) }^{\\text{lnc }}$, there exists $a < b < c$ such that $a \\blacktriangleright b \\blacktriangleright c$ . Since $a{ \\vartriangleleft }^{\\text{IWOIPid }}c$, we have $a{ \\vartriangleleft }^{\\text{IWOIPid }}b$ or $b{ \\vartriangleleft }^{\\text{IWOIPid }}c$, thus either $\\left( {a, b}\\right)$ or $\\left( {b, c}\\right)$ belongs to $\\left( {{ \\vartriangleleft }^{\\text{IWOIPid }} \\smallsetminus }\\right.$ $\\blacktriangleleft {)}^{\\text{Inc }}$ contradicting the minimality of $c - a$ .",
        "id": "college_math_6259"
    },
    {
        "informal_statement": "Theorem 16. Let $l$ be an arbitrary state of an irreducible recurrent Markov chain. The system (20) admits a non-negative solution\\n\\n$$ \\n{x}_{l} = 1,\\;{x}_{i} = {}_{l}G\\\\left( {l, i}\\\\right) \\\\;\\\\left( {i \\\\neq l}\\\\right) ,\\\\;i \\\\in I.\\n$$",
        "informal_proof": "Proof. Set\\n\\n$$ \\n{u}_{l} = 1,\\;{u}_{i} = {}_{l}G\\\\left( {l, i}\\\\right) \\\\;\\\\left( {i \\\\neq l}\\\\right) . \\\\tag{37}\\n$$\\n\\nWe have for $i \\\\neq l$\\n\\n$$ \\n\\\\mathop{\\\\sum }\\\\limits_{{j \\\\in I}}{u}_{j}p\\\\left( {j, i}\\\\right) = p\\\\left( {l, i}\\\\right) + \\\\mathop{\\\\sum }\\\\limits_{{j \\\\neq l}}{lG}\\\\left( {l, j}\\\\right) p\\\\left( {j, i}\\\\right) =\\n$$\\n\\n$$ \\n= p\\\\left( {l, i}\\\\right) + \\\\mathop{\\\\sum }\\\\limits_{{j \\\\neq l}}\\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{\\\\infty }{}_{l}{p}^{\\\\left( n\\\\right) }\\\\left( {l, j}\\\\right) p\\\\left( {j, i}\\\\right) =\\n$$\\n\\n$$ \\n= p\\\\left( {l, i}\\\\right) + \\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{\\\\infty }{}_{l}{p}^{\\\\left( n + 1\\\\right) }\\\\left( {l, i}\\\\right) = {}_{l}G\\\\left( {l, i}\\\\right) = {u}_{i},\\n$$\\n\\nif however $i = l$, then\\n\\n$$ \\n\\\\mathop{\\\\sum }\\\\limits_{{j \\\\in I}}{u}_{j}p\\\\left( {j, l}\\\\right) = p\\\\left( {l, l}\\\\right) + \\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{\\\\infty }{f}^{\\\\left( n + 1\\\\right) }\\\\left( {l, l}\\\\right) = \\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{\\\\infty }{f}^{\\\\left( n\\\\right) }\\\\left( {l, l}\\\\right) = {u}_{l}.\\n$$\\n\\nand the theorem is proved.",
        "id": "college_math_134224"
    },
    {
        "informal_statement": "Example 5.1.13 After staring at the graph of Figure 29 we observe that it has an optimum branching \\( {B}^{\\prime } \\) of \\( {w}^{\\prime } \\) weight 13 . It is indicated with bold edges. The formula of the theorem now holds because",
        "informal_proof": "\\[ {33} - {13} = \\left( {8 + {18}}\\right) - \\left( {1 + 5}\\right) . \\] We get a branching in the original graph by for each cycle, extending \\( {B}^{\\prime } \\) with all but one of the edges in \\( {C}_{1} \\) and all but one of the edges in \\( {C}_{2} \\) . We do this in such a way that for the first cycle \\( {C}_{1} \\) with no ingoing edges in \\( {B}^{\\prime } \\), we add all but the smallest weight edge of \\( {C}_{1} \\) . For cycle \\( {C}_{2} \\) we add the edges which do not have head equal to the vertex of \\( {C}_{2} \\) already being a the head of an arc in \\( {B}^{\\prime } \\) .",
        "id": "college_math_218956"
    },
    {
        "informal_statement": "Corollary 147. Let $\\\\mathbf{R}$ be a Bezout domain, $m \\\\geq 2$, and $u = \\\\left( {{u}_{1},\\\\ldots ,{u}_{m}}\\\\right) \\\\in {\\\\mathbf{R}}^{m} \\\\smallsetminus \\\\{ 0\\\\}$ . Then, denoting by $d = \\\\gcd \\\\left( {{u}_{1}\\\\ldots ,{u}_{m}}\\\\right) ,\\\\operatorname{Syz}\\\\left( u\\\\right)$ is generated as $\\\\mathbf{R}$ -module by the obvious syzygies of $\\\\frac{1}{d}u$ .",
        "informal_proof": "Proof. This follows from Proposition 146 since $\\\\operatorname{Syz}\\\\left( u\\\\right) = \\\\operatorname{Syz}\\\\left( {\\\\frac{1}{d}u}\\\\right)$ and $\\\\frac{1}{d}u$ is unimodular.",
        "id": "college_math_48597"
    },
    {
        "informal_statement": "Problem 1.7.10. Can the number obtained by writing the numbers from 1 to \\( n \\) in order \\( \\left( {n > 1}\\right) \\) be the same when read left-to-right and right-to-left?",
        "informal_proof": "Solution. This is not possible. Suppose \\( N = {123}\\ldots {321} \\) is an \\( m \\) -digit symmetric number, formed by writing the numbers from 1 to \\( n \\) in succession. Clearly \\( m > {18} \\) . Also let \\( A \\) and \\( B \\) be the numbers formed from the first and last \\( k \\) digits, respectively, of \\( N \\), where \\( k = \\lfloor m/2\\rfloor \\) . If \\( {10}^{p} \\) is the largest power of 10 dividing \\( A \\), then \\( n < 2 \\cdot {10}^{p + 1} \\), that is, \\( n \\) has at most \\( p + 2 \\) digits. Moreover, \\( A \\) and \\( B \\) must contain the fragments\\n\\n![01917fb1-7342-7ab9-8150-93264d6ead1d_61_295591.jpg](images/01917fb1-7342-7ab9-8150-93264d6ead1d_61_295591.jpg)\\n\\nrespectively, which is impossible.",
        "id": "college_math_256508"
    },
    {
        "informal_statement": "Proposition 3.2. Suppose that $\\\\mathcal{C}\\\\left( K\\\\right) \\\\sim \\\\mathcal{C}\\\\left( L\\\\right)$ . Then ${\\\\mathcal{L}}^{K}/{\\\\mathcal{L}}_{\\\\mathrm{{wc}}}^{K} \\\\cong {\\\\mathcal{L}}^{L}/{\\\\mathcal{L}}_{\\\\mathrm{{wc}}}^{L}$ .",
        "informal_proof": "Proof. Define a map $\\\\theta : {\\\\mathcal{L}}^{K} \\\\rightarrow {\\\\mathcal{L}}^{L}/{\\\\mathcal{L}}_{\\\\text{wc }}^{L}$ by setting $\\\\theta \\\\left( T\\\\right) \\\\mathrel{\\\\text{:=}} {JT}{J}^{-1} + {\\\\mathcal{L}}_{\\\\text{wc }}^{L}$, where $J : \\\\mathcal{C}\\\\left( K\\\\right) \\\\rightarrow \\\\mathcal{C}\\\\left( L\\\\right)$ is an isomorphism. Then $\\\\theta$ is a ring homomorphism, $\\\\operatorname{Ker}\\\\left( \\\\theta \\\\right) = \\\\left\\\\{ {T \\\\in {\\\\mathcal{L}}^{K} : {JT}{J}^{-1} \\\\in {\\\\mathcal{L}}_{\\\\mathrm{{wc}}}^{L}}\\\\right\\\\} = {\\\\mathcal{L}}_{\\\\mathrm{{wc}}}^{K}$ and $\\\\operatorname{Im}\\\\left( \\\\theta \\\\right) = {\\\\mathcal{L}}^{L}/{\\\\mathcal{L}}_{\\\\mathrm{{wc}}}^{L}$ . The result follows from the first isomorphism theorem for rings. $\\\\;\\\\square$",
        "id": "college_math_78950"
    },
    {
        "informal_statement": "Lemma 1.2.6. If \\( R \\) is complete at \\( I \\) and \\( u \\in R \\) is invertible modulo \\( I \\), then \\( u \\) is invertible.",
        "informal_proof": "Proof. By hypothesis there is an element \\( y \\in R \\) with \\( a = 1 - {uy} \\in I \\) . Put \\( {s}_{n} \\mathrel{\\text{:=}} \\) \\( 1 + a + {a}^{2} + \\cdots + {a}^{n} \\) . Then \\( \\left\\{ {s}_{n}\\right\\} \\) is a strong Cauchy sequence, which therefore converges to some element \\( s \\in R \\) . Since \\( \\left( {1 - a}\\right) {s}_{n} = 1 - {a}^{n + 1} \\), we obtain \\( \\left( {1 - a}\\right) s = \\) 1 and thus \\( {u}^{-1} = {ys} \\) .",
        "id": "college_math_162769"
    },
    {
        "informal_statement": "Theorem 7.1. With the above notation we have\\n\\n$$ \\n\\\\mathop{\\\\sup }\\\\limits_{x}\\\\left| {{P}_{T}f\\\\left( x\\\\right) - {E}_{{Q}_{\\\\mathcal{D}, x}^{\\\\left( k\\\\right) }}f}\\\\right| \\\\leq {C}_{T}\\\\parallel \\\\nabla f{\\\\parallel }_{\\\\infty }\\\\left( {{s}_{k}^{1/2} + \\\\mathop{\\\\sum }\\\\limits_{{i = 1}}^{{k - 1}}\\\\frac{{s}_{i}^{\\\\left( {m + 1}\\\\right) /2}}{{\\\\left( T - {t}_{i}\\\\right) }^{m/2}}}}\\\\right) ,\\n$$ \\n\\nwhere ${C}_{T}$ is a constant independent of $f$ .",
        "informal_proof": "In other words compared to the KLV method introduced in section 6 the error bound for the approximation of ${P}_{T}f$ is only increased by a constant factor. The proof of the theorem is an easy consequence of the estimates obtained for the KLV method in [13] and Proposition 7.1 and may be found in [12].",
        "id": "college_math_122437"
    },
    {
        "informal_statement": "Example 3.25 Suppose that a single die is rolled over and over. How many rolls are needed so that the probability of rolling a six three times within this number of rolls is at least 50%?",
        "informal_proof": "Solution. Let the random variable \\( X \\) be defined as the number of rolls of the die until a six appears for the third time. Then the random variable \\( X \\) has a negative binomial distribution with parameters \\( r = 3 \\) and \\( p = \\frac{1}{6} \\) . We are asking for the smallest integer \\( k \\) for which \\( P\\left( {X \\leq k}\\right) \\geq {0.5}.{}^{11} \\) The smallest integer \\( k \\) for which\\n\\n\\[ \\mathop{\\sum }\\limits_{{j = 3}}^{k}\\left( \\begin{matrix} j - 1 \\\\ 2 \\end{matrix}\\right) {\\left( \\frac{1}{6}\\right) }^{3}{\\left( \\frac{5}{6}\\right) }^{j - 3} \\geq {0.5} \\]\\n\\nis given by \\( k = {16} \\) . Hence,16 rolls are needed.",
        "id": "college_math_220908"
    },
    {
        "informal_statement": "Lemma 10.3.1. All norms on \\( V = {\\mathbb{K}}^{n} \\) are equivalent. In other words, if \\( {N}_{1} \\) and \\( {N}_{2} \\) are two norms on \\( V = {\\mathbb{K}}^{n} \\), there exist constants \\( \\gamma > 0 \\) and \\( \\Gamma \\geq \\gamma \\) such that\\n\\n(10.3.2)\\n\\n\\[ \\gamma {N}_{1}\\left( x\\right) \\leq {N}_{2}\\left( x\\right) \\leq \\Gamma {N}_{1}\\left( x\\right) ,\\;\\forall x \\in {\\mathbb{K}}^{n}. \\]\\n",
        "informal_proof": "Proof. Let \\( S \\) be the unit sphere for the norm \\( {\\left| \\cdot \\right| }_{\\infty } \\) ; it is closed and bounded for the topology of \\( V \\), and hence compact. Relation (10.3.1) shows that \\( {N}_{1} \\) is continuous over \\( V \\) . Therefore, it attains its minimum and its maximum over \\( S \\) :\\n\\n\\[ {\\gamma }_{1} \\leq {N}_{1}\\left( x\\right) \\leq {\\Gamma }_{1},\\;\\forall x \\in S. \\]\\n\\nFurthermore, \\( {\\gamma }_{1} \\) cannot be zero, since there exists \\( y \\in S \\) such that \\( {\\gamma }_{1} = {N}_{1}\\left( y\\right) \\) . If \\( {\\gamma }_{1} \\) were zero, then the definition of the norm would imply that \\( y \\) would be zero, and we could not have \\( {\\left| y\\right| }_{\\infty } = 1 \\) . If \\( x \\neq 0 \\), we see that\\n\\n\\[ {\\gamma }_{1} \\leq {N}_{1}\\left( \\frac{x}{{\\left| x\\right| }_{\\infty }}\\right) \\leq {\\Gamma }_{1} \\]\\n\\nConsequently,\\n\\n\\[ {\\gamma }_{1}{\\left| x\\right| }_{\\infty } \\leq {N}_{1}\\left( x\\right) \\leq {\\Gamma }_{1}{\\left| x\\right| }_{\\infty }. \\]\\n\\nSimilarly, we have the following inequality for \\( {N}_{2} \\) :\\n\\n\\[ {\\gamma }_{2}{\\left| x\\right| }_{\\infty } \\leq {N}_{2}\\left( x\\right) \\leq {\\Gamma }_{2}{\\left| x\\right| }_{\\infty }. \\]\\n\\nIf we let \\( \\gamma = {\\gamma }_{2}/{\\Gamma }_{1} \\) and \\( \\Gamma = {\\Gamma }_{2}/{\\gamma }_{1} \\), we obtain the conclusion of the lemma.",
        "id": "college_math_224831"
    },
    {
        "informal_statement": "Theorem 1.1.5. Let $F : {\\\\mathbb{R}}^{n} \\\\times {\\\\mathbb{R}}^{n} \\\\rightarrow {\\\\mathbb{R}}^{n}$ be a smooth map. Consider the differential equations\\n\\n$$ \\n\\\\frac{{d}^{2}x}{d{t}^{2}} = F\\\\left( {x,\\\\dot{x}}\\\\right)\\n$$ \\n\\nfor $x : I \\\\subset \\\\mathbb{R} \\\\rightarrow {\\\\mathbb{R}}^{n}$ . Then for each point $\\\\left( {x, v}\\\\right) \\\\in {\\\\mathbb{R}}^{n} \\\\times {\\\\mathbb{R}}^{n}$ there exists a neighborhood $U \\\\times V$ of $\\\\left( {x, v}\\\\right)$ and an $\\\\varepsilon > 0$ such that for $\\\\left( {x, v}\\\\right) \\\\in U \\\\times V$, the above equation has a unique solution ${x}_{v} : \\\\left( {-\\\\varepsilon ,\\\\varepsilon }\\\\right) \\\\rightarrow {\\\\mathbb{R}}^{n}$ with the initial conditions: ${x}_{v}\\\\left( 0\\\\right) = x$ and ${x}_{v}^{\\\\prime }\\\\left( 0\\\\right) = v$ . Moreover the map $X : U \\\\times V \\\\times \\\\left( {-\\\\varepsilon ,\\\\varepsilon }\\\\right) \\\\rightarrow {\\\\mathbb{R}}^{n}$ defined by $X\\\\left( {x, v, t}\\\\right) = {x}_{v}\\\\left( t\\\\right)$ is smooth.",
        "informal_proof": "Proof. Introduce the new variables ${v}_{i} = \\\\frac{d{x}_{i}}{dt}$ . Then this system of $n$ second order equations becomes a system of ${2n}$ first order equations:\\n\\n$$ \\n\\\\frac{dx}{dt} = v\\\\;\\\\& \\\\;\\\\frac{dv}{dt} = F\\\\left( {x, v}\\\\right)\\n$$ \\n\\nThe result now follows from the last result.",
        "id": "college_math_111586"
    },
    {
        "informal_statement": "Suppose that $M$ is a $d$ -dimensional ${C}^{\\left( 2\\right) }$ -differential manifold in a Euclidean space $E$ . We define the tangent bundle $T\\left( M\\right)$ of $M$ to be the subset $\\left\\{ {\\left( {x, v}\\right) : x \\in M, v \\in {T}_{x}}\\right\\}$ of $E \\times E$ . Let us show that $T\\left( M\\right)$ is a ${2d}$ -dimensional differential manifold in $E \\times E$ . Suppose that $x \\in M$, that ${U}_{x}$ is an open neighhbourhood of $x$ in $E$ and that $g : {U}_{x} \\rightarrow F$ is a submersion for which $M \\cap {U}_{x} = \\left\\{ {y \\in {U}_{x} : g\\left( y\\right) = 0}\\right\\}$ . Define $G : {U}_{x} \\times E \\rightarrow F \\times F$ by setting $G\\left( {y, v}\\right) = \\left( {g\\left( y\\right), D{g}_{y}\\left( v\\right) }\\right)$ . Then\\n\\n$$ T\\left( M\\right) \\cap \\left( {{U}_{x} \\times E}\\right) = \\{ \\left( {y, v}\\right) : G\\left( \\left( {y, v}\\right) \\right) = 0\\} .",
        "informal_proof": "We must show that $G$ is a submersion. $G$ is continuously differentiable, and the matrix of partial derivatives is\\n\\n$$ \\left\\lbrack \\begin{matrix} D{g}_{y} & 0 \\\\ {D}^{2}{g}_{y}\\left( {v, \\cdot }\\right) & D{g}_{y} \\end{matrix}\\right\\rbrack $$\\n\\nSince $D{g}_{y}$ has rank $d, D{G}_{\\left( y, v\\right) }$ has rank ${2d}$, and so $G$ is a submersion.",
        "id": "college_math_27858"
    },
    {
        "informal_statement": "Corollary 3.4.16. An even dimensional sphere, \\( {S}^{n} \\), cannot admit a nonzero continuous tangent vector field.",
        "informal_proof": "Proof. Here\\n\\n\\[ \\n{H}^{k}\\left( {{S}^{n},\\mathbb{Q}}\\right) = \\left\\{ {\\begin{array}{ll} \\mathbb{Q} & k = n \\\\ 0 & k \\neq n \\end{array}.}\\right. \\]\\n\\nTherefore, \\( L\\left( f\\right) = \\chi \\left( {S}^{n}\\right) = 1 + {\\left( -1\\right) }^{n} = 2 \\neq 0 \\) .",
        "id": "college_math_284069"
    },
    {
        "informal_statement": "Example 5 Construct the Taylor polynomial of degree 10 about \\( x = 0 \\) for the function \\( f\\left( x\\right) = {e}^{x} \\) . Check the accuracy of this approximation at \\( x = 1 \\) .",
        "informal_proof": "Solution We have \\( f\\left( 0\\right) = 1 \\) . Since the derivative of \\( {e}^{x} \\) is equal to \\( {e}^{x} \\), all the higher-order derivatives are equal to \\( {e}^{x} \\) . Consequently, for any \\( k = 1,2,\\ldots ,{10},{f}^{\\left( k\\right) }\\left( x\\right) = {e}^{x} \\) and \\( {f}^{\\left( k\\right) }\\left( 0\\right) = {e}^{0} = 1 \\) . Therefore, the Taylor polynomial approximation of degree 10 is given by\\n\\n\\[ \\n{e}^{x} \\approx {P}_{10}\\left( x\\right) = 1 + x + \\frac{{x}^{2}}{2!} + \\frac{{x}^{3}}{3!} + \\frac{{x}^{4}}{4!} + \\cdots + \\frac{{x}^{10}}{{10}!},\\;\\text{ for }x\\text{ near }0.\\n\\]\\n\\nTo check the accuracy of the approximation, we substitute \\( x = 1 \\) to get \\( {P}_{10}\\left( 1\\right) = {2.718281801} \\) and compare with \\( e = {e}^{1} = {2.718281828}\\ldots \\) . We see \\( {P}_{10} \\) yields the first seven decimal places for \\( e \\) .",
        "id": "college_math_171878"
    },
    {
        "informal_statement": "Proposition 2.34. If \\( \\alpha \\) is not an endpoint of \\( {D}_{\\left( q,\\tau \\right) } \\), then \\( {\\sigma }^{n}\\left( \\alpha \\right) \\in {D}_{\\tau }^{\\prime \\prime } \\) for some positive integer \\( n \\) .",
        "informal_proof": "Proof. Assume \\( \\alpha \\notin {D}_{\\tau }^{\\prime \\prime } \\), since otherwise we are done. Since \\( \\alpha \\) is not an endpoint of \\( {D}_{\\left( q,\\tau \\right) } \\), there is a point \\( \\beta \\in {D}_{\\left( q,\\tau \\right) } \\) distinct from \\( \\alpha \\) such that \\( \\alpha \\in \\left\\lbrack {\\beta ,\\tau }\\right\\rbrack \\) . Then the unique itinerary property implies that there is a point \\( \\delta \\in \\left\\lbrack {\\alpha ,\\beta }\\right\\rbrack \\) such that \\( {\\sigma }^{n}\\left( \\delta \\right) = \\tau \\) for some \\( n \\) (pick \\( n \\) so that \\( {\\alpha }_{n} \\) and \\( {\\beta }_{n} \\) are different). Then \\( {\\sigma }^{n}\\left( \\alpha \\right) \\in {\\sigma }^{n}\\left\\lbrack {\\tau ,\\delta }\\right\\rbrack \\subseteq {D}_{\\tau }^{n} \\), by the previous lemma. \\( \\square \\)",
        "id": "college_math_229622"
    },
    {
        "informal_statement": "Theorem 1.4.9. The nonwandering set $\\\\Omega \\\\left( T\\\\right)$ of a system $T : X \\\\rightarrow X$ enjoys the following properties:\\n\\n(a) $\\\\Omega \\\\left( T\\\\right)$ is closed.\\n\\n(b) $\\\\varnothing \\\\neq \\\\mathop{\\\\bigcup }\\\\limits_{{x \\\\in X}}\\\\omega \\\\left( x\\\\right) \\\\subseteq \\\\Omega \\\\left( T\\\\right)$ .\\n\\n(c) $\\\\operatorname{Per}\\\\left( T\\\\right) \\\\subseteq \\\\Omega \\\\left( T\\\\right)$ .\\n\\n(d) $\\\\Omega \\\\left( T\\\\right)$ is forward $T$ -invariant.\\n\\n(e) If $T$ is a homeomorphism, then $\\\\Omega \\\\left( T\\\\right) = \\\\Omega \\\\left( {T}^{-1}\\\\right)$ and is completely $T$ -invariant.",
        "informal_proof": "(a) The nonwandering set $\\\\Omega \\\\left( T\\\\right)$ is closed since its complement, the set of wandering points $X \\\\smallsetminus \\\\Omega \\\\left( T\\\\right)$, is open. Indeed, if a point $x$ is wandering, then there exists an open neighborhood $U$ of $x$ such that the preimages of $U$ are mutually disjoint. Therefore, all points of $U$ are wandering as well. So $X \\\\smallsetminus \\\\Omega \\\\left( T\\\\right)$ is open.\\n\\n(b) Let $x \\\\in X$ and $y \\\\in \\\\omega \\\\left( x\\\\right)$ . Then there is a strictly increasing sequence ${\\\\left( {n}_{k}\\\\right) }_{k = 1}^{\\\\infty }$ of nonnegative integers such that $\\\\mathop{\\\\lim }\\\\limits_{{k \\\\rightarrow \\\\infty }}{T}^{{n}_{k}}\\\\left( x\\\\right) = y$ . Thus, given any open neighborhood $U$ of $y$, there are numbers ${n}_{k} < {n}_{l}$ such that ${T}^{{n}_{k}}\\\\left( x\\\\right) \\\\in U$ and ${T}^{{n}_{l}}\\\\left( x\\\\right) \\\\in U$ . Then, letting $n = {n}_{l} - {n}_{k}$ and $z = {T}^{{n}_{k}}\\\\left( x\\\\right)$, we have $z \\\\in U$ and ${T}^{n}\\\\left( z\\\\right) \\\\in U$, that is, ${T}^{-n}\\\\left( U\\\\right) \\\\cap U \\\\neq \\\\varnothing$ . As this is true for every open neighborhood $U$ of $y$, we deduce that $y \\\\in \\\\Omega \\\\left( T\\\\right)$ . Hence, $\\\\mathop{\\\\bigcup }\\\\limits_{{x \\\\in X}}\\\\omega \\\\left( x\\\\right) \\\\subseteq \\\\Omega \\\\left( T\\\\right)$ . In particular, $\\\\Omega \\\\left( T\\\\right) \\\\neq \\\\varnothing$ since $\\\\omega \\\\left( x\\\\right) \\\\neq \\\\varnothing$ for every $x$.\\n\\n(c) Since $\\\\omega \\\\left( x\\\\right) = {\\\\mathcal{O}}_{ + }\\\\left( x\\\\right) \\\\ni x$ for every periodic point $x$, all the periodic points of $T$ belong to $\\\\Omega \\\\left( T\\\\right)$ . More simply, every periodic point is nonwandering as it eventually returns to itself under iteration.\\n\\n(d) Let $x \\\\in \\\\Omega \\\\left( T\\\\right)$ and $U$ an open neighborhood of $T\\\\left( x\\\\right)$ . Then ${T}^{-1}\\\\left( U\\\\right)$ is an open neighborhood of $x$ . As $x \\\\in \\\\Omega \\\\left( T\\\\right)$, there exists $n \\\\in \\\\mathbb{N}$ such that ${T}^{-n}\\\\left( {{T}^{-1}\\\\left( U\\\\right) }\\\\right) \\\\cap {T}^{-1}\\\\left( U\\\\right) \\\\neq \\\\varnothing$ . That is, ${T}^{-1}\\\\left( {{T}^{-n}\\\\left( U\\\\right) \\\\cap U}\\\\right) \\\\neq \\\\varnothing$, which implies that ${T}^{-n}\\\\left( U\\\\right) \\\\cap U \\\\neq \\\\varnothing$ . Since this is true for every open neighborhood $U$ of $T\\\\left( x\\\\right)$, we conclude that $T\\\\left( x\\\\right) \\\\in \\\\Omega \\\\left( T\\\\right)$, and hence $T\\\\left( {\\\\Omega \\\\left( T\\\\right) }\\\\right) \\\\subseteq \\\\Omega \\\\left( T\\\\right)$.\\n\\n(e) Suppose $T$ is a homeomorphism. It is easy to show that $\\\\Omega \\\\left( {T}^{-1}\\\\right) = \\\\Omega \\\\left( T\\\\right)$ . By (d), we then have $T\\\\left( {\\\\Omega \\\\left( T\\\\right) }\\\\right) \\\\subseteq \\\\Omega \\\\left( T\\\\right)$ and ${T}^{-1}\\\\left( {\\\\Omega \\\\left( T\\\\right) }\\\\right) \\\\subseteq \\\\Omega \\\\left( T\\\\right)$ . This implies ${T}^{-1}\\\\left( {\\\\Omega \\\\left( T\\\\right) }\\\\right) =$ $\\\\Omega \\\\left( T\\\\right)$.",
        "id": "college_math_82090"
    },
    {
        "informal_statement": "Lemma 3.2. Assume (17). Let $C > 0$ be given in (33), and $\\parallel g{\\parallel }_{\\infty } =$ $\\parallel \\left| {g\\left( \\theta \\right) }\\right| {\\parallel }_{{L}^{\\infty }\\left( {\\mathbf{T},{d\\theta }}\\right) },$ where $g\\left( \\theta \\right)$ is given by (16). We have for any $\\lambda > 0$ such that ${2\\lambda }{C}^{2}\\parallel g{\\parallel }_{\\infty }^{2} < 1$ ,\\n\\n$$ \\frac{1}{n}\\log \\mathbb{E}\\left\\{ {\\exp \\left\\lbrack {\\lambda \\mathop{\\sum }\\limits_{{k = 1}}^{n}{\\left| {X}_{k}\\right| }^{2}}\\right\\rbrack }\\right\\} \\leq - \\frac{d}{2}\\log \\left( {1 - {2\\lambda }{C}^{2}\\parallel g{\\parallel }_{\\infty }^{2}}\\right) . \\tag{34} $$",
        "informal_proof": "Proof. The main difficulty resides in the nonlinear property of ${x}^{2}$ . The trick consists to reduce it to an estimation of linear type in the following way:\\n\\n$$ \\mathbb{E}\\left\\{ {\\exp \\left\\lbrack {\\frac{1}{2}{t}^{2}\\mathop{\\sum }\\limits_{{k = 1}}^{n}{\\left| {X}_{k}\\right| }^{2}}\\right\\rbrack }\\right\\} $$\\n\\n$$ = {\\int }_{{\\left( {\\mathbb{R}}^{d}\\right) }^{n}}\\mathbb{E}\\left\\{ {\\exp \\left\\lbrack {t\\mathop{\\sum }\\limits_{{k = 1}}^{n}\\left\\langle {{X}_{k},{y}_{k}}\\right\\rangle }\\right\\rbrack }\\right\\} \\gamma \\left( {d{y}_{1}}\\right) \\cdots \\gamma \\left( {d{y}_{n}}\\right) \\tag{35} $$\\n\\nwhere $\\gamma$ is the standard Gaussian law $N\\left( {0, I}\\right)$ on ${\\mathbb{R}}^{p}$ .\\n\\nSince\\n\\n$$ \\mathop{\\sum }\\limits_{{k = 1}}^{n}\\left\\langle {{X}_{k},{y}_{k}}\\right\\rangle = \\mathop{\\sum }\\limits_{{j \\in \\mathbb{Z}}}\\left\\langle {{\\xi }_{j},\\mathop{\\sum }\\limits_{{k = 1}}^{n}{a}_{j - k}^{ * }{y}_{k}}\\right\\rangle $$\\n\\nwe get by Lemma 3.1 and the i.i.d. property of $\\left( {\\xi }_{j}\\right)$ ,\\n\\n$$ \\mathbb{E}\\left\\{ {\\exp \\left\\lbrack {t\\mathop{\\sum }\\limits_{{k = 1}}^{n}\\left\\langle {{X}_{k},{y}_{k}}\\right\\rangle }\\right\\rbrack }\\right\\} \\leq \\exp \\left\\lbrack {\\frac{{C}^{2}{t}^{2}}{2}\\mathop{\\sum }\\limits_{{j \\in \\mathbb{Z}}}{\\left| \\mathop{\\sum }\\limits_{{k = 1}}^{n}{a}_{j - k}^{ * }{y}_{k}\\right| }^{2}}\\right\\rbrack . \\tag{36} $$\\n\\nNow observe that\\n\\n$$ \\mathop{\\sum }\\limits_{{j \\in \\mathbb{Z}}}{\\left| \\mathop{\\sum }\\limits_{{k = 1}}^{n}{a}_{j - k}^{ * }{y}_{k}\\right| }^{2} = \\frac{1}{2\\pi }{\\int }_{-\\pi }^{\\pi }{\\left| \\mathop{\\sum }\\limits_{{k = 1}}^{n}{g}^{ * }\\left( \\theta \\right) {y}_{k}{e}^{ik\\theta }\\right| }^{2}{d\\theta } $$\\n\\n$$ \\leq \\frac{1}{2\\pi }\\parallel g{\\parallel }_{\\infty }^{2}{\\int }_{-\\pi }^{\\pi }{\\left| \\mathop{\\sum }\\limits_{{k = 1}}^{n}{e}^{ik\\theta }{y}_{k}\\right| }^{2}{d\\theta } = \\parallel g{\\parallel }_{\\infty }^{2}\\mathop{\\sum }\\limits_{{k = 1}}^{n}{\\left| {y}_{k}\\right| }^{2}. $$\\n\\nSubstituting it into (36), we obtain by (35) that if ${C}^{2}{t}^{2}\\parallel g{\\parallel }_{\\infty }^{2} < 1$ ,\\n\\n$$ \\mathbb{E}\\left\\{ {\\exp \\left\\lbrack {\\frac{1}{2}{t}^{2}\\mathop{\\sum }\\limits_{{k = 1}}^{n}{\\left| {X}_{k}\\right| }^{2}}\\right\\rbrack }\\right\\} $$\\n\\n$$ \\leq {\\int }_{{\\left( {\\mathbb{R}}^{d}\\right) }^{n}}\\left\\{ {\\exp \\left\\lbrack {\\frac{1}{2}{C}^{2}{t}^{2}\\parallel g{\\parallel }_{\\infty }^{2}\\mathop{\\sum }\\limits_{{k = 1}}^{n}{\\left| {y}_{k}\\right| }^{2}}\\right\\rbrack }\\right\\} \\gamma \\left( {d{y}_{1}}\\right) \\cdots \\gamma \\left( {d{y}_{n}}\\right) $$\\n\\n$$ = {\\left( 1 - {C}^{2}{t}^{2}\\parallel g{\\parallel }_{\\infty }^{2}\\right) }^{-{nd}/2} $$\\n\\nwhere (34) follows with $\\lambda = {t}^{2}/2$ .",
        "id": "college_math_100654"
    },
    {
        "informal_statement": "Proposition 5.3 Two objects are related to each other by ids if and only if they are equal to each other, and elements of set S.\\n\\n\\\\[ \\n\\\\forall x, y : X \\\\cdot \\\\left( {x, y}\\\\right) \\\\in i{d}_{S} \\\\Leftrightarrow x = y \\\\land x \\\\in S \\n\\\\]",
        "informal_proof": "Proof The proof follows in a rather straightforward manner from the definitions:\\n\\n\\\\( \\\\left( {x, y}\\\\right) \\\\in i{d}_{S} \\n\\n\\\\( \\\\Leftrightarrow \\\\left\\\\{ \\\\right. \\\\) definition of \\\\( \\\\left. {i{d}_{S}}\\\\right\\\\} \\n\\n\\\\( \\\\left( {x, y}\\\\right) \\\\in \\\\{ z : X \\\\mid z \\\\in S \\\\bullet \\\\left( {z, z}\\\\right) \\\\} \\n\\n\\\\( \\\\Leftrightarrow \\\\{ \\\\) membership of set comprehension \\\\} \\n\\n\\\\( \\\\exists z : X \\\\cdot z \\\\in S \\\\land \\\\left( {z, z}\\\\right) = \\\\left( {x, y}\\\\right) \\n\\n\\\\( \\\\Leftrightarrow \\\\{ \\\\) definition of equality of pairs \\\\} \\n\\n\\\\( \\\\exists z : X \\\\cdot z \\\\in S \\\\land z = x \\\\land z = y \\n\\n\\\\( \\\\Leftrightarrow \\\\{ \\\\) using the one-point rule on \\\\( z = x\\\\} \\n\\n\\\\( x \\\\in S \\\\land x = y \\n\\n\\\\( \\\\Leftrightarrow \\\\{ \\\\) commutativity of conjunction \\\\} \\n\\n\\\\( x = y \\\\land x \\\\in S \\n",
        "id": "college_math_302310"
    },
    {
        "informal_statement": "Lemma 8.3. If \\( E \\) is an elliptic curve over \\( \\mathbf{Q} \\), and \\( E \\) has a cyclic subgroup of order 4 defined over \\( \\mathbf{Q} \\), then \\( E \\) is modular.",
        "informal_proof": "Proof. Suppose \\( C \\) is a rational cyclic subgroup of \\( E \\) of order 4. Let \\( D = \\) \\( C \\cap E\\left\\lbrack 2\\right\\rbrack \\) . Then \\( D \\) is a subgroup of \\( E \\) of order 2, and \\( D \\) is defined over \\( \\mathbf{Q} \\) . Let \\( {E}^{\\prime } = E/D \\), an elliptic curve over \\( \\mathbf{Q} \\) . The quotient map \\( \\varphi : E \\rightarrow {E}^{\\prime } \\) is an isogeny defined over \\( \\mathbf{Q} \\) . Therefore to show that \\( E \\) is modular, it suffices to show that \\( {E}^{\\prime } \\) is modular. Fix a generator \\( x \\) of \\( C \\) and fix \\( y \\in E\\left\\lbrack 2\\right\\rbrack - D \\) . Then \\( \\varphi \\left( x\\right) \\) generates \\( C/D \\), which is a rational subgroup of \\( {E}^{\\prime } \\) of order 2. Therefore, \\( \\varphi \\left( x\\right) \\) is defined over \\( \\mathbf{Q} \\) . Similarly, \\( \\varphi \\left( y\\right) \\) generates \\( E\\left\\lbrack 2\\right\\rbrack /D \\) , a rational subgroup of \\( {E}^{\\prime } \\) of order 2, so \\( \\varphi \\left( y\\right) \\) is defined over \\( \\mathbf{Q} \\) . Since \\( x - y \\notin C \\), we have \\( x - y \\notin D \\), so \\( \\varphi \\left( x\\right) \\neq \\varphi \\left( y\\right) \\) . Therefore, \\( {E}^{\\prime } \\) has all its points of order 2 defined over \\( \\mathbf{Q} \\) . By Lemma 8.2, \\( {E}^{\\prime } \\) is modular.",
        "id": "college_math_178605"
    },
    {
        "informal_statement": "Proposition 11. Let \\( L/K \\) be a finite Galois extension in characteristic \\( p > 0 \\) with Galois group \\( G \\) . On the ring of Witt vectors \\( {W}_{r}\\left( L\\right) \\) over \\( L \\) of given length \\( r \\), consider the componentwise action of \\( G \\) . Then\\n\\n\\[ \\n{H}^{1}\\left( {G,{W}_{r}\\left( L\\right) }\\right) = 0 \\n\\]\\n\\ni.e., every 1-cocycle is already a 1-coboundary.",
        "informal_proof": "Proof. We proceed similarly as in the proof of \\( {4.8}/2 \\), but in addition, must make use of the trace map\\n\\n\\[ \\n{\\operatorname{tr}}_{L/K} : {W}_{r}\\left( L\\right) \\rightarrow {W}_{r}\\left( K\\right) ,\\;a \\mapsto \\mathop{\\sum }\\limits_{{\\sigma \\in G}}\\sigma \\left( a\\right) .\\n\\]\\n\\nSince every \\( \\sigma \\in G \\) defines a \\( {W}_{r}\\left( K\\right) \\) -automorphism of \\( {W}_{r}\\left( L\\right) \\), we see immediately that the trace map is \\( {W}_{r}\\left( K\\right) \\) -linear. In addition, \\( {\\operatorname{tr}}_{L/K} \\) is compatible with the projection \\( {W}_{r}\\left( L\\right) \\rightarrow {W}_{1}\\left( L\\right) = L \\), where the trace map on \\( {W}_{1}\\left( L\\right) \\) coincides by \\( {4.7}/4 \\) with the usual trace map \\( {\\operatorname{tr}}_{L/K} : L \\rightarrow K \\) . Proceeding by induction on \\( r \\) , we want to show that \\( {\\operatorname{tr}}_{L/K} : {W}_{r}\\left( L\\right) \\rightarrow {W}_{r}\\left( K\\right) \\) is surjective.\\n\\nIf \\( r = 1 \\), we have to deal with the usual trace map, as defined for finite field extensions. The assertion then follows from \\( {4.7}/7 \\) . Otherwise, we can use the fact that the trace map on \\( {W}_{r}\\left( L\\right) \\) is compatible with the Verschiebung operator, and hence for \\( r > 1 \\), leads to a commutative diagram of the following type:\\n\\n\\[ \\n0 \\rightarrow {W}_{r - 1}\\left( K\\right) \\overset{{V}_{r - 1}^{1}}{ \\rightarrow }{W}_{r}\\left( K\\right) \\rightarrow {W}_{1}\\left( K\\right) \\rightarrow 0 \\n\\]\\n\\nAs we know, the trace map is surjective on \\( {W}_{1}\\left( L\\right) \\), and by the induction hypothesis, also on \\( {W}_{r - 1}\\left( L\\right) \\) . Therefore, it will be surjective on \\( {W}_{r}\\left( L\\right) \\) as well. In particular, there exists an element \\( a \\in {W}_{r}\\left( L\\right) \\) such that \\( {\\operatorname{tr}}_{L/K}\\left( a\\right) = 1 \\) .\\n\\nNow let \\( f : G \\rightarrow {W}_{r}\\left( L\\right) \\) be a 1-cocycle. Considering the Poincaré series\\n\\n\\[ \\nb = \\mathop{\\sum }\\limits_{{{\\sigma }^{\\prime } \\in G}}f\\left( {\\sigma }^{\\prime }\\right) \\cdot {\\sigma }^{\\prime }\\left( a\\right) \\n\\]\\n\\nwe obtain for arbitrary \\( \\sigma \\in G \\) the equation\\n\\n\\[ \\n\\sigma \\left( b\\right) = \\mathop{\\sum }\\limits_{{{\\sigma }^{\\prime } \\in G}}\\sigma \\left( {f\\left( {\\sigma }^{\\prime }\\right) }\\right) \\cdot \\left( {\\sigma \\circ {\\sigma }^{\\prime }}\\right) \\left( a\\right) \\n\\]\\n\\n\\[ \\n= \\mathop{\\sum }\\limits_{{{\\sigma }^{\\prime } \\in G}}\\left( {f\\left( {\\sigma \\circ {\\sigma }^{\\prime }}\\right) - f\\left( \\sigma \\right) }\\right) \\cdot \\left( {\\sigma \\circ {\\sigma }^{\\prime }}\\right) \\left( a\\right) \\n\\]\\n\\n\\[ \\n= \\mathop{\\sum }\\limits_{{{\\sigma }^{\\prime } \\in G}}f\\left( {\\sigma \\circ {\\sigma }^{\\prime }}\\right) \\cdot \\left( {\\sigma \\circ {\\sigma }^{\\prime }}\\right) \\left( a\\right) - \\mathop{\\sum }\\limits_{{{\\sigma }^{\\prime } \\in G}}f\\left( \\sigma \\right) \\cdot \\left( {\\sigma \\circ {\\sigma }^{\\prime }}\\right) \\left( a\\right) \\n\\]\\n\\n\\[ \\n= b - f\\left( \\sigma \\right) \\cdot {\\operatorname{tr}}_{L/K}\\left( a\\right) = b - f\\left( \\sigma \\right) ,\\n\\]\\n\\ni.e., \\( f \\) is a 1-coboundary.\\n\\nThis concludes the proof of Theorem 8.",
        "id": "college_math_273824"
    },
    {
        "informal_statement": "Example 1.4.15. As we saw in the previous example, if you're given two different points \\( \\left( {{x}_{0},{y}_{0},{z}_{0}}\\right) \\) and \\( \\left( {{x}_{1},{y}_{1},{z}_{1}}\\right) \\), then, the displacement vector \\( \\mathbf{v} = \\left( {{x}_{1},{y}_{1},{z}_{1}}\\right) - \\) \\( \\left( {{x}_{0},{y}_{0},{z}_{0}}\\right) \\) is parallel to the line through \\( \\left( {{x}_{0},{y}_{0},{z}_{0}}\\right) \\) and \\( \\left( {{x}_{1},{y}_{1},{z}_{1}}\\right) \\) .",
        "informal_proof": "Thus,\\n\\n\\[ \\left( {x, y, z}\\right) = \\left( {{x}_{0},{y}_{0},{z}_{0}}\\right) + t\\mathbf{v} = \\left( {{x}_{0},{y}_{0},{z}_{0}}\\right) + t\\left\\lbrack {\\left( {{x}_{1},{y}_{1},{z}_{1}}\\right) - \\left( {{x}_{0},{y}_{0},{z}_{0}}\\right) }\\right\\rbrack \\]\\n\\nis a parameterization of the line through \\( \\left( {{x}_{0},{y}_{0},{z}_{0}}\\right) \\) and \\( \\left( {{x}_{1},{y}_{1},{z}_{1}}\\right) \\) .",
        "id": "college_math_241607"
    },
    {
        "informal_statement": "Lemma 3.3 \\( \\left( {t > 0, v \\in \\mathbb{R}}\\right) \\) .\\n\\n\\[ \\n{\\theta }_{v} \\circ \\left( {\\mathop{\\sum }\\limits_{{x \\in E}}{P}_{x, x}^{t}{\\lambda }_{x}}\\right) = \\mathop{\\sum }\\limits_{{x \\in E}}{P}_{x, x}^{t}{\\lambda }_{x}. \\tag{3.24} \\n\\]",
        "informal_proof": "Proof. The measure \\( \\mathop{\\sum }\\limits_{{x \\in E}}{P}_{x, x}^{t}{\\lambda }_{x} \\) is concentrated on \\( {L}_{r, t} \\), and for \\( \\gamma \\in {L}_{r, t} \\) ,\\n\\n\\( {\\theta }_{\\ell t}\\left( \\gamma \\right) = \\gamma \\), for any \\( \\ell \\in \\mathbb{Z} \\), due to the fact that \\( s \\in \\mathbb{R} \\rightarrow {X}_{s}\\left( \\gamma \\right) \\in E \\) has period \\( t \\) . We can thus assume that \\( 0 < v < t \\) . The claim (3.24) will then follow once we show that for any \\( 0 < {t}_{1} < \\cdots < {t}_{k} = t - v < {t}_{k + 1} < \\cdots < {t}_{n} = t \\), one has for \\( {x}_{1},\\ldots ,{x}_{n} \\in E, \\)\\n\\n\\[ \\n\\mathop{\\sum }\\limits_{{x \\in E}}{P}_{x, x}^{t}\\left\\lbrack {{X}_{{t}_{1}} = {x}_{1},\\ldots ,{X}_{{t}_{n}} = {x}_{n}}\\right\\rbrack {\\lambda }_{x} \\n\\]\\n\\n\\[ \\n= \\mathop{\\sum }\\limits_{{x \\in E}}{P}_{x, x}^{t}\\left\\lbrack {{X}_{v + {t}_{1}} = {x}_{1},\\ldots ,{X}_{v + {t}_{k}} = {x}_{k},{X}_{v + {t}_{k + 1} - t} = {x}_{k + 1},\\ldots ,{X}_{v} = {x}_{n}\\rbrack {\\lambda }_{x}.}\\right. \\n\\]\\n\\n\\[ \\n{t}_{k} = t - v \\tag{3.25} \\n\\]\\n\\nThe expression on the left-hand side of (3.25), as in (3.14), is equal to\\n\\n\\[ \\n{r}_{{t}_{2} - {t}_{1}}\\left( {{x}_{1},{x}_{2}}\\right) {\\lambda }_{{x}_{2}}\\ldots {r}_{{t}_{n} - {t}_{n - 1}}\\left( {{x}_{n - 1},{x}_{n}}\\right) {\\lambda }_{{x}_{n}}{r}_{{t}_{1}}\\left( {{x}_{n},{x}_{1}}\\right) {\\lambda }_{{x}_{1}}\\;\\left( {\\text{ recall that }{t}_{n} = t}\\right) .\\n\\]\\n\\nNote that \\( 0 < v + {t}_{k + 1} - t < \\cdots < v = v + {t}_{n} - t < v + {t}_{1} < \\cdots < v + {t}_{k} = t \\), and therefore using once again the calculation in (3.14), the expression on the right-hand side of (3.25) equals\\n\\n\\[ \\n{r}_{{t}_{k + 2} - {t}_{k + 1}}\\left( {{x}_{k + 1},{x}_{k + 2}}\\right) {\\lambda }_{{x}_{k + 2}}\\ldots {r}_{{t}_{n} - {t}_{n - 1}}\\left( {{x}_{n - 1},{x}_{n}}\\right) {\\lambda }_{{x}_{n}} \\n\\]\\n\\n\\[ \\n{r}_{{t}_{1}}\\left( {{x}_{n},{x}_{1}}\\right) {\\lambda }_{{x}_{1}}\\ldots {r}_{{t}_{k + 1} - {t}_{k}}\\left( {{x}_{k},{x}_{k + 1}}\\right) {\\lambda }_{{x}_{k + 1}}.\\n\\]\\n\\nThis shows that (3.25) holds and completes the proof of (3.24).",
        "id": "college_math_196127"
    },
    {
        "informal_statement": "Theorem 10.7. The hash function \\( {H}_{\\text{rsa }} \\) is collision resistant under the RSA assumption.\\n\\nIn particular, for every collision-finding adversary \\( \\mathcal{A} \\), there exists an RSA adversary \\( \\mathcal{B} \\), which\\n\\nis an elementary wrapper around \\( \\mathcal{A} \\), such that\\n\\n\\[ \\operatorname{CRadv}\\left\\lbrack {\\mathcal{A},{H}_{\\mathrm{{rsa}}}}\\right\\rbrack \\leq \\operatorname{RSAadv}\\left\\lbrack {\\mathcal{B},\\ell, e}\\right\\rbrack \\]\\n\\n\\( \\left( {10.3}\\right) \\)",
        "informal_proof": "Proof. We construct an adversary \\( {\\mathcal{B}}^{\\prime } \\) that plays the alternative RSA attack game considered in Theorem 10.5. We will show that \\( \\operatorname{CRadv}\\left\\lbrack {\\overline{\\mathcal{A}},{H}_{\\mathrm{{rsa}}}}\\right\\rbrack = \\operatorname{uRSAadv}\\left\\lbrack {{\\mathcal{B}}^{\\prime },\\ell, e}\\right\\rbrack \\), and the theorem will the follow from Theorem 10.5.\\n\\nOur RSA adversary \\( {\\mathcal{B}}^{\\prime } \\) runs as follows. It receives \\( \\left( {n, y}\\right) \\) from its challenger, where \\( n \\) is an RSA modulus and \\( y \\) is a random element of \\( {\\mathbb{Z}}_{n}^{ * } \\) . The values \\( e, n, y \\) define the hash function \\( {H}_{\\mathrm{{rsa}}} \\), and adversary \\( {\\mathcal{B}}^{\\prime } \\) runs adversary \\( \\mathcal{A} \\) with this hash function. Suppose that \\( \\mathcal{A} \\) finds a collision. This is a pair of inputs \\( \\left( {a, b}\\right) \\neq \\left( {{a}^{\\prime },{b}^{\\prime }}\\right) \\) such that\\n\\n\\[ {a}^{e}{y}^{b} = {\\left( {a}^{\\prime }\\right) }^{e}{y}^{{b}^{\\prime }} \\]\\n\\nwhich we may rewrite as\\n\\n\\[ {\\left( a/{a}^{\\prime }\\right) }^{e} = {y}^{{b}^{\\prime } - b}. \\]\\n\\nUsing this collision, \\( {\\mathcal{B}}^{\\prime } \\) will compute an \\( e \\) th root of \\( y \\).\\n\\nObserve that \\( {b}^{\\prime } - b \\neq 0 \\), since otherwise we would have \\( \\left( {a/{a}^{\\prime }}\\right) = 1 \\) and hence \\( a = {a}^{\\prime } \\) . Also observe that since \\( \\left| {b - {b}^{\\prime }}\\right| < e \\) and \\( e \\) is prime, we must have \\( \\gcd \\left( {e, b - {b}^{\\prime }}\\right) = 1 \\) . So now we simply apply Theorem 10.6 with \\( n, e \\), and \\( y \\) as given, and \\( w \\mathrel{\\text{:=}} a/{a}^{\\prime } \\) and \\( f \\mathrel{\\text{:=}} {b}^{\\prime } - b \\) . \\( \\square \\)",
        "id": "college_math_304561"
    },
    {
        "informal_statement": "Corollary 11.25. Let \\( R : X \\rightarrow \\mathrm{L}\\left( V\\right) \\) be a completely reducible representation which is isomorphic to a sum of irreducible representations \\( {R}_{1},\\ldots ,{R}_{m} \\) . Then every subrepresentation and every quotient representation of \\( R \\) are isomorphic to a sum of some of the representations \\( {R}_{i} \\) .",
        "informal_proof": "Proof. Let\\n\\n\\[ V = {V}_{1} \\oplus \\cdots \\oplus {V}_{m} \\]\\n\\nbe a decomposition into a direct sum of invariant subspaces such that \\( {R}_{{V}_{i}} \\simeq \\) \\( {R}_{i} \\) and let \\( U \\subset V \\) be an invariant subspace. By Theorem 11.20, there exists a subset \\( I \\subset \\{ 1,\\ldots, m\\} \\) such that \\( V = U \\oplus {V}_{I} \\) . Clearly,\\n\\n\\[ {R}_{V/U} \\simeq {R}_{{V}_{I}} \\simeq \\mathop{\\sum }\\limits_{{i \\in I}}{R}_{i} \\]\\n\\nNow, let \\( J = \\{ 1,\\ldots, m\\} \\smallsetminus I \\) . Then \\( V = {V}_{J} \\oplus {V}_{I} \\), hence\\n\\n\\[ {R}_{U} \\simeq {R}_{V/{V}_{I}} \\simeq {R}_{{V}_{J}} \\simeq \\mathop{\\sum }\\limits_{{j \\in J}}{R}_{j} \\]",
        "id": "college_math_204021"
    },
    {
        "informal_statement": "Proposition 8.10 (Preservation of the total amount of homology) The operations \\( Y \\mapsto {Y}^{\\prime } \\) and \\( Y \\mapsto \\widetilde{Y} \\) preserve the total amount of homology.",
        "informal_proof": "For \\( {Y}^{\\prime } \\) this was proved in Section 3.1. But we will give a different formulation that is better for our purposes:\\n\\nFrom the retraction (recall Section 3.1) \\( Y \\rightarrow {Y}_{ + } \\) it follows that we have an isomorphism:\\n\\n\\[ \\n{H}_{i}\\left( Y\\right) \\cong {H}_{i}\\left( {Y}_{ + }\\right) \\oplus {H}_{i}\\left( {Y,{Y}_{ + }}\\right) \\n\\]\\n\\nSince\\n\\n\\[ \\n{H}_{i}\\left( {Y,{Y}_{ + }}\\right) \\cong {H}_{i}\\left( {{Y}_{ - },{Y}_{0}}\\right) \\cong {H}^{n - i}\\left( {Y}_{ - }\\right) \\n\\]\\n\\n(by the excision isomorphism and Lefschetz Duality, where \\( n \\) is the dimension of \\( Y \\) ), we have\\n\\n\\[ \\n{H}_{i}\\left( Y\\right) \\cong {H}_{i}\\left( {Y}_{ + }\\right) \\oplus {H}^{n - i}\\left( {Y}_{ + }\\right) \\n\\]\\n\\nThe same applies to \\( {Y}^{\\prime } \\) :\\n\\n\\[ \\n{H}_{i}\\left( {Y}^{\\prime }\\right) \\cong {H}_{i}\\left( {Y}_{ + }^{\\prime }\\right) \\oplus {H}^{n + 1 - i}\\left( {Y}_{ + }^{\\prime }\\right) .\\n\\]\\n\\nSince we know that \\( {Y}_{ + } \\) and \\( {Y}_{ + }^{\\prime } \\) are homotopy equivalent, the total amount of homology of \\( Y \\) and of \\( {Y}^{\\prime } \\) is the same.\\n\\nFor the operation \\( \\widetilde{Y} \\) observe that \\( Y \\) arises from a primitive configuration (i.e., one with \\( s = 0 \\) and \\( {n}_{i} = 1 \\) for all \\( i \\) ) by applying the operations \\( {Y}^{\\prime } \\) and \\( \\widetilde{Y} \\) a number of times each. Since one can assume all the \\( \\widetilde{Y} \\) operations are applied first (because they commute), it follows inductively from Lemma 8.8 and the preservation of the total amount of homology by the operation \\( {Y}^{\\prime } \\) that \\( Y \\) has the same amount of homology as that primitive configuration. Now, \\( Y \\) and \\( \\widetilde{Y} \\) arise from the same primitive configuration, so the proposition is proved.",
        "id": "college_math_182449"
    },
    {
        "informal_statement": "Proposition 8.1.6 If \\( R \\) is left hereditary, then \\( {\\operatorname{Ext}}_{R}^{n}\\left( {A, B}\\right) = 0 \\) for all left \\( R \\) -modules \\( A, B \\) and all \\( n \\geq 2 \\) .",
        "informal_proof": "Proof. Let \\( A, B \\) be left \\( R \\) -modules. There exists a projective left \\( R \\) - module \\( {P}_{0} \\) and an epimorphism \\( \\epsilon : {P}_{0} \\rightarrow A \\) . Let \\( {K}_{0} = \\operatorname{Ker}\\epsilon \\) . Then the sequence\\n\\n\\[ 0 \\rightarrow {K}_{0}\\overset{i}{ \\rightarrow }{P}_{0}\\overset{\\epsilon }{ \\rightarrow }A \\rightarrow 0 \\]\\n\\nwhere \\( \\mathrm{i} \\) is the inclusion map, is exact. Since \\( {K}_{0} \\) is projective, we get a projective resolution of \\( A \\) by taking \\( {P}_{1} = {K}_{0} \\) and \\( {P}_{n} = 0 \\) for \\( n \\geq 2 \\) . Then, for \\( n \\geq 2 \\) ,\\n\\n\\[ {\\operatorname{Ext}}_{R}^{n}\\left( {A, B}\\right) \\cong H\\left( {{\\operatorname{Hom}}_{R}\\left( {{P}_{n - 1}, B}\\right) - {\\operatorname{Hom}}_{R}\\left( {{P}_{n}, B}\\right) - {\\operatorname{Hom}}_{R}\\left( {{P}_{n + 1}, B}\\right) }\\right) \\]\\n\\n\\[ = 0.\\square \\]",
        "id": "college_math_183560"
    },
    {
        "informal_statement": "Example 3. Suppose that $M$ corresponds to a finite-dimensional representation $\\varphi$ of a group $G$ over $\\mathbb{C}$, again with a Hermitian scalar product defined such that all the operators ${\\varphi }_{g}$ are unitary for $g \\in G$, that is\\n\\n$$ \\n\\\\left( {{\\\\varphi }_{g}\\\\left( x\\\\right) ,{\\\\varphi }_{g}\\\\left( y\\\\right) }\\\\right) = \\\\left( {x, y}\\\\right) . \\\\tag{1}\\n$$ \\n\\nThen $M$ is semisimple.",
        "informal_proof": "The proof is the same as in Example 2.",
        "id": "college_math_34768"
    },
    {
        "informal_statement": "Theorem 6.2.16. If \\( A \\) is a skew PBW extension over a \\( \\left( {\\sum ,\\Delta }\\right) \\) -compatible and \\( \\sum \\) -skew Armendariz ring \\( R \\), then\\n\\n\\[ \\n{\\operatorname{Nil}}_{ * }\\left( A\\right) = {\\operatorname{Nil}}_{ * }\\left( R\\right) A = {\\operatorname{Nil}}_{ * }\\left( {R;\\sum ,\\Delta }\\right) A = {\\operatorname{Nil}}_{ * }\\left( {R;\\sum }\\right) A = {\\operatorname{Nil}}_{ * }\\left( {R;\\Delta }\\right) A.\\n\\]",
        "informal_proof": "Proof. One can see that \\( {\\operatorname{Nil}}_{ * }\\left( {R;\\sum ,\\Delta }\\right) A \\subseteq {\\operatorname{Nil}}_{ * }\\left( A\\right) \\), and together with Theorem 6.2.15, we can assert that \\( {\\operatorname{Nil}}_{ * }\\left( R\\right) A = {\\operatorname{Nil}}_{ * }\\left( {R;\\sum ,\\Delta }\\right) A \\subseteq {\\operatorname{Nil}}_{ * }\\left( A\\right) \\) . On the other hand, let \\( f \\in {\\operatorname{Nil}}_{ * }\\left( A\\right) \\) be given by \\( f = \\mathop{\\sum }\\limits_{{i = 0}}^{m}{a}_{i}{X}_{i} \\) . From Theorem 6.2.13 we obtain that \\( f \\in {\\operatorname{Nil}}_{0}\\left( A\\right) \\), that is \\( \\langle f{\\rangle }^{t} = 0 \\), for some \\( t \\in \\mathbb{N} \\), where \\( \\langle f\\rangle \\) is the two-sided ideal of \\( A \\) generated by \\( f \\) . In this way, Lemma 6.2.9 guarantees that \\( {\\left( R{a}_{i}R\\right) }^{t} = 0 \\), for every value of \\( i \\), which means that \\( {a}_{i} \\in {\\operatorname{Nil}}_{ * }\\left( R\\right) \\) . Hence \\( f \\in {\\operatorname{Nil}}_{ * }\\left( R\\right) A \\), and so \\( {\\operatorname{Nil}}_{ * }\\left( A\\right) = {\\operatorname{Nil}}_{ * }\\left( R\\right) A \\) . The other equalities follow from Theorem 6.2.15.",
        "id": "college_math_299222"
    },
    {
        "informal_statement": "Theorem 3.45. Every \\( {\\mathfrak{M}}_{L} \\) -measurable set \\( E \\) with \\( {\\mu }_{L}\\left( E\\right) > 0 \\) contains a non- \\( {\\mathfrak{M}}_{L} \\) - measurable set.",
        "informal_proof": "Proof. Let \\( M \\) be as in Theorem 3.44. Then by (2) and (3) of Theorem 3.44, we have \\( {\\mu }_{*, L}\\left( {M \\cap E}\\right) = 0 \\) and \\( {\\mu }_{L}^{ * }\\left( {M \\cap E}\\right) = {\\mu }_{L}\\left( E\\right) > 0 \\) and therefore \\( {\\mu }_{*, L}\\left( {M \\cap E}\\right) \\neq {\\mu }_{L}^{ * }\\left( {M \\cap E}\\right) . This shows that \\( M \\cap E \\notin {\\mathfrak{M}}_{L} \\) by (a) of Theorem 3.37.",
        "id": "college_math_162422"
    },
    {
        "informal_statement": "Exercise 64. Fill in the \\( < \\# > \\) with the appropriate variables in the following proof of the proposition.",
        "informal_proof": "Proof: For any integer \\( \\ell \\) we may write \\( \\ell = {ak} + b \\), where \\( b \\in {\\mathbb{Z}}_{ < 1 > } \\) . It follows that\\n\\n\\[ \\n{\\tau }^{\\ell } = {\\tau }^{ < \\underline{2} > \\cdot k + < \\underline{3} > } = \\left( {\\tau }^{ < \\underline{4} > \\cdot k}\\right) {\\tau }^{ < \\underline{5} > } = {\\left( {\\tau }^{k}\\right) }^{ < \\underline{6} > }{\\tau }^{ < \\underline{7} > } = {\\left( \\mathsf{{id}}\\right) }^{ < \\underline{8} > }{\\tau }^{ < \\underline{9} > } = {\\tau }^{ < \\underline{10} > }.\\n\\]\\n\\nTherefore \\( {\\tau }^{\\ell } = \\) id if and only if \\( {\\tau }^{ < \\underline{11} > } = \\) id. However, we know that \\( \\underline{ < {12} > } < k \\), and we also know that \\( \\underline{ < {13} > } \\) is the smallest positive integer such that \\( {\\tau }^{ < \\underline{14} > } = \\) id. Hence it must be the case that \\( b \\equiv < {15} > \\) mod \\( k \\), which is the same thing as saying that \\( \\ell \\equiv \\underline{ < {16} > }\\mathrm{\\;{mod}}\\underline{ < {17} > } \\) . \\( \\diamond \\)",
        "id": "college_math_190335"
    },
    {
        "informal_statement": "Proposition 2.5 (polarization and primary decomposition) Let $I$ be a monomial ideal in a polynomial ring $R = k\\left\\lbrack {{x}_{1},\\ldots ,{x}_{n}}\\right\\rbrack$, and let $\\mathcal{P}\\left( I\\right)$ be the polarization of $I$ in $S = k\\left\\lbrack {x}_{i, j}\\right\\rbrack$ as described in Definition 2.1.\\n\\n1. If $I = \\left( {{x}_{{i}_{1}}{}^{{a}_{1}},\\ldots ,{x}_{{i}_{r}}{}^{{a}_{r}}}\\right)$ where the ${a}_{j}$ are positive integers, then\\n\\n$$ P\\left( I\\right) = \\mathop{\\bigcap }\\limits_{\\substack{{1 \\leq {c}_{j} \\leq {a}_{j}} \\\\ {1 \\leq j \\leq r} }}\\left( {{x}_{{i}_{1},{c}_{1}},\\ldots ,{x}_{{i}_{r},{c}_{r}}}\\right) . $$\\n\\n2. If $I = {\\left( {x}_{{i}_{1}},\\ldots ,{x}_{{i}_{r}}\\right) }^{m}$, where $1 \\leq {i}_{1},\\ldots ,{i}_{r} \\leq n$ and $m$ is a positive integer, then $P\\left( I\\right)$ has the following irredundant irreducible primary decomposition:\\n\\n$$ P\\left( I\\right) = \\mathop{\\bigcap }\\limits_{\\substack{{1 \\leq {c}_{j} \\leq m} \\\\ {\\sum {c}_{j} \\leq m + r - 1} }}\\left( {{x}_{{i}_{1},{c}_{1}},\\ldots ,{x}_{{i}_{r},{c}_{r}}}\\right) . $$\\n\\n3. Suppose that $I = {\\mathfrak{q}}_{1} \\cap \\ldots \\cap {\\mathfrak{q}}_{m}$ is the unique irredundant irreducible primary decomposition of $I$, such that for each $i = 1,\\ldots, m$ ,\\n\\n$$ {\\mathfrak{q}}_{i} = \\left( {{x}_{1}{}^{{a}_{1}^{i}},\\ldots ,{x}_{n}{}^{{a}_{n}^{i}}}\\right) , $$\\n\\nwhere the ${a}_{j}^{i}$ are nonnegative integers, and if ${a}_{j}^{i} = 0$ we assume that ${x}_{j}{}^{{a}_{j}^{i}} =$ 0.\\n\\nThen $P\\left( I\\right)$ has the following irreducible primary decomposition (some primes might be repeated):\\n\\n$$ P\\left( I\\right) = \\mathop{\\bigcap }\\limits_{{1 \\leq i \\leq m}}\\mathop{\\bigcap }\\limits_{\\substack{{1 \\leq {c}_{j} \\leq {a}_{j}^{i}} \\\\ {1 \\leq j \\leq n} }}\\left( {{x}_{1,{c}_{1}},\\ldots ,{x}_{n,{c}_{n}}}\\right) $$\\n\\nwhere when ${a}_{j}^{i} = 0$, we assume that ${c}_{j} = {x}_{j,0} = 0$ .",
        "informal_proof": "## Proof:\\n\\n1. We know that\\n\\n$$ \\mathcal{P}\\left( I\\right) = \\left( {{x}_{{i}_{1},1}\\ldots {x}_{{i}_{1},{a}_{1}},\\ldots ,{x}_{{i}_{1},1}\\ldots {x}_{{i}_{1},{a}_{r}}}\\right) . $$\\n\\nClearly the minimal primes of $P\\left( I\\right)$ are $\\left( {{x}_{{i}_{1},{c}_{1}},\\ldots ,{x}_{{i}_{r},{c}_{r}}}\\right)$ for all ${c}_{j} \\leq {a}_{j}$ . This settles the claim.\\n\\n2. Assume, without loss of generality, that $I = {\\left( {x}_{1},\\ldots ,{x}_{r}\\right) }^{m}$ . So we can write\\n\\n$$ I = \\left( {{x}_{1}{}^{{b}_{1}}\\ldots {x}_{r}{}^{{b}_{r}} \\mid 0 \\leq {b}_{i} \\leq m,{b}_{1} + \\cdots + {b}_{r} = m}\\right) $$\\n\\nso that\\n\\n$$ \\mathcal{P}\\left( I\\right) = \\left( {{x}_{1,1}\\ldots {x}_{1,{b}_{1}}\\ldots {x}_{r,1}\\ldots {x}_{r,{b}_{r}} \\mid 0 \\leq {b}_{i} \\leq m,{b}_{1} + \\cdots + {b}_{r} = m}\\right) . $$\\n\\nWe first show that $P\\left( I\\right)$ is contained in the intersection of the ideals of the form $\\left( {{x}_{1,{c}_{1}},\\ldots ,{x}_{r,{c}_{r}}}\\right)$ described above. It is enough to show this for each generator of $P\\left( I\\right)$ . So we show that\\n\\n$$ \\mathcal{U} = {x}_{1,1}\\ldots {x}_{1,{b}_{1}}\\ldots {x}_{r,1}\\ldots {x}_{r,{b}_{r}} \\in I = \\left( {{x}_{1,{c}_{1}},\\ldots ,{x}_{r,{c}_{r}}}\\right) $$\\n\\nwhere $0 \\leq {b}_{i} \\leq m,{b}_{1} + \\cdots + {b}_{r} = m,1 \\leq {c}_{j} \\leq m$ and ${c}_{1} + \\cdots + {c}_{r} \\leq m +$ $r - 1$ .\\n\\nIf for any $i,{b}_{i} \\geq {c}_{i}$, then it would be clear that $\\mathcal{U} \\in I$ .\\n\\nAssume ${b}_{i} \\leq {c}_{i} - 1$ for $i = 1,\\ldots, r - 1$ . It follows that\\n\\n$$ m - {b}_{r} = {b}_{1} + \\cdots + {b}_{r - 1} $$\\n\\n$$ \\leq {c}_{1} + \\cdots + {c}_{r - 1} - \\left( {r - 1}\\right) $$\\n\\n$$ \\leq m + r - 1 - {c}_{r} - \\left( {r - 1}\\right) $$\\n\\n$$ = m - {c}_{r} $$\\n\\nwhich implies that ${b}_{r} \\geq {c}_{r}$, hence $\\mathcal{U} \\in I$ .\\n\\nSo far we have shown one direction of the inclusion.\\n\\nTo show the opposite direction, take any monomial\\n\\n$$ \\mathcal{U} \\in \\bigcap \\left( {{x}_{1,{c}_{1}},\\ldots ,{x}_{r,{c}_{r}}}\\right) $$\\n\\nwhere $1 \\leq {c}_{j} \\leq m$ and ${c}_{1} + \\cdots + {c}_{r} \\leq m + r - 1$ .\\n\\nNotice that for some $i \\leq r,{x}_{i,1} \\mid \\mathcal{U}$ ; this is becau",
        "id": "college_math_44478"
    },
    {
        "informal_statement": "Corollary 6.5 A linear representation \\( V \\) is irreducible if and only if \\( \\left( {{\\chi }_{V},{\\chi }_{V}}\\right) = 1 \\) .",
        "informal_proof": "Proof It follows from Corollary 6.1 and the orthonormality of irreducible characters that\\n\\n\\[ \\left( {{\\chi }_{V},{\\chi }_{V}}\\right) = \\mathop{\\sum }\\limits_{{\\lambda \\in \\operatorname{Irr}\\left( G\\right) }}{m}_{\\lambda }^{2}\\left( V\\right) \\]\\n\\nwhere all the multiplicities \\( {m}_{\\lambda }\\left( V\\right) \\) are nonnegative integers. This sum equals one if and only if it is exhausted by exactly one summand equal to one.",
        "id": "college_math_367214"
    },
    {
        "informal_statement": "Proposition 10.22. If \\( \\phi \\in {C}_{c}^{1}\\left( \\mathbb{C}\\right) \\), the Cauchy integral \\( C\\left( \\phi \\right) \\) is of class \\( {C}^{1} \\) on \\( \\mathbb{C} \\) and satisfies \\( \\bar{\\partial }C\\left( \\phi \\right) = \\phi \\) . If \\( \\phi \\in {C}_{c}^{k}\\left( \\mathbb{C}\\right) \\), then \\( C\\left( \\phi \\right) \\in {C}^{k}\\left( \\mathbb{C}\\right) \\) .",
        "informal_proof": "Proof. With a change of variable one has\\n\\n\\[ C\\left( \\phi \\right) \\left( z\\right) = \\frac{1}{\\pi }{\\int }_{\\mathbb{C}}\\frac{\\phi \\left( {z - w}\\right) }{w}{dm}\\left( w\\right) .\\n\\]\\n\\nApplying Proposition 7.36, on differentiation of convolutions, it turns out that\\n\\n\\[ \\bar{\\partial }C\\left( \\phi \\right) \\left( z\\right) = \\frac{1}{\\pi }{\\int }_{\\mathbb{C}}\\frac{\\bar{\\partial }\\phi \\left( {z - w}\\right) }{w}{dm}\\left( w\\right) = \\frac{1}{\\pi }{\\int }_{\\mathbb{C}}\\frac{\\bar{\\partial }\\phi \\left( w\\right) }{z - w}{dm}\\left( w\\right) = \\phi \\left( z\\right) ,\\n\\]\\n\\nwhere the last equality is a consequence of Proposition 10.19.",
        "id": "college_math_242312"
    },
    {
        "informal_statement": "Theorem 25.140. (Lemma of Enriques-Severi-Zariski) Let \\( k \\) be a field and let \\( X \\) be a projective \\( k \\) -scheme of finite type. Let \\( \\mathcal{L} \\) be an ample line bundle on \\( X \\). Let \\( \\mathcal{F} \\) be a coherent \\( {\\mathcal{O}}_{X} \\) -module. Assume that \\( {\\operatorname{depth}}_{{\\mathcal{O}}_{X, x}}\\left( {\\mathcal{F}}_{x}\\right) \\geq 2 \\) for all closed points \\( x \\in X \\) (e.g., this holds, if \\( X \\) is normal of dimension \\( \\geq 2 \\) and \\( \\mathcal{F} \\) is locally free). Then there exists \\( {n}_{0} \\) such that \\( {H}^{1}\\left( {X,\\mathcal{F} \\otimes {\\mathcal{L}}^{-n}}\\right) = 0 \\) for all \\( n \\geq {n}_{0} \\).",
        "informal_proof": "Proof. Some power \\( {\\mathcal{L}}^{r} \\) of \\( \\mathcal{L} \\) is very ample and gives rise to a closed embedding \\( \\iota : X \\rightarrow {\\mathbb{P}}_{k}^{N} \\). Then \\( {H}^{1}\\left( {X,\\mathcal{G}}\\right) \\cong {H}^{1}\\left( {{\\mathbb{P}}_{k}^{N},{\\iota }_{ * }\\mathcal{G}}\\right) \\) for every \\( {\\mathcal{O}}_{X} \\) -module \\( \\mathcal{G} \\). Applying this to \\( \\mathcal{F},\\mathcal{F} \\otimes \\mathcal{L},\\ldots \\) , \\( \\mathcal{F} \\otimes {\\mathcal{L}}^{r - 1} \\), we reduce to the case that \\( X = {\\mathbb{P}}_{k}^{N} \\) and \\( \\mathcal{L} = {\\mathcal{O}}_{{\\mathbb{P}}_{k}^{N}}\\left( 1\\right) \\). Note that the depth of \\( {\\left( {\\iota }_{ * }\\mathcal{G}\\right) }_{\\iota \\left( x\\right) } \\) (over \\( {\\mathcal{O}}_{{\\mathbb{P}}_{k}^{N},\\iota \\left( x\\right) } \\) ) equals the depth of \\( {\\mathcal{G}}_{x} \\) (over \\( {\\mathcal{O}}_{X, x} \\) ) for \\( x \\in X \\), and \\( {\\left( {\\iota }_{ * }\\mathcal{G}\\right) }_{y} = 0 \\) has depth \\( \\infty \\) (by convention) for \\( y \\notin \\iota \\left( X\\right) \\).\\n\\nFor the regular scheme \\( X = {\\mathbb{P}}_{k}^{N} \\), we may use Serre duality in the form of Corollary 25.129 (4) and obtain \\( {H}^{1}{\\left( X,\\mathcal{F}\\left( -n\\right) \\right) }^{ \\vee } \\cong {\\operatorname{Ext}}_{{\\mathcal{O}}_{{\\mathbb{P}}_{k}^{N}}}^{N - 1}\\left( {\\mathcal{F}\\left( {-n}\\right) ,{\\omega }_{{\\mathbb{P}}_{k}^{N}}}\\right) \\).\\n\\nNow for \\( n \\) sufficiently large, we have\\n\\n\\[{\\operatorname{Ext}}_{{\\mathcal{O}}_{{\\mathbb{P}}_{k}^{N}}}^{N - 1}\\left( {\\mathcal{F}\\left( {-n}\\right) ,{\\omega }_{{\\mathbb{P}}_{k}^{N}}}\\right) \\cong {\\operatorname{Ext}}_{{\\mathcal{O}}_{{\\mathbb{P}}_{k}^{N}}}^{N - 1}\\left( {\\mathcal{F},{\\omega }_{{\\mathbb{P}}_{k}^{N}}\\left( n\\right) }\\right) \\cong \\Gamma ({\\mathbb{P}}_{k}^{N},{\\mathcal{{Ext}}}_{{\\mathcal{O}}_{{\\mathbb{P}}_{k}^{N}}}^{N - 1}\\left( {\\mathcal{F},{\\omega }_{{\\mathbb{P}}_{k}^{N}}\\left( n\\right) }\\right) ,\\]\\n\\nwhere the first equality follows from 21.148 (3) (and holds for all \\( n \\) ), and the second equality follows from the local-to-global spectral sequence for Ext groups (Corollary 21.108) together with Serre's criterion for ampleness (in the form of Lemma 23.5) which implies the vanishing\\n\\n\\[{H}^{p}\\left( {X,\\mathcal{E}x{t}^{q}\\left( {\\mathcal{F},{\\omega }_{{\\mathbb{P}}_{k}^{N}}\\left( n\\right) }\\right) }\\right) \\cong {H}^{p}\\left( {X,\\mathcal{E}x{t}^{q}\\left( {\\mathcal{F},{\\omega }_{{\\mathbb{P}}_{k}^{N}}}\\right) \\left( n\\right) }\\right) = 0\\]\\n\\nfor all \\( q \\), all \\( p > 0 \\) and all sufficiently large \\( n \\). Note that by Corollary 22.67 (2), the sheaves \\( \\mathcal{E}x{t}^{q}\\left( {\\mathcal{F},{\\omega }_{{\\mathbb{P}}_{k}^{N}}}\\right) \\) are coherent.\\n\\nIt is thus enough to show that \\( \\mathcal{E}x{t}_{{\\mathcal{O}}_{{\\mathbb{P}}_{k}^{N}}}^{N - 1}\\left( {\\mathcal{F},{\\omega }_{{\\mathbb{P}}_{k}^{N}}\\left( n\\right) }\\right) = 0 \\). We claim that in fact \\( {\\mathcal{{Ext}}}_{{\\mathcal{O}}_{{\\mathbb{P}}_{k}^{N}}}^{N - 1}\\left( {\\mathcal{F},{\\omega }_{{\\mathbb{P}}_{k}^{N}}\\left( n\\right) }\\right) = 0 \\) .",
        "id": "college_math_374504"
    },
    {
        "informal_statement": "Theorem 9.6. Let $X$ be a smooth variety, and $D$ be a pseudo-effective $\\mathbb{R}$ -divisor on $X$ . Then the following are equivalent:\\n\\n(9.6.1) $D \\equiv {N}_{\\sigma }\\left( D\\right)$ ;\\n\\n(9.6.2) for any ample divisor $A$, the function ${h}^{0}\\left( {X,{\\mathcal{O}}_{X}\\left( {\\lfloor {tD}\\rfloor + A}\\right) }\\right)$ is bounded;\\n\\n(9.6.3) for any ample divisor $A,\\mathop{\\lim }\\limits_{{t \\rightarrow \\infty }}\\frac{1}{t}{h}^{0}\\left( {X,{\\mathcal{O}}_{X}\\left( {\\lfloor {tD}\\rfloor + A}\\right) }\\right) = 0$ .",
        "informal_proof": "Proof. We follow [Nak04, V.1.12]. We begin by showing that (1) implies (2). Since $P =$ ${N}_{\\sigma }\\left( D\\right) - D \\equiv 0$, one sees that there exists an ample divisor ${A}^{\\prime }$ such that $\\left| {{A}^{\\prime } - A+\\lfloor {tP}\\rfloor }\\right| \\neq$ $\\varnothing$ for any integer $t \\geq 0$ . (It suffices in fact to pick ${A}^{\\prime }$ such that\\n\\n$${A}^{\\prime } - A + \\lfloor {tP}\\rfloor - {K}_{X} - \\left( {n + 1}\\right) H \\equiv {A}^{\\prime } - A - \\{ {tP}\\} - {K}_{X} - \\left( {n + 1}\\right) H$$\\n\\nis ample for some very ample divisor $H$ and any integer $t > 0$ and to apply (6.26).) By (2.3) $\\left\\lfloor {t{N}_{\\sigma }\\left( D\\right) }\\right\\rfloor \\geq \\lfloor {tD}\\rfloor + \\lfloor {tP}\\rfloor$, and therefore\\n\\n$${h}^{0}\\left( {{\\mathcal{O}}_{X}\\left( {\\lfloor {tD}\\rfloor + A}\\right) }\\right) \\leq {h}^{0}\\left( {{\\mathcal{O}}_{X}\\left( {\\left\\lfloor {t{N}_{\\sigma }\\left( D\\right) }\\right\\rfloor + {A}^{\\prime }}\\right) }\\right) .$$\\n\\nFor any divisor $Q$ in the support of ${N}_{\\sigma }\\left( D\\right)$, and any $k \\gg 0$, we have that ${\\sigma }_{Q}\\left( {k{N}_{\\sigma }\\left( D\\right) + {A}^{\\prime }}\\right) > 0$ . By (9.4), it follows that for $t > k$, we have ${\\sigma }_{Q}\\left( {t{N}_{\\sigma }\\left( D\\right) + {A}^{\\prime }}\\right) >$ $\\left( {t - k}\\right) {\\sigma }_{Q}\\left( {{N}_{\\sigma }\\left( D\\right) }\\right)$ . In particular ${h}^{0}\\left( {{\\mathcal{O}}_{X}\\left( {\\left\\lfloor {t{N}_{\\sigma }\\left( D\\right) }\\right\\rfloor + {A}^{\\prime }}\\right) }\\right) = {h}^{0}\\left( {{\\mathcal{O}}_{X}\\left( {\\left\\lfloor {k{N}_{\\sigma }\\left( D\\right) }\\right\\rfloor + {A}^{\\prime }}\\right) }\\right)$ .\\n\\nClearly (2) implies (3).\\n\\nWe will now see that (3) implies (1). Since $D - {N}_{\\sigma }\\left( D\\right)$ is pseudo-effective and\\n\\n$${h}^{0}\\left( {{\\mathcal{O}}_{X}\\left( {\\left\\lfloor {t\\left( {D - {N}_{\\sigma }\\left( D\\right) }\\right) }\\right\\rfloor + A}\\right) }\\right) \\leq {h}^{0}\\left( {{\\mathcal{O}}_{X}\\left( {\\lfloor {tD}\\rfloor + A}\\right) }\\right)$$\\n\\nwe may replace $D$ by $D - {N}_{\\sigma }\\left( D\\right)$ . We may therefore assume that ${N}_{\\sigma }\\left( D\\right) = 0$ and we must show that $D \\equiv 0$ . Let $A$ be a very ample divisor and $C$ be a curve given by the intersection of $n - 1 = \\dim X - 1$ general elements of $\\left| A\\right|$ . We fix an ample divisor ${A}^{\\prime }$ such that $\\frac{1}{2}{A}^{\\prime } - {K}_{X} - \\{ {mD}\\} - \\left( {n - 1}\\right) A$ is ample for all $m \\geq 0$ . Let ${H}_{i} \\in \\left| A\\right|$ be general elements vanishing along $C$, then $\\mathcal{J}\\left( {\\frac{n - 1}{n}\\left( {{H}_{1} + \\cdots + {H}_{n}}\\right) }\\right) = {\\mathcal{I}}_{C}$ . Since ${\\mathbf{B}}_{ - }\\left( D\\right)$ is the union of countably many subvarieties of codimension at least $2,{\\mathbf{B}}_{ - }\\left( D\\right) \\cap C = \\varnothing$ . Therefore, there exists an $\\mathbb{R}$ -divisor $\\Delta { \\sim }_{\\mathbb{Q}}{mD} + \\frac{1}{2}{A}^{\\prime }$ such that $\\mathcal{J}\\left( {\\Delta + \\frac{n - 1}{n}\\left( {{H}_{1} + \\cdots + {H}_{n}}\\right) }\\right) = {\\mathcal{I}}_{C}$ on a neighborhood of $C$ cf. (9.22). Therefore, we have that\\n\\n$${H}^{1}\\left( {\\mathcal{J}\\left( {\\Delta + \\frac{n - 1}{n}\\left( {{H}_{1} + \\cdots + {H}_{n}}\\right) }\\right) \\left( {\\lfloor {mD}\\rfloor + {A}^{\\prime }}\\right) }\\right) = 0$$\\n\\nand so\\n\\n$${H}^{0}\\left( {{\\mathcal{O}}_{X}\\left( {\\lfloor {mD}\\rfloor + {A}^{\\prime }}\\right) }\\right) \\rightarrow {H}^{0}\\left( {{\\mathcal{O}}_{C}\\left( {\\lfloor {mD}\\rfloor + {A}^{\\prime }}\\right) }\\right)$$\\n\\nand hence $D \\equiv 0$.",
        "id": "college_math_127569"
    },
    {
        "informal_statement": "Lemma 2.1.8. Let ${\\\\left( {H}_{n}\\\\left( C\\\\right) \\\\right) }_{n \\\\in \\\\mathbb{Z}}$ be the Sobolev chain associated with the operator $C \\\\subseteq H \\\\oplus H$ . Then\\n\\n$$ \\n{C}_{\\\\left( k\\\\right) } : {H}_{k + 1}\\\\left( C\\\\right) \\\\subseteq {H}_{k}\\\\left( C\\\\right) \\\\rightarrow {H}_{k}\\\\left( C\\\\right) ,\\n$$\\n\\n$$ \\nx \\\\mapsto {C}_{k + 1, k}x\\n$$\\n\\n is a densely defined, closed linear operator with\\n\\n$$ \\n\\\\varrho \\\\left( {C}_{\\\\left( k\\\\right) }\\\\right) = \\\\varrho \\\\left( C\\\\right)\\n$$\\n\\n for every $k \\\\in \\\\mathbb{Z}$ . Moreover,\\n\\n$$ \\n{C}_{\\\\left( k\\\\right) }\\\\text{and}C \\\\equiv {C}_{\\\\left( 0\\\\right) }\\\\text{are unitarily equivalent,}\\;k \\\\in \\\\mathbb{Z}\\\\text{.} \\n$$",
        "informal_proof": "Proof. That ${C}_{\\\\left( k\\\\right) }$ is densely defined follows from the density of ${H}_{k + 1}\\\\left( C\\\\right)$ in ${H}_{k}\\\\left( C\\\\right)$ . The closedness of ${C}_{\\\\left( k\\\\right) }$ follows from the unitarity of ${C}_{k + 1, k}$, since\\n\\n$$ \\n{C}_{k + 1, k}x = {C}_{\\\\left( k\\\\right) }x\\\\;\\\\text{ for all }x \\\\in {H}_{k + 1}\\\\left( C\\\\right) .\\n$$\\n\\nIf we show that ${C}_{\\\\left( k\\\\right) }$ is unitarily equivalent to ${C}_{\\\\left( 0\\\\right) } = C$, then equality of resolvent sets, indeed of all spectral parts, follows.\\n\\nThen we have\\n\\n$$ \\n{C}_{k, j} = {C}_{j, j - 1}^{ * }\\\\cdots {C}_{k + 1, k}^{ * } = {C}_{j - 1, j}\\\\cdots {C}_{k, k + 1}\\\\;\\\\text{ for }k < j.\\n$$\\n\\nIt follows that, as composition of unitary mappings, we have\\n\\n$$ \\n{C}_{k, j} : {H}_{k}\\\\left( C\\\\right) \\\\rightarrow {H}_{j}\\\\left( C\\\\right) \\\\text{ is unitary,}\\;j, k \\\\in \\\\mathbb{Z}, \\n$$\\n\\nand so\\n\\n$$ \\n{C}_{j, k} = {C}_{k, j}^{ * } = {C}_{k, j}^{-1},\\\\;{C}_{j, k}{C}_{k, j} = {1}_{{H}_{j}\\\\left( C\\\\right) }\\n$$\\n\\nfor all $j, k \\\\in \\\\mathbb{Z}$ .\\n\\nWe shall now show that\\n\\n$$ \\n{C}_{\\\\left( k\\\\right) } = {C}_{0, k}{C}_{\\\\left( 0\\\\right) }{C}_{k,0}. \\n$$\\n\\nFor $k \\\\in \\\\mathbb{N}$ and $x \\\\in {H}_{k + 1}\\\\left( C\\\\right)$ we have\\n\\n$$ \\n{C}_{\\\\left( k\\\\right) }x = {Cx} = {C}^{-k}C{C}^{k}x = {C}_{0, k}{C}_{\\\\left( 0\\\\right) }{C}_{k,0}x.\\n$$\\n\\nThus, equality follows since\\n\\n$$ \\n{C}_{0, k}\\\\left\\\\lbrack {D\\\\left( {C}_{\\\\left( 0\\\\right) }\\\\right) }\\\\right\\\\rbrack = {C}_{0, k}\\\\left\\\\lbrack {{H}_{1}\\\\left( C\\\\right) }\\\\right\\\\rbrack = {C}^{-k}\\\\left\\\\lbrack {{H}_{1}\\\\left( C\\\\right) }\\\\right\\\\rbrack = {C}^{-k - 1}\\\\left\\\\lbrack {{H}_{0}\\\\left( C\\\\right) }\\\\right\\\\rbrack = {H}_{k + 1}\\\\left( C\\\\right)\\n$$\\n\\nand so\\n\\n$$ \\nD\\\\left( C\\\\right) = D\\\\left( {C}_{\\\\left( 0\\\\right) }\\\\right) = {C}_{k,0}\\\\left\\\\lbrack {{H}_{k + 1}\\\\left( C\\\\right) }\\\\right\\\\rbrack .\\n$$\\n\\nFor $k \\\\in {\\\\mathbb{Z}}_{ < 0}$ we may argue in the following way. Equation (2.1.9) still holds at least for $x \\\\in {H}_{\\\\left| k\\\\right| + 1}\\\\left( C\\\\right)$ . Since ${H}_{\\\\left| k\\\\right| + 1}\\\\left( C\\\\right)$ is dense in ${H}_{k + 1}\\\\left( C\\\\right)$, it follows that\\n\\n$$ \\n{C}_{\\\\left( k\\\\right) } = \\\\overline{{C}_{\\\\left( k\\\\right) }{ \\\\mid }_{{H}_{\\\\left| k\\\\right| + 1}\\\\left( C\\\\right) }} = \\\\overline{\\\\left( {{C}_{0, k}{C}_{\\\\left( 0\\\\right) }{C}_{k,0}}\\\\right) { \\\\mid }_{{H}_{\\\\left| k\\\\right| + 1}\\\\left( C\\\\right) }}.\\n$$\\n\\nOn the other hand,\\n\\n$$ \\nD\\\\left( {\\\\left. \\\\overline{\\\\left( {C}_{0, k}{C}_{\\\\left( 0\\\\right) }{C}_{k,0}\\\\right) }\\\\right| }_{{H}_{\\\\left| k\\\\right| + 1}\\\\left( C\\\\right) }\\\\right) = D\\\\left( {\\\\left. \\\\overline{\\\\left( {C}_{\\\\left( 0\\\\right) }{C}_{k,0}\\\\right) }\\\\right| }_{{H}_{\\\\left| k\\\\right| + 1}\\\\left( C\\\\right) }\\\\right)\\n$$\\n\\nand for every $y \\\\in D\\\\left( {\\\\left. \\\\left( {C}_{\\\\left( 0\\\\right) }{C}_{k,0}\\\\right) \\\\right| }_{{H}_{\\\\left| k\\\\right| + 1}\\\\left( C\\\\right) }\\\\right)$ there is a sequence $x = {\\\\left( {x}_{j}\\\\right) }_{j} \\\\in {H}_{\\\\left| k\\\\right| + 1}{\\\\left( C\\\\right) }^{\\\\mathbb{N}}$ such that\\n\\n$$ \\n{x}_{j} \\\\rightarrow y\\\\;\\\\text{ in }{H}_{k}\\\\left( C\\\\right)\\n$$\\n\\nand\\n\\n$$ \\n{C}_{\\\\left( 0\\\\right) }{C}_{k,0}{x}_{j} = {C}_{\\\\left( 0\\\\right) }^{k + 1}{x}_{j} \\\\rightarrow {z}_{\\\\infty } = \\\\overline{\\\\left. \\\\left( {{C}_{\\\\left( 0\\\\right) }{C}_{k,0}}\\\\right) \\\\right| {H}_{\\\\left| k\\\\right| + 1}\\\\left( C\\\\right) }y\\\\;\\\\text{ in }{H}_{0}\\\\left( C\\\\right) .\\n$$\\n\\nfor $j \\\\rightarrow \\\\infty$ . Thus, we find that\\n\\n$$ \\n{C}_{\\\\left( 0\\\\right) }^{k}{x}_{j} \\\\rightarrow {C}_{k,0}y\\\\;\\\\text{ in }{H}_{0}\\\\left( C\\\\right)\\n$$\\n\\nand\\n\\n$$ \\n{C}_{\\\\left( 0\\\\right) }^{k + 1}{x}_{j} \\\\rightarrow {C}_{k,0}y\\\\;\\\\text{ in }{H}_{0}\\\\left( C\\\\right)\\n$$\\n\\nfor $j \\\\rightarrow \\\\infty$ .\\n\\nThus, we have shown that ${C}_{\\\\left( k\\\\right) }$ is unitarily equivalent to ${C}_{\\\\left( 0\\\\right) } = C$, and hence the lemma is proved.",
        "id": "college_math_92758"
    },
    {
        "informal_statement": "Theorem 8.30. If \\( S \\) is a self-adjoint operator and \\( f \\) is an analytic vector of \\( S \\) , then\\n\\n\\[ f \\in D\\left( {\\mathrm{e}}^{zS}\\right) \\text{ and }{\\mathrm{e}}^{zS}f = \\mathop{\\sum }\\limits_{{n = 0}}^{\\infty }\\frac{{z}^{n}}{n!}{S}^{n}f \\]\\n\\nfor every \\( z \\in \\mathbb{K} \\) such that \\( \\left| z\\right| < t\\left( f\\right) \\) .",
        "informal_proof": "Proof. Let \\( E \\) denote the spectral family of \\( S \\) . Then\\n\\n\\[ {\\left\\{ {\\int }_{-M}^{M}{\\left| {\\mathrm{e}}^{zs}\\right| }^{2}\\mathrm{\\;d}\\parallel E\\left( s\\right) f{\\parallel }^{2}\\right\\} }^{1/2} = \\begin{Vmatrix}{{\\int }_{-M}^{M}{\\mathrm{e}}^{zs}\\mathrm{\\;d}E\\left( s\\right) f}\\end{Vmatrix} \\]\\n\\n\\[ = \\begin{Vmatrix}{{\\int }_{-M}^{M}\\mathop{\\sum }\\limits_{{n = 0}}^{\\infty }\\frac{{\\left( zs\\right) }^{n}}{n!}\\mathrm{d}E\\left( s\\right) f}\\end{Vmatrix} \\]\\n\\n\\[ \\leq \\mathop{\\sum }\\limits_{{n = 0}}^{\\infty }\\frac{{\\left| z\\right| }^{n}}{n!}\\begin{Vmatrix}{{\\int }_{-M}^{M}{s}^{n}\\mathrm{\\;d}E\\left( s\\right) f}\\end{Vmatrix} \\]\\n\\n\\[ \\leq \\mathop{\\sum }\\limits_{{n = 0}}^{\\infty }\\frac{{\\left| z\\right| }^{n}}{n!}\\begin{Vmatrix}{{S}^{n}f}\\end{Vmatrix} \\]\\n\\nfor every \\( M > 0 \\) and \\( z \\in \\mathbb{K} \\) such that \\( \\left| z\\right| < t\\left( f\\right) \\) . Letting \\( M \\) tend to \\( \\infty \\), we obtain that \\( f \\in D\\left( {\\mathrm{e}}^{zS}\\right) \\) . Furthermore,\\n\\n\\[ {\\mathrm{e}}^{zS}f = {\\int }_{-\\infty }^{\\infty }{\\mathrm{e}}^{zs}\\mathrm{\\;d}E\\left( s\\right) f = {\\int }_{-\\infty }^{\\infty }\\mathop{\\sum }\\limits_{{n = 0}}^{\\infty }\\frac{{\\left( zs\\right) }^{n}}{n!}\\mathrm{d}E\\left( s\\right) f \\]\\n\\n\\[ = \\mathop{\\sum }\\limits_{{n = 0}}^{m}\\frac{{z}^{n}}{n!}{\\int }_{-\\infty }^{\\infty }{s}^{n}\\mathrm{\\;d}E\\left( s\\right) f + {\\int }_{-\\infty }^{\\infty }\\mathop{\\sum }\\limits_{{n = m + 1}}^{\\infty }\\frac{{\\left( zs\\right) }^{n}}{n!}\\mathrm{d}E\\left( s\\right) f \\]\\n\\n\\[ = \\mathop{\\lim }\\limits_{{m \\rightarrow \\infty }}\\mathop{\\sum }\\limits_{{n = 0}}^{m}\\frac{{z}^{n}}{n!}{S}^{n}f = \\mathop{\\sum }\\limits_{{n = 0}}^{\\infty }\\frac{{z}^{n}}{n!}{S}^{n}f \\]\\n\\nsince\\n\\n\\[ \\begin{Vmatrix}{{\\int }_{-\\infty }^{\\infty }\\mathop{\\sum }\\limits_{{n = m + 1}}^{\\infty }\\frac{{\\left( zs\\right) }^{n}}{n!}\\mathrm{d}E\\left( s\\right) f}\\end{Vmatrix} = \\mathop{\\lim }\\limits_{{M \\rightarrow \\infty }}\\begin{Vmatrix}{{\\int }_{-M}^{M}\\mathop{\\sum }\\limits_{{n = m + 1}}^{\\infty }\\frac{{\\left( zs\\right) }^{n}}{n!}\\mathrm{d}E\\left( s\\right) f}\\end{Vmatrix} \\]\\n\\n\\[ = \\mathop{\\lim }\\limits_{{M \\rightarrow \\infty }}\\begin{Vmatrix}{\\mathop{\\sum }\\limits_{{n = m + 1}}^{\\infty }\\frac{{z}^{n}}{n!}{\\int }_{-M}^{M}{s}^{n}\\mathrm{\\;d}E\\left( s\\right) f}\\end{Vmatrix} \\]\\n\\n\\[ \\leq \\mathop{\\sum }\\limits_{{n = m + 1}}^{\\infty }\\frac{{\\left| z\\right| }^{n}}{n!}\\begin{Vmatrix}{{\\int }_{-\\infty }^{\\infty }{s}^{n}\\mathrm{\\;d}E\\left( s\\right) f}\\end{Vmatrix} \\]\\n\\n\\[ = \\mathop{\\sum }\\limits_{{n = m + 1}}^{\\infty }\\frac{{\\left| z\\right| }^{n}}{n!}\\begin{Vmatrix}{{S}^{n}f}\\end{Vmatrix} \\rightarrow 0\\;\\text{ as }\\;m \\rightarrow \\infty . \\]",
        "id": "college_math_222454"
    },
    {
        "informal_statement": "Proposition 2.4. Let \\( \\mathcal{R}\\left( {\\mathrm{x},\\mathrm{t}}\\right) \\) be the Schwartz kernel of the operator \\( \\mathrm{R} \\) . Then the wave front set of \\( \\mathcal{R} \\) consists of the points \\( \\{ \\left( {\\mathrm{x},\\xi ,\\mathrm{t}, - \\tau }\\right) \\} \\) where \\( \\left( {\\mathrm{x},\\xi ,\\mathrm{t},\\tau }\\right) \\) lies on the graph of the symplectic imbedding \\( \\phi : {\\mathrm{T}}^{ * }{\\mathrm{R}}^{\\mathrm{q}} \\rightarrow {\\mathrm{T}}^{ * }{\\mathrm{R}}^{\\mathrm{n}} \\) , \\( \\phi \\left( {\\mathrm{t}, r}\\right) = \\left( {0,0,\\mathrm{t}, r}\\right) \\)",
        "informal_proof": "Proof. \\( \\mathcal{R} \\) is the oscillatory integral:\\n\\n\\[ \\mathcal{R}\\left( {y, t,{t}^{\\prime }}\\right) = \\int {e}^{i\\left( {t - {t}^{\\prime }}\\right) r - 1/2{\\left| y\\right| }^{2}\\left| r\\right| }{\\left( \\left| r\\right| /\\pi \\right) }^{p/4}{dr} \\]\\n\\n\\( {}^{ * } \\) Strictly speaking we should assume \\( k \\leq 0 \\) and require that \\( Q \\) map \\( {L}^{2} \\) into \\( {\\mathrm{L}}^{2} \\) ; however we will deliberately be a little vague on this point.\\n\\nwhich is smooth except when \\( y = 0 \\) and \\( t = {t}^{\\prime } \\) . For \\( {t}^{\\prime } \\) fixed, \\( \\mathcal{R} \\) satisfies the equations, \\( {D}_{j}\\mathcal{R} = 0 \\), and it also satisfies the equations \\( \\left( {1/\\sqrt{-1}}\\right) \\left( {\\partial /\\partial {t}_{i} - \\partial /\\partial {t}_{i}^{\\prime }}\\right) \\mathcal{R} = 0 \\) with \\( i = 1,\\cdots, q \\) ; so the wave front set is contained in the set where the symbols of these equations vanish. Q.E.D.",
        "id": "college_math_291066"
    },
    {
        "informal_statement": "Corollary 8.5. In a bipartite graph, the size of a maximum independent set is equal to the size of a minimum line cover.",
        "informal_proof": "Proof. We know from Proposition 8.17 that the complement of a point cover is an independent set. Consequently, by Corollary 8.4, the size of a maximum independent set is \\( n - \\left| M\\right| \\), where \\( M \\) is a maximum matching and \\( n \\) is the number of vertices in \\( G \\) . Now, from Theorem 8.15 (3), for any maximum matching \\( M \\) and any minimal line cover \\( \\mathcal{C} \\) we have \\( \\left| M\\right| + \\left| \\mathcal{C}\\right| = n \\) and so, the size of a maximum independent set is equal to the size of a minimal line cover.",
        "id": "college_math_160787"
    },
    {
        "informal_statement": "Proposition 7.2\\n\\n\\\\[ \\nE\\\\left\\\\lbrack {S}_{N\\\\left( t\\\\right) + 1}\\\\right\\\\rbrack = \\\\mu \\\\left\\\\lbrack {m\\\\left( t\\\\right) + 1}\\\\right\\\\rbrack \\n\\\\]",
        "informal_proof": "To see how Proposition 7.2 can be used to establish the elementary renewal theorem, let \\\\( Y\\\\left( t\\\\right) \\\\) denote the time from \\\\( t \\\\) until the next renewal. \\\\( Y\\\\left( t\\\\right) \\\\) is called the excess, or residual life, at \\\\( t \\\\) . As the first renewal after \\\\( t \\\\) will occur at time \\\\( t + Y\\\\left( t\\\\right) \\\\), we see that\\n\\n\\\\[ \\n{S}_{N\\\\left( t\\\\right) + 1} = t + Y\\\\left( t\\\\right) \\n\\\\]\\n\\nTaking expectations and utilizing Proposition 7.2 yields\\n\\n\\\\[ \\n\\\\mu \\\\left\\\\lbrack {m\\\\left( t\\\\right) + 1}\\\\right\\\\rbrack = t + E\\\\left\\\\lbrack {Y\\\\left( t\\\\right) }\\\\right\\\\rbrack \\\\tag{7.9} \\n\\\\]\\n\\nwhich implies that\\n\\n\\\\[ \\n\\\\frac{m\\\\left( t\\\\right) }{t} = \\\\frac{1}{\\\\mu } - \\\\frac{1}{t} + \\\\frac{E\\\\left\\\\lbrack {Y\\\\left( t\\\\right) }\\\\right\\\\rbrack }{t\\\\mu } \\n\\\\]",
        "id": "college_math_369663"
    },
    {
        "informal_statement": "Theorem 10.49 (Kato). Fix $n = 1,2,3$ and denote by $\\\\left( {{\\\\mathbf{y}}_{1},\\\\ldots ,{\\\\mathbf{y}}_{N}}\\\\right)$ the elements in ${\\\\mathbb{R}}^{nN}$, where ${\\\\mathbf{y}}_{k} \\\\in {\\\\mathbb{R}}^{n}$ for any $k = 1,\\\\ldots, N$ . If $\\\\Delta$ is the Laplacian (10.62) on ${\\\\mathbb{R}}^{nN}$, consider the differential operator $- \\\\Delta + V, V$ being the multiplicative operator given by:\\n\\n$$ V\\\\left( {{\\\\mathbf{y}}_{1},\\\\ldots ,{\\\\mathbf{y}}_{N}}\\\\right) \\\\mathrel{\\\\text{:=}} \\\\mathop{\\\\sum }\\\\limits_{{k = 1}}^{N}{V}_{k}\\\\left( {\\\\mathbf{y}}_{k}\\\\right) + \\\\mathop{\\\\sum }\\\\limits_{{i, j = 1}}^{N}{V}_{ij}\\\\left( {{\\\\mathbf{y}}_{i} - {\\\\mathbf{y}}_{j}}\\\\right) , \\\\tag{10.70} $$\\n\\nwhere\\n\\n$$ {\\\\left\\\\{ {V}_{k}\\\\right\\\\} }_{k = 1,\\\\ldots, N} \\\\subset {L}^{2}\\\\left( {{\\\\mathbb{R}}^{n},{dx}}\\\\right) + {L}^{\\\\infty }\\\\left( {{\\\\mathbb{R}}^{n},{dx}}\\\\right) ,{\\\\left\\\\{ {V}_{ij}\\\\right\\\\} }_{i < {ji}, j = 1,\\\\ldots, N} \\\\subset {L}^{2}\\\\left( {{\\\\mathbb{R}}^{n},{dx}}\\\\right) + {L}^{\\\\infty }\\\\left( {{\\\\mathbb{R}}^{n},{dx}}\\\\right) $$\\n\\nare real functions. Then:\\n\\n(a) $- \\\\Delta + V$ is essentially self-adjoint on $\\\\mathcal{D}\\\\left( {\\\\mathbb{R}}^{nN}\\\\right)$ and $\\\\mathcal{S}\\\\left( {\\\\mathbb{R}}^{nN}\\\\right)$ .\\n\\n(b) The only self-adjoint extension $\\\\overline{-\\\\Delta + V}$ of the operators in (a) coincides with the (self-adjoint) operator $\\\\overline{-\\\\Delta } + V$ defined on $D\\\\left( \\\\overline{-\\\\Delta }\\\\right)$ .\\n\\n(c) $\\\\sigma \\\\left( \\\\overline{-\\\\Delta + V}\\\\right)$ is lower bounded.",
        "informal_proof": "Proof. We prove for $n = 3$, for the other cases are identical. Consider the potential ${V}_{12}\\\\left( {{\\\\mathbf{y}}_{1} - {\\\\mathbf{y}}_{2}}\\\\right)$ and call ${\\\\Delta }_{1}$ the Laplacian corresponding to the coordinates of ${\\\\mathbf{y}}_{1}$ . Take $\\\\varphi \\\\in \\\\mathcal{S}\\\\left( {\\\\mathbb{R}}^{3N}\\\\right)$ . Fix ${\\\\mathbf{y}}_{2},\\\\ldots {\\\\mathbf{y}}_{N} \\\\in {\\\\mathbb{R}}^{3\\\\left( {N - 1}\\\\right) }$ and define ${\\\\mathbb{R}}^{3} \\\\ni {\\\\mathbf{y}}_{1} \\\\mapsto {\\\\varphi }^{\\\\prime }\\\\left( {\\\\mathbf{y}}_{1}\\\\right) \\\\mathrel{\\\\text{:=}}$ $\\\\varphi \\\\left( {{\\\\mathbf{y}}_{1},{\\\\mathbf{y}}_{2},\\\\ldots ,{\\\\mathbf{y}}_{N}}\\\\right) .{\\\\varphi }^{\\\\prime }$ belongs in $\\\\mathcal{D}\\\\left( {\\\\mathbb{R}}^{3N}\\\\right)$ or $\\\\mathcal{S}\\\\left( {\\\\mathbb{R}}^{3N}\\\\right)$, according to whether $\\\\varphi \\\\in$ $\\\\mathcal{D}\\\\left( {\\\\mathbb{R}}^{3N}\\\\right)$ or $\\\\varphi \\\\in \\\\mathcal{S}\\\\left( {\\\\mathbb{R}}^{3N}\\\\right)$ respectively. Similarly, let ${\\\\mathbb{R}}^{3} \\\\ni {\\\\mathbf{y}}_{1} \\\\mapsto {V}_{12}^{\\\\prime }\\\\left( {\\\\mathbf{y}}_{1}\\\\right) \\\\mathrel{\\\\text{:=}} {V}_{12}\\\\left( {{\\\\mathbf{y}}_{1} - }\\\\right.$ $\\\\left. {\\\\mathbf{y}}_{2}\\\\right)$ . As in the previous proof, by decomposing ${V}_{12} = {\\\\left( {V}_{12}\\\\right) }_{2} + {\\\\left( {V}_{12}\\\\right) }_{\\\\infty }$ we arrive at the estimate, for any $a > 0$ and any ${\\\\mathbf{y}}_{2},\\\\ldots ,{\\\\mathbf{y}}_{N}$ :\\n\\n$$ {\\\\begin{Vmatrix}{V}_{12}^{\\\\prime }{\\\\varphi }^{\\\\prime }\\\\end{Vmatrix}}_{{L}^{2}\\\\left( {\\\\mathbb{R}}^{3}\\\\right) } \\\\leq a{\\\\begin{Vmatrix}{\\\\left( {V}_{12}\\\\right) }_{2}\\\\end{Vmatrix}}_{{L}^{2}\\\\left( {\\\\mathbb{R}}^{3}\\\\right) }{\\\\begin{Vmatrix}-{\\\\Delta }_{1}{\\\\varphi }^{\\\\prime }\\\\end{Vmatrix}}_{{L}^{2}\\\\left( {\\\\mathbb{R}}^{3}\\\\right) } + \\\\left( {b + {\\\\begin{Vmatrix}{\\\\left( {V}_{12}\\\\right) }_{\\\\infty }\\\\end{Vmatrix}}_{{L}^{\\\\infty }\\\\left( {\\\\mathbb{R}}^{3}\\\\right) }}\\\\right) {\\\\begin{Vmatrix}{\\\\varphi }^{\\\\prime }\\\\end{Vmatrix}}_{2} $$\\n\\nwhere $b > 0$ depends on $a$, not on ${\\\\mathbf{y}}_{2},\\\\ldots {\\\\mathbf{y}}_{N} \\\\in {\\\\mathbb{R}}^{3\\\\left( {N - 1}\\\\right) }$ . Norms are in the spaces over the first copy of ${\\\\mathbb{R}}^{3}$ in ${\\\\mathbb{R}}^{3N}$ . It is important to note, due to the invariance of $\\\\left( {{\\\\mathbf{y}}_{1},{\\\\mathbf{y}}_{2}}\\\\right) \\\\mapsto {V}_{12}\\\\left( {{\\\\mathbf{y}}_{1} - {\\\\mathbf{y}}_{2}}\\\\right)$ under translations, that the norms ${\\\\begin{Vmatrix}{\\\\left( {V}_{12}\\\\right) }_{k}\\\\end{Vmatrix}}_{{L}^{k}\\\\left( {\\\\mathbb{R}}^{3}\\\\right) }$ do not depend on the variable ${\\\\mathbf{y}}_{2}$ . From Remarks 10.42 this inequality is the same as\\n\\n$$ {\\\\begin{Vmatrix}{V}_{12}^{\\\\prime }{\\\\varphi }^{\\\\prime }\\\\end{Vmatrix}}_{{L}^{2}\\\\left( {\\\\mathbb",
        "id": "college_math_116170"
    },
    {
        "informal_statement": "Lemma 3 Let $\\\\kappa : \\\\mathcal{B} \\\\times S \\\\rightarrow {\\\\mathbb{R}}_{ + }$ be a Feller kernel and ${A}_{ * }$ be the bounded linear map on ${C}^{b}\\\\left( S\\\\right)$ induced by (9). Then the following are equivalent:\\n\\n(a) $\\\\kappa$ is top-irreducible.\\n\\n(b) For any nonempty open strict subset $U$ of $S$ there exists some $s \\\\in S \\\\smallsetminus U$ such that $\\\\kappa \\\\left( {U, s}\\\\right) > 0$ .\\n\\n(c) For any $f \\\\in {\\\\dot{C}}_{ + }^{b}\\\\left( S\\\\right), S = \\\\mathop{\\\\bigcup }\\\\limits_{{n \\\\in {\\\\mathbb{Z}}_{ + }}}\\\\left\\\\{ {{A}_{ * }^{n}f > 0}\\\\right\\\\} = : U\\\\left( f\\\\right)$ .\\n\\n(d) For any Lipschitz continuous $f : S \\\\rightarrow {\\\\mathbb{R}}_{ + }$ that is not identically equal to 0,\\n\\n$$ S = \\\\mathop{\\\\bigcup }\\\\limits_{{n \\\\in {\\\\mathbb{Z}}_{ + }}}\\\\left\\\\{ {{A}_{ * }^{n}f > 0}\\\\right\\\\} = : U\\\\left( f\\\\right) . $$\\n\\nHere, $\\\\left\\\\{ {{A}_{ * }^{n}f > 0}\\\\right\\\\}$ is a shorthand for $\\\\left\\\\{ {s \\\\in S;\\\\left( {{A}_{ * }^{n}f}\\\\right) \\\\left( s\\\\right) > 0}\\\\right\\\\}$ .",
        "informal_proof": "Proof (a) $\\\\Rightarrow$ (b): Suppose that (b) does not hold: Then there exists some nonempty open strict subset $U$ of $S$ such that $\\\\kappa \\\\left( {U, s}\\\\right) = 0$ for all $s \\\\in S \\\\smallsetminus U$ . Since $\\\\kappa \\\\left( {S, \\\\cdot }\\\\right)$ is bounded, there exists some $c > 0$ such that $\\\\kappa \\\\left( {U, s}\\\\right) \\\\leq c{\\\\chi }_{U}\\\\left( s\\\\right)$ for all $s \\\\in S$ . Then\\n\\n$$ {\\\\kappa }^{*2}\\\\left( {U, s}\\\\right) = {\\\\int }_{S}\\\\kappa \\\\left( {U, t}\\\\right) \\\\kappa \\\\left( {{dt}, s}\\\\right) \\\\leq {\\\\int }_{S}c{\\\\chi }_{U}\\\\left( s\\\\right) \\\\kappa \\\\left( {{dt}, s}\\\\right) = {c\\\\kappa }\\\\left( {U, s}\\\\right) \\\\leq {c}^{2}{\\\\chi }_{U}\\\\left( s\\\\right) . $$\\n\\nBy induction, ${\\\\kappa }^{n * }\\\\left( {U, s}\\\\right) \\\\leq {c}^{n}{\\\\chi }_{U}\\\\left( s\\\\right)$ for all $s \\\\in S$ and all $n \\\\in \\\\mathbb{N}$ . So (a) does not hold.\\n\\n(b) $\\\\Rightarrow$ (c): Since $\\\\kappa$ is a Feller kernel, the functions ${A}_{ * }^{n}f$ in part (c) are continuous and $U\\\\left( f\\\\right)$ is open as union of open sets. Since $f$ is not the zero function and ${A}_{ * }^{0}f = f$ , $U\\\\left( f\\\\right)$ is nonempty. Suppose $U\\\\left( f\\\\right) \\\\neq S$ . By (b), there exists some $s \\\\in S \\\\smallsetminus U\\\\left( f\\\\right)$ such that $\\\\kappa \\\\left( {U\\\\left( f\\\\right), s}\\\\right) > 0$ . Since the measure $\\\\kappa \\\\left( {\\\\cdot, s}\\\\right)$ is continuous from below, there is some $n \\\\in \\\\mathbb{N}$ such that $\\\\kappa \\\\left( {\\\\left\\\\{ {{A}_{ * }^{n}f > 0}\\\\right\\\\}, s}\\\\right) > 0$ . This implies that $\\\\left( {{A}_{ * }^{n + 1}f}\\\\right) \\\\left( s\\\\right) > 0$ and $s \\\\in U\\\\left( f\\\\right)$, a contradiction.\\n\\n(c) $\\\\Rightarrow$ (d): obvious.\\n\\n(d) $\\\\Rightarrow$ (a): Let $U$ be a nonempty open subset of $S$ . Choose some ${t}_{0} \\\\in U$ . Then there exists some Lipschitz continuous $f : S \\\\rightarrow \\\\left\\\\lbrack {0,1}\\\\right\\\\rbrack$ such that $f\\\\left( {t}_{0}\\\\right) = 1, f\\\\left( t\\\\right) \\\\leq {\\\\chi }_{U}\\\\left( t\\\\right)$ for all $t \\\\in S$ [14, L.2.1]. By (d), for any $s \\\\in S$, there is some $n \\\\in {\\\\mathbb{Z}}_{ + }$ such that $0 <$ $\\\\left( {{A}_{ * }^{n}f}\\\\right) \\\\left( s\\\\right)$ . Let $s \\\\in S \\\\smallsetminus U$ . Then $\\\\left( {{A}_{ * }^{0}f}\\\\right) \\\\left( s\\\\right) = f\\\\left( s\\\\right) \\\\leq {\\\\chi }_{U}\\\\left( s\\\\right) = 0$ and $0 < \\\\left( {{A}_{ * }^{n}f}\\\\right) \\\\left( s\\\\right)$ for some $n \\\\in \\\\mathbb{N}$ . Since ${A}_{ * }^{n}$ is induced by ${\\\\kappa }^{n * }$ ,\\n\\n$$ 0 < \\\\left( {{A}_{ * }^{n}f}\\\\right) \\\\left( s\\\\right) \\\\leq {\\\\int }_{S}{\\\\chi }_{U}\\\\left( t\\\\right) {\\\\kappa }^{n * }\\\\left( {{dt}, s}\\\\right) \\\\leq {\\\\kappa }^{n * }\\\\left( {U, s}\\\\right) . $$\\n\\nSo (a) holds.",
        "id": "college_math_102385"
    },
    {
        "informal_statement": "Theorem 14.6.2 Let ${N}_{P}$ be the number of clients (fixed, independent of $w$ ). Assume Heuristic 14.6.1 and that all clients are reliable and have the same computing power. The average-case expected number of group operations performed by the distributed kangaroo method for each client is $\\left( {2 + o\\left( 1\\right) }\\right) \\sqrt{w}/{N}_{P}$ .",
        "informal_proof": "Proof Since we do not know where the wild kangaroo is, we speak of the front herd and the rear herd. The distance (in the exponent) between the front herd and the rear herd is, on average, $w/4$ . So it takes $w/\\left( {4m}\\right)$ steps for the rear herd to reach the starting point of the front herd.\\n\\n---\\n\\nWe now consider the footsteps of the rear herd in the region already visited by the front herd of kangaroos. Assuming the ${N}_{P}/2$ kangaroos of the front herd are independent, the region already covered by these kangaroos is expected to have ${N}_{P}/2$ footprints in each interval of length $m$ . Hence, under our heuristic assumptions, the probability that a random footprint of one of the rear kangaroos lands on a footprint of one of the front kangaroos is ${N}_{P}/\\left( {2m}\\right)$ . Since there are ${N}_{P}/2$ rear kangaroos, all mutually independent, the probability of one of the rear kangaroos landing on a tame footprint is ${N}_{P}^{2}/\\left( {4m}\\right)$ . By the heuristic assumption the expected number of footprints to be made before a collision occurs is ${4m}/{N}_{P}^{2}$ .\\n\\nFinally, the collision will not be detected until a distinguished point is visited. Hence, one expects a further $1/\\theta$ steps to be made.\\n\\nThe expected number of group operations made by each client in the average case is therefore $w/\\left( {4m}\\right) + {4m}/{N}_{P}^{2} + 1/\\theta$ . Ignoring the $1/\\theta$ term, this expression is minimised by taking $m = {N}_{P}\\sqrt{w}/4$ . The result follows.",
        "id": "college_math_73049"
    },
    {
        "informal_statement": "Theorem 8.3 (Chebyshev’s inequality) Let \\( X \\) be a random variable with range \\( \\mathbb{R} \\) and let \\( c \\geq 0 \\) be arbitrary; then\\n\\n\\[ P\\left( {\\left| {X - \\mu }\\right| \\geq c}\\right) \\leq \\frac{{\\sigma }^{2}}{{c}^{2}} \\tag{8.11} \\]",
        "informal_proof": "Proof Let \\( A = \\{ x \\in R;\\left| {x - \\mu }\\right| \\geq c\\} \\) . By (8.9) and using the fact that \\( \\mathbb{R} = A \\cup \\bar{A} \\) , we have\\n\\n\\[ {\\sigma }^{2} = {\\int }_{-\\infty }^{\\infty }{\\left( x - \\mu \\right) }^{2}f\\left( x\\right) \\mathrm{d}x \\]\\n\\n\\[ = {\\int }_{A}{\\left( x - \\mu \\right) }^{2}f\\left( x\\right) \\mathrm{d}x + {\\int }_{\\bar{A}}{\\left( x - \\mu \\right) }^{2}f\\left( x\\right) \\mathrm{d}x \\]\\n\\n\\[ \\geq {\\int }_{A}{\\left( x - \\mu \\right) }^{2}f\\left( x\\right) \\mathrm{d}x \\]\\n\\n\\[ \\geq {c}^{2}{\\int }_{A}f\\left( x\\right) \\mathrm{d}x \\]\\n\\n\\[ = {c}^{2}{p}_{X}\\left( A\\right) \\]\\n\\n\\[ = {c}^{2}P\\left( {\\left| {X - \\mu }\\right| \\geq c}\\right) \\]\\n\\nby \\( \\left( {8.1}\\right) \\)\\n\\nand the result follows.",
        "id": "college_math_252536"
    },
    {
        "informal_statement": "Example 11.5.5 Translations of a normed space.\\n\\nSuppose that $\\\\left( {E,\\\\parallel \\\\text{.||}}\\\\right)$ is a normed space. If $a \\\\in E$, let ${T}_{a}\\\\left( x\\\\right) = x + a;{T}_{a}$ is a translation. It is an isometry of $\\\\left( {E,\\\\parallel \\\\text{.||}}\\\\right)$ onto itself, since",
        "informal_proof": "$$ \\n\\\\begin{Vmatrix}{{T}_{a}\\\\left( x\\\\right) - {T}_{a}\\\\left( y\\\\right) }\\\\end{Vmatrix} = \\\\parallel \\\\left( {x + a}\\\\right) - \\\\left( {y + a}\\\\right) \\\\parallel = \\\\parallel x - y\\\\parallel .\\n$$",
        "id": "college_math_27579"
    },
    {
        "informal_statement": "Lemma 2.75. The implicit scheme and the Crank-Nicolson scheme are \\( {L}_{2} \\) stable for all values of \\( h \\) and \\( \\tau \\) . The explicit scheme is \\( {L}_{2} \\) stable if \\( \\tau /{h}^{2} \\leq 1/2 \\) .",
        "informal_proof": "Proof: Since all eigenvalues \\( {\\lambda }_{j} \\) of the operator \\( - {D}^{ - }{D}^{ + } \\) are positive by (6.11), the \\( {L}_{2} \\) stability of the implicit and the Crank-Nicolson schemes is immediate from (6.14). For the explicit scheme, for each \\( j \\) we have \\( \\left| {q\\left( {\\tau {\\lambda }_{j}}\\right) }\\right| \\leq 1 \\) if and only if \\( \\tau {\\lambda }_{j} \\leq 2 \\) . Again invoking (6.11), this condition is satisfied if \\( \\tau /{h}^{2} \\leq 1/2 \\) . -",
        "id": "college_math_213504"
    },
    {
        "informal_statement": "Example 2.4.2. Suppose now we wish to find the discrete logarithm $x =$ ${\\log }_{59}{67}{\\;\\operatorname{mod}\\;{113}}$, such that ${67} = {59}^{x}{\\;\\operatorname{mod}\\;{113}}$ .",
        "informal_proof": "[1] $y = {67}, a = {59}$ and $n = {113}, s = \\lfloor \\sqrt{113}\\rfloor = {10}$ . \\n\\n[2] Computing the baby step: \\n\\n$$ \\nS = \\left\\{ {\\left( {y,0}\\right) ,\\left( {{ya},1}\\right) ,\\left( {y{a}^{2},2}\\right) ,\\left( {y{a}^{3},3}\\right) ,\\cdots ,\\left( {y{a}^{9},9}\\right) {\\;\\operatorname{mod}\\;{113}}}\\right\\} \\n$$ \\n\\n$$ \\n= \\;\\{ \\left( {{67},0}\\right) ,\\left( {{67} \\cdot {59},1}\\right) ,\\left( {{67} \\cdot {59}^{2},2}\\right) ,\\left( {{67} \\cdot {59}^{3},3}\\right) ,\\left( {{67} \\cdot {59}^{4},4}\\right) , \\n$$ \\n\\n$$ \\n\\left( {{67} \\cdot {59}^{5},5}\\right) ,\\left( {{67} \\cdot {59}^{6},6}\\right) ,\\left( {{67} \\cdot {59}^{7},7}\\right) ,\\left( {{67} \\cdot {59}^{8},8}\\right) , \\n$$ \\n\\n$$ \\n\\left. {\\left( {{67} \\cdot {59}^{9},9}\\right) {\\;\\operatorname{mod}\\;{113}}}\\right\\} \\n$$ \\n\\n$$ \\n= \\;\\{ \\left( {{67},0}\\right) ,\\left( {{111},1}\\right) ,\\left( {{108},2}\\right) ,\\left( {{44},3}\\right) ,\\left( {{110},4}\\right) ,\\left( {{49},5}\\right) ,\\left( {{66},6}\\right) , \\n$$ \\n\\n$$ \\n\\left( {{52},7}\\right) ,\\left( {{17},8}\\right) ,\\left( {{99},9}\\right) \\} \\n$$ \\n\\n$$ \\n= \\{ \\left( {{17},8}\\right) ,\\left( {{44},3}\\right) ,\\left( {{49},5}\\right) ,\\left( {{52},7}\\right) ,\\left( {{66},6}\\right) ,\\left( {{67},0}\\right) ,\\left( {{99},9}\\right) , \\n$$ \\n\\n$$ \\n\\left( {{108},2}\\right) ,\\left( {{110},4}\\right) ,\\left( {{111},1}\\right) \\} \\n$$ \\n\\n[3] Computing the giant-step: \\n\\n$$ \\nT = \\left\\{ {\\left( {{a}^{s}, s}\\right) ,\\left( {{a}^{2s},{ss}}\\right) ,\\left( {{a}^{3s},{3s}}\\right) ,\\cdots \\left( {{a}^{10s},{10s}}\\right) {\\;\\operatorname{mod}\\;{113}}}\\right\\} \\n$$ \\n\\n$$ \\n= \\;\\{ \\left( {{59}^{10},{10}}\\right) ,\\left( {{59}^{2 \\cdot {10}},2 \\cdot {10}}\\right) ,\\left( {{59}^{3 \\cdot {10}},3 \\cdot {10}}\\right) ,\\left( {{59}^{4 \\cdot {10}},4 \\cdot {10}}\\right) , \\n$$ \\n\\n$$ \\n\\left( {{59}^{5 \\cdot {10}},5 \\cdot {10}}\\right) ,\\left( {{59}^{6 \\cdot {10}},6 \\cdot {10}}\\right) ,\\left( {{59}^{7 \\cdot {10}},7 \\cdot {10}}\\right) ,\\left( {{59}^{8 \\cdot {10}},8 \\cdot {10}}\\right) , \\n$$ \\n\\n$$ \\n\\left. {\\left( {{59}^{9 \\cdot {10}},9 \\cdot {10}}\\right) {\\;\\operatorname{mod}\\;{113}}}\\right\\} \\n$$ \\n\\n$$ \\n= \\;\\{ \\left( {{72},{10}}\\right) ,\\left( {{99},{20}}\\right) ,\\left( {9,{30}}\\right) ,\\left( {{83},{40}}\\right) ,\\left( {{100},{50}}\\right) ,\\left( {{81},{60}}\\right) , \\n$$ \\n\\n$$ \\n\\left( {{69},{70}}\\right) ,\\left( {{109},{80}}\\right) ,\\left( {{51},{90}}\\right) ,\\left( {{56},{100}}\\right) \\} \\n$$ \\n\\n$$ \\n= \\;\\{ \\left( {9,{30}}\\right) ,\\left( {{51},{90}}\\right) ,\\left( {{56},{100}}\\right) ,\\left( {{69},{70}}\\right) ,\\left( {{72},{10}}\\right) ,\\left( {{81},{60}}\\right) ,\\left( {{83},{40}}\\right) , \\n$$ \\n\\n$$ \\n\\left( {{99},{20}}\\right) ,\\left( {{100},{50}}\\right) ,\\left( {{109},{80}}\\right) \\} \\n$$ \\n\\n[4] Matching and computing: The number 99 is the common value of the first element in pairs of both lists $S$ and $T$ with $r = 9$ and ${st} = {20}$ , so $x = {st} - r = {20} - 9 = {11}$ . That is, ${\\log }_{59}{67}\\left( {\\;\\operatorname{mod}\\;{113}}\\right) = {11}$, or equivalently, ${59}^{11}\\left( {\\;\\operatorname{mod}\\;{113}}\\right) = {67}$ .",
        "id": "college_math_85924"
    },
    {
        "informal_statement": "Theorem 2.6.2. Every analytic s.m.s. is u.m.",
        "informal_proof": "Proof. See Cohn (1980), Theorem 8.6.13, p. 294, and Dudley (1989), Theorem 13.2.6.",
        "id": "college_math_31300"
    },
    {
        "informal_statement": "Lemma 6.5.13\\n\\nIf we replace \\( {P}_{i} \\) and \\( {P}_{j} \\) by \\( {Q}_{i} \\) and \\( {Q}_{j} \\) in \\( \\mathcal{P} \\), then the resulting set of paths has the property that their end vertices belong to \\( S \\) and that the sum of the lengths of \\( {Q}_{i} \\) and \\( {Q}_{j} \\) is less than that of the sum of the lengths of the paths \\( {P}_{i} \\) and \\( {P}_{j} \\) in \\( \\mathcal{P} \\) . This contradicts the choice of \\( \\mathcal{P} \\), and hence \\( \\mathcal{P} \\) has the stated property.",
        "informal_proof": "Proof of Corollary 6.5.12. Let \\( G \\) be a 4-edge-connected graph. In view of Theorem 6.5.4, it suffices to show that \\( G \\) contains a dominating trail.\\n\\nBy Corollary 4.4.6, \\( G \\) contains two edge-disjoint spanning trees \\( {T}_{1} \\) and \\( {T}_{2} \\) . Let \\( S \\) be the set of vertices of odd degree in \\( {T}_{1} \\) . Then \\( \\left| S\\right| \\) is even. Let \\( \\left| S\\right| = {2k}, k \\geq 1 \\) . By Lemma 6.5.13, there exists a set of \\( k \\) pairwise edge-disjoint paths \\( \\left\\{ {{P}_{l},{P}_{2},\\ldots ,{P}_{k}}\\right\\} \\) in \\( {T}_{2} \\) with the property stated in Lemma 6.5.13. Then \\( {G}_{0} = {T}_{1} \\cup \\left( {{P}_{1} \\cup {P}_{2} \\cup \\ldots {P}_{k}}\\right) \\) is a connected spanning subgraph of \\( G \\) in which each vertex is of even degree. Thus, \\( {G}_{0} \\) is a dominating trail of \\( G \\) .",
        "id": "college_math_193044"
    },
    {
        "informal_statement": "Lemma 2. If there exist two lines ${l}_{1}$ and ${l}_{2}$ such that $\\\\left| {\\\\operatorname{right}\\\\left( {l}_{1}\\\\right) \\\\cap R}\\\\right| = \\\\mid \\\\operatorname{right}\\\\left( {l}_{2}\\\\right) \\\\cap$ $R\\\\left| {\\\\operatorname{and}\\\\left| {\\\\operatorname{right}\\\\left( {l}_{1}\\\\right) \\\\cap B}\\\\right| < }\\\\right| \\\\operatorname{right}\\\\left( {l}_{2}\\\\right) \\\\cap B \\\\mid$, then for every integer $n,\\\\left| {\\\\operatorname{right}\\\\left( {l}_{1}\\\\right) \\\\cap B}\\\\right| \\\\leq$ $n \\\\leq \\\\left| {\\\\operatorname{right}\\\\left( {l}_{2}\\\\right) \\\\cap B}\\\\right|$, there exists a line ${l}_{3}$ such that $\\\\left| {\\\\operatorname{right}\\\\left( {l}_{3}\\\\right) \\\\cap R}\\\\right| = \\\\left| {\\\\operatorname{right}\\\\left( {l}_{1}\\\\right) \\\\cap R}\\\\right|$ and $\\\\left| {\\\\operatorname{right}\\\\left( {l}_{3}\\\\right) \\\\cap B}\\\\right| = n$ .",
        "informal_proof": "Proof of Theorem 4. Let $g = {g}_{1} + {g}_{2} + {g}_{3}$ . We shall prove the theorem by induction on $g$ . It is trivial that the theorem holds for $g = 1$, and so we may assume $g \\\\geq 2$ . Moreover, by Theorem 1, if ${g}_{1} = {g}_{2} = 0,{g}_{2} = {g}_{3} = 0$ or ${g}_{1} = {g}_{3} = 0$ then the theorem is true. So we may assume that\\n\\nat least two of ${g}_{1},{g}_{2}$ and ${g}_{3}$ are greater than or equal to 1 .(2)\\n\\nAssume that there exist three integers $r \\\\geq 0, s \\\\geq 0, t \\\\geq 0$ and two lines ${l}_{1}$ and ${l}_{2}$ such that $1 \\\\leq r + s + t \\\\leq g - 1,\\\\left| {\\\\operatorname{right}\\\\left( {l}_{1}\\\\right) \\\\cap R}\\\\right| = \\\\left| {\\\\operatorname{right}\\\\left( {l}_{2}\\\\right) \\\\cap R}\\\\right| = a\\\\left( {r + s}\\\\right) + \\\\left( {a + 1}\\\\right) t$ ,\\n\\n$\\\\left| {\\\\operatorname{right}\\\\left( {l}_{1}\\\\right) \\\\cap B}\\\\right| \\\\leq {br} + \\\\left( {b + 1}\\\\right) \\\\left( {s + t}\\\\right)$ and $\\\\left| {\\\\operatorname{right}\\\\left( {l}_{2}\\\\right) \\\\cap B}\\\\right| \\\\geq {br} + \\\\left( {b + 1}\\\\right) \\\\left( {s + t}\\\\right)$ . Then by Lemma 2, there exists a line ${l}_{3}$ that satisfies\\n\\n$$\\n\\\\left| {\\\\operatorname{right}\\\\left( {l}_{3}\\\\right) \\\\cap R}\\\\right| = a\\\\left( {r + s}\\\\right) + \\\\left( {a + 1}\\\\right) t\\\\text{ and }\\\\left| {\\\\operatorname{right}\\\\left( {l}_{3}\\\\right) \\\\cap B}\\\\right| = {br} + \\\\left( {b + 1}\\\\right) \\\\left( {s + t}\\\\right) . \\\\tag{3}\\n$$\\n\\nBy applying the inductive hypotheses to $\\\\operatorname{right}\\\\left( {l}_{3}\\\\right)$ and $\\\\operatorname{left}\\\\left( {l}_{3}\\\\right)$ respectively, we can obtain the desired balanced subdivision of the plane. Therefore we may assume that the next claim holds.\\n\\nClaim 1. Let $\\\\left( {r, s, t}\\\\right)$ be a triple of integers such that $0 \\\\leq r \\\\leq {g}_{1},0 \\\\leq s \\\\leq {g}_{2}$ , $0 \\\\leq t \\\\leq {g}_{3}$ and $1 \\\\leq r + s + t \\\\leq g - 1$ . Then for every line $l$ with $\\\\left| {\\\\operatorname{right}\\\\left( l\\\\right) \\\\cap R}\\\\right| =$ $a\\\\left( {r + s}\\\\right) + \\\\left( {a + 1}\\\\right) t$, we always have either\\n\\n$$\\n\\\\left| {\\\\operatorname{right}\\\\left( l\\\\right) \\\\cap B}\\\\right| < {br} + \\\\left( {b + 1}\\\\right) \\\\left( {s + t}\\\\right) \\\\;\\\\text{ or }\\\\;\\\\left| {\\\\operatorname{right}\\\\left( l\\\\right) \\\\cap B}\\\\right| > {br} + \\\\left( {b + 1}\\\\right) \\\\left( {s + t}\\\\right) ,\\n$$\\n\\nin particular, $\\\\left| {\\\\operatorname{right}\\\\left( l\\\\right) \\\\cap B}\\\\right| \\\\neq {br} + \\\\left( {b + 1}\\\\right) \\\\left( {s + t}\\\\right)$ .\\n\\nBy Claim 1, we can define the sign of every triple $\\\\left( {i, j, k}\\\\right)$ with $0 \\\\leq i \\\\leq {g}_{1},0 \\\\leq$ $j \\\\leq {g}_{2},0 \\\\leq k \\\\leq {g}_{3}$ and $1 \\\\leq i + j + k \\\\leq g - 1$ as follows: For every line $l$ with $\\\\left| {\\\\operatorname{right}\\\\left( l\\\\right) \\\\cap R}\\\\right| = a\\\\left( {i + j}\\\\right) + \\\\left( {a + 1}\\\\right) k,$",
        "id": "college_math_57978"
    },
    {
        "informal_statement": "Example 10.28 A fisherman catches fish in a large lake with lots of fish, at a Poisson rate of two per hour. If, on a given day, the fisherman spends randomly anywhere between 3 and 8 hours fishing, find the expected value and the variance of the number of fish he catches.",
        "informal_proof": "Solution: Let \\( X \\) be the number of hours the fisherman spends fishing. Then \\( X \\) is a uniform random variable over the interval \\( \\\\left( {3,8}\\\\right) \\) . Label the time the fisherman begins fishing on the given day at \\( t = 0 \\) . Let \\( N\\\\left( t\\\\right) \\) denote the total number of fish caught at or prior to \\( t \\) . Then \\( \\\\{ N\\\\left( t\\\\right) : t \\\\geq 0\\\\} \\) is a Poisson process with parameter \\( \\\\lambda = 2 \\) . Assuming that \\( X \\) is independent of \\( \\\\{ N\\\\left( t\\\\right) : t \\\\geq 0\\\\} \\), we have\\n\\n\\[ E\\\\left\\\\lbrack {N\\\\left( X\\\\right) \\\\mid X = t}\\\\right\\\\rbrack = E\\\\left\\\\lbrack {N\\\\left( t\\\\right) }\\\\right\\\\rbrack = {2t}. \\]\\n\\nThis implies that\\n\\n\\[ E\\\\left\\\\lbrack {N\\\\left( X\\\\right) \\\\mid X}\\\\right\\\\rbrack = {2X}. \\]\\n\\nTherefore,\\n\\n\\[ E\\\\left\\\\lbrack {N\\\\left( X\\\\right) }\\\\right\\\\rbrack = E\\\\left\\\\lbrack {E\\\\left( {N\\\\left( X\\\\right) \\\\mid X}\\\\right) }\\\\right\\\\rbrack = E\\\\left( {2X}\\\\right) = {2E}\\\\left( X\\\\right) = 2 \\\\cdot \\\\frac{8 + 3}{2} = {11}. \\]\\n\\nSimilarly,\\n\\n\\[ \\\\operatorname{Var}\\\\left( {N\\\\left( X\\\\right) \\\\mid X = t}\\\\right) = \\\\operatorname{Var}\\\\left( {N\\\\left( t\\\\right) }\\\\right) = {2t}. \\]\\n\\nThus\\n\\n\\[ \\\\operatorname{Var}\\\\left( {N\\\\left( X\\\\right) \\\\mid X}\\\\right) = {2X} \\]\\n\\nBy Theorem 10.10,\\n\\n\\[ \\\\operatorname{Var}\\\\left( {N\\\\left( X\\\\right) }\\\\right) = E\\\\left\\\\lbrack {\\\\operatorname{Var}\\\\left( {N\\\\left( X\\\\right) \\\\mid X}\\\\right) }\\\\right\\\\rbrack + \\\\operatorname{Var}\\\\left( {E\\\\left\\\\lbrack {N\\\\left( X\\\\right) \\\\mid X}\\\\right\\\\rbrack }\\\\right) \\]\\n\\n\\[ = E\\\\left( {2X}\\\\right) + \\\\operatorname{Var}\\\\left( {2X}\\\\right) = {2E}\\\\left( X\\\\right) + 4\\\\operatorname{Var}\\\\left( X\\\\right) \\]\\n\\n\\[ = 2 \\\\cdot \\\\frac{8 + 3}{2} + 4 \\\\cdot \\\\frac{{\\\\left( 8 - 3\\\\right) }^{2}}{12} = {19.33}.\\\\;\\\\blacklozenge \\]",
        "id": "college_math_279509"
    },
    {
        "informal_statement": "Theorem 1. for any $k \\in \\mathbb{N}$ , $$ {\\mathrm{{QNC}}}^{k} \\subseteq {\\mathrm{{QMNC}}}^{k} \\subseteq {\\mathrm{{QNC}}}^{k + 1} $$ $$ {\\mathrm{{BQNC}}}^{k} \\subseteq {\\mathrm{{BQMNC}}}^{k} \\subseteq {\\mathrm{{BQNC}}}^{k + 1} $$",
        "informal_proof": "Proof. Lemma 1 implies ${\\mathrm{{QNC}}}^{k} \\subseteq {\\mathrm{{QMNC}}}^{k}$ . Moreover, for any measurement pattern $t$ of input size $n$ and polynomial size, $\\log \\left( {\\operatorname{size}\\left( t\\right) }\\right) = O\\left( {\\log \\left( n\\right) }\\right)$ . So, according to Lemma 1, ${\\mathrm{{QMNC}}}^{k} \\subseteq {\\mathrm{{QNC}}}^{k + 1}$ .",
        "id": "college_math_129626"
    },
    {
        "informal_statement": "Corollary 29.41. Assume (H). Then the accessory problem (A) has a solution, and the minimal value \\( {\\mu }_{\\min } \\) of (A) is the smallest eigenvalue of (E). Let\\n\\n\\( {\\delta F}\\left( {u;h}\\right) = b\\left( h\\right) \\) for all \\( h \\in X \\) . Then:\\n\\n(a) If \\( {\\mu }_{\\min } > 0 \\), then \\( u \\) is a strict local minimal point of \\( \\left( \\mathbf{P}\\right) \\) and \\( u \\) is stable.\\n\\n(b) If \\( {\\mu }_{\\min } < 0 \\) and \\( X \\) is dense in \\( Y \\), then \\( u \\) is not a local minimal point of \\( \\left( \\mathbf{P}\\right) \\) and \\( u \\) is unstable.",
        "informal_proof": "Proof of Corollary 29.41. The first statement follows from Theorem 22.G.\\n\\nAd(a). This is a consequence of Theorem 29.I.\\n\\n\\( \\operatorname{Ad}\\left( b\\right) \\) . If \\( {\\mu }_{\\min } < 0 \\), then there exists an \\( h \\in Y \\) such that \\( a\\left( {h, h}\\right) < 0 \\) . Since \\( a : Y \\times Y \\rightarrow \\mathbb{R} \\) is continuous and \\( X \\) is dense in \\( Y \\), there is an \\( \\bar{h} \\in X \\) such that \\( a\\left( {\\bar{h},\\bar{h}}\\right) < 0 \\), and hence \\( {\\delta }^{2}F\\left( {u;\\bar{h}}\\right) < 0 \\).",
        "id": "college_math_155099"
    },
    {
        "informal_statement": "Example 7.22 Suppose \\( n = 3 \\), we model prior ignorance by setting \\( v = 0 \\) , \\( \\mathbf{\\beta } = \\mathbf{0} \\), and \\( \\alpha = - 1 \\), and we obtain the following data:\\n\\n<table><thead><tr><th>Case</th><th>\\( {X}_{1} \\)</th><th>\\( {X}_{2} \\)</th><th>\\( {X}_{3} \\)</th></tr></thead><tr><td>1</td><td>1</td><td>2</td><td>6</td></tr><tr><td>2</td><td>5</td><td>8</td><td>2</td></tr><tr><td>3</td><td>2</td><td>4</td><td>1</td></tr><tr><td>4</td><td>8</td><td>6</td><td>3</td></tr></table>\\n\\nThen \\( M = 4 \\) and\\n\\n\\[{\\mathbf{x}}^{\\left( 1\\right) } = \\left( \\begin{array}{l} 1 \\\\ 2 \\\\ 6 \\end{array}\\right) \\;{\\mathbf{x}}^{\\left( 2\\right) } = \\left( \\begin{array}{l} 5 \\\\ 8 \\\\ 2 \\end{array}\\right) \\;{\\mathbf{x}}^{\\left( 3\\right) } = \\left( \\begin{array}{l} 2 \\\\ 4 \\\\ 1 \\end{array}\\right) \\;{\\mathbf{x}}^{\\left( 4\\right) } = \\left( \\begin{array}{l} 8 \\\\ 6 \\\\ 3 \\end{array}\\right) .\\n\\]",
        "informal_proof": "So\\n\\n\\[ \\begin{aligned} \\overline{\\mathbf{x}} & = \\frac{\\left( \\begin{array}{l} 1 \\\\ 2 \\\\ 6 \\end{array}\\right) + \\left( \\begin{array}{l} 5 \\\\ 8 \\\\ 2 \\end{array}\\right) + \\left( \\begin{array}{l} 2 \\\\ 4 \\\\ 1 \\end{array}\\right) + \\left( \\begin{array}{l} 8 \\\\ 6 \\\\ 3 \\end{array}\\right) }{4} \\\\ & = \\left( \\begin{array}{l} 4 \\\\ 5 \\\\ 3 \\end{array}\\right) \\end{aligned} \\]\\n\\nand\\n\\n\\[ \\mathbf{s} = \\left( \\begin{array}{r} - 3 \\\\ - 3 \\\\ 3 \\end{array}\\right) \\left( \\begin{array}{lll} - 3 & - 3 & 3 \\end{array}\\right) + \\left( \\begin{array}{r} 1 \\\\ 3 \\\\ - 1 \\end{array}\\right) \\left( \\begin{array}{lll} 1 & 3 & - 1 \\end{array}\\right) \\]\\n\\n\\[ + \\left( \\begin{array}{l} - 2 \\\\ - 1 \\\\ - 2 \\end{array}\\right) \\left( \\begin{array}{lll} - 2 & - 1 & - 2 \\end{array}\\right) + \\left( \\begin{array}{l} 4 \\\\ 1 \\\\ 0 \\end{array}\\right) \\left( \\begin{array}{lll} 4 & 1 & 0 \\end{array}\\right) \\]\\n\\n\\[ = \\left( \\begin{matrix} {30} & {18} & - 6 \\\\ {18} & {20} & - {10} \\\\ - 6 & - {10} & {14} \\end{matrix}\\right) \\]\\n\\nSo\\n\\n\\[ {\\mathbf{\\beta }}^{ * } = \\mathbf{s} = \\left( \\begin{matrix} {30} & {18} & - 6 \\\\ {18} & {20} & - {10} \\\\ - 6 & - {10} & {14} \\end{matrix}\\right) \\;\\text{ and }\\;{\\alpha }^{ * } = M - 1 = 3, \\]\\n\\nand\\n\\n\\[ {\\mathbf{\\mu }}^{ * } = \\overline{\\mathbf{x}} = \\left( \\begin{array}{l} 4 \\\\ 5 \\\\ 3 \\end{array}\\right) \\;\\text{ and }\\;{v}^{ * } = M = 4. \\]",
        "id": "college_math_308307"
    },
    {
        "informal_statement": "Theorem 1.9. Let ${\\\\left( T\\\\left( t\\\\right) \\\\right) }_{t \\\\geq 0}$ be a ${C}_{0}$ -semigroup on a Banach space $\\\\mathbb{X}$, with generator $A$ . Then\\n\\n$${\\\\sigma }_{p}\\\\left( {T\\\\left( t\\\\right) }\\\\right) \\\\smallsetminus \\\\{ 0\\\\} = {e}^{t{\\\\sigma }_{p}\\\\left( A\\\\right) },\\\\;\\\\forall t \\\\geq 0.$$",
        "informal_proof": "Proof. For the proof see e.g. p. 46 in [Pazy (90)].",
        "id": "college_math_136084"
    },
    {
        "informal_statement": "Theorem 2. Let \\( |\\psi \\rangle ,\\left| {\\psi }^{\\prime }\\right\\rangle \\) be n-qubit symmetric states with Majorana configurations \\( {\\mathcal{C}}_{\\psi },{\\mathcal{C}}_{{\\psi }^{\\prime }} \\) . Then \\( |\\psi \\rangle ,\\left| {\\psi }^{\\prime }\\right\\rangle \\) are local unitary equivalent if and only if there exists an element \\( g \\) in \\( U\\left( 2\\right) \\) such that\\n\\n\\[{\\mathcal{C}}_{{\\psi }^{\\prime }} = g{\\mathcal{C}}_{\\psi }\\]",
        "informal_proof": "Proof of Theorem 2. Let \\( \\left| {\\psi \\rangle ,\\left| {\\psi }^{\\prime }\\right\\rangle \\text{be symmetric states with Majorana configura-}}\\right| \\) tions \\( {\\mathcal{C}}_{\\psi } = \\left\\{ {\\left| {\\psi }_{1}\\right\\rangle ,\\ldots ,\\left| {\\psi }_{n}\\right\\rangle }\\right\\} \\) and \\( {\\mathcal{C}}_{{\\psi }^{\\prime }} = \\left\\{ {\\left| {\\psi }_{1}^{\\prime }\\right\\rangle ,\\ldots ,\\left| {\\psi }_{n}^{\\prime }\\right\\rangle }\\right\\} \\) . If there is a rotation of the Bloch sphere given by \\( g \\) in \\( U\\left( 2\\right) \\) that takes \\( {\\mathcal{C}}_{\\psi } \\) to \\( {\\mathcal{C}}_{{\\psi }^{\\prime }} \\), then (possibly after renumbering) we have \\( g\\left| {\\psi }_{j}\\right\\rangle \\equiv \\left| {\\psi }_{j}^{\\prime }\\right\\rangle \\) for \\( 1 \\leq j \\leq n \\), and hence \\( \\left| {\\psi }^{\\prime }\\right\\rangle \\equiv {g}^{\\otimes n}|\\psi \\rangle \\) . Conversely, if \\( \\left| {\\psi }^{\\prime }\\right\\rangle = U|\\psi \\rangle \\) for some local unitary \\( U \\), then by Theorem 1, there is a \\( g \\) in \\( U\\left( 2\\right) \\) such that \\( \\left| {\\psi }^{\\prime }\\right\\rangle = {g}^{\\otimes n}|\\psi \\rangle \\) . We can interpret this \\( g \\) as a rotation of the Bloch sphere, and it is clear that we have \\( {\\mathcal{C}}_{{\\psi }^{\\prime }} = g{\\mathcal{C}}_{\\psi } \\) .",
        "id": "college_math_257306"
    },
    {
        "informal_statement": "Theorem 7.5.7. Let $\\\\mu \\\\in {M}_{{\\\\Gamma }_{ + }}\\\\left( G\\\\right)$ be a regular Borel measure on $G$, such that $\\\\mu \\\\star {m}_{r}$ is absolutely continuous with respect to the Haar measure $\\\\sigma$ on $G$ for some $r \\\\in \\\\left( {0,1}\\\\right)$ . Then there exists a function $f \\\\in {L}^{1}\\\\left( {G,\\\\sigma }\\\\right)$ such that $\\\\operatorname{sp}\\\\left( f\\\\right) \\\\subset {\\\\Gamma }_{ + }$ and ${d\\\\mu } = d{\\\\mu }^{f} = {fd\\\\sigma }.$",
        "informal_proof": "For the proof we need several properties of ${\\\\Gamma }_{ + }$-analytic measures on $G$ . Without loss of generality we can assume that ${2\\\\pi } \\\\in \\\\Gamma$ . Let $K = \\\\operatorname{Ker}\\\\left( {\\\\chi }^{2\\\\pi }\\\\right) = \\\\{ g \\\\in$ $\\\\left. {G : {\\\\chi }^{2\\\\pi }\\\\left( g\\\\right) = 1}\\\\right\\\\}$ . Given a $t \\\\in \\\\mathbb{R}$, choose ${g}_{t} \\\\in G$ so that ${\\\\chi }^{a}\\\\left( {g}_{t}\\\\right) = {e}^{iat}$ for each $a \\\\in \\\\Gamma$ . The map $t \\\\mapsto {g}_{t}$ is an imbedding of the group $\\\\mathbb{R}$ into $G$ . This map generates a homomorphism $\\\\pi : K \\\\times \\\\mathbb{R} \\\\rightarrow G$, defined as $\\\\pi \\\\left( {g, t}\\\\right) = g{g}_{t}$ . We recall that the kernel of $\\\\pi$ is the set $\\\\operatorname{Ker}\\\\left( \\\\pi \\\\right) = {\\\\left\\\\{ \\\\left( {g}_{n}, - n\\\\right) \\\\right\\\\} }_{n = 1}^{\\infty }$ (cf. Section 3.1). Clearly, the set $K \\\\times \\\\lbrack 0,1)$ is a fundamental domain of $\\\\pi$, and therefore we can identify the group $G$ with the set $K \\\\times \\\\lbrack 0,1)$ .\\n\\nDenote by $\\\\widetilde{M}\\\\left( {K \\\\times \\\\mathbb{R}}\\\\right)$ the set of locally finite measures on $K \\\\times \\\\mathbb{R}$ which are invariant with respect to shifts by elements in $\\\\operatorname{Ker}\\\\left( \\\\pi \\\\right)$ . Note that the restriction of the space $\\\\widetilde{M}\\\\left( {K \\\\times \\\\mathbb{R}}\\\\right)$ on the set $K \\\\times \\\\lbrack 0,1)$ coincides with the space $M\\\\left( G\\\\right)$ . In particular, if ${\\\\sigma }_{K}$ is the Haar measure on the group $K$, then the restriction of the measure $d{\\\\sigma }_{K} \\\\times {dt}$ on $K \\\\times \\\\lbrack 0,1)$ is the Haar measure $\\\\sigma$ .\\n\\nLet ${H}^{1}$ be the Hardy space on the unit circle $\\\\mathbb{T}$ . The fractional linear transformation $w\\\\left( z\\\\right) = \\\\frac{i - z}{i + z}$ maps $\\\\mathbb{R}$ onto $\\\\mathbb{T} \\\\smallsetminus \\\\{ 1\\\\}$ . Every function in the space ${H}^{1} \\\\circ w =$ $\\\\left\\\\{ {f \\\\circ w : f \\\\in {H}^{1}}\\\\right\\\\}$ can be extended as an analytic function in the upper half-plane ${\\\\mathbb{C}}_{ + }$ .\\n\\nLet ${\\\\widetilde{M}}^{1}\\\\left( {K \\\\times \\\\mathbb{R}}\\\\right)$ be the space of measures on $K \\\\times \\\\mathbb{R}$ of type $f\\\\left( {g, t}\\\\right) \\\\left( {{d\\\\nu }\\\\left( g\\\\right) \\\\times {dt}}\\\\right)$ , where:\\n\\n(a) $\\\\nu$ is a probability measure on $K$ .\\n\\n(b) $f\\\\left( {g, t}\\\\right) \\\\in {L}^{1}\\\\left( {{d\\\\nu }\\\\left( g\\\\right) \\\\times \\\\frac{dt}{1 + {t}^{2}}}\\\\right)$ .\\n\\n(c) For $\\\\nu$ -almost all $g \\\\in K$ the function $t \\\\mapsto f\\\\left( {g, t}\\\\right)$ belongs to ${H}^{1} \\\\circ w$ .\\n\\n(d) The measure $f\\\\left( {g, t}\\\\right) {d\\\\nu }\\\\left( g\\\\right)$ belongs to $\\\\widetilde{M}\\\\left( {K \\\\times \\\\mathbb{R}}\\\\right)$ .\\n\\n(e) $\\\\left| {f\\\\left( {g, t}\\\\right) }\\\\right| \\\\left( {{d\\\\nu }\\\\left( g\\\\right) \\\\times {dt}}\\\\right)$ and ${d\\\\nu }\\\\left( g\\\\right) \\\\times {dt}$ are mutually absolutely continuous measures.\\n\\nThe proof of the next lemma is straightforward (cf. [G10], Theorem 3.2).",
        "id": "college_math_131053"
    },
    {
        "informal_statement": "Proposition 2.3.3. Suppose that both $\\\\left( {\\\\mathrm{P}}_{\\\\mathrm{{SDP}}}\\\\right)$ and $\\\\left( {\\\\mathrm{P}}_{\\\\mathrm{{SDP}} - \\\\mathrm{F}}^{ * }\\\\right)$ have strictly feasible solutions. Then $\\\\bar{X} \\\\in {\\\\mathbb{S}}^{n}$ and ${\\\\overline{\\\\mathbf{z}}}^{ * } \\\\in {\\\\mathbb{R}}^{m}$ are optimal solutions of $\\\\left( {\\\\mathrm{P}}_{\\\\mathrm{{SDP}}}\\\\right)$ and $\\\\left( {\\\\mathrm{P}}_{\\\\mathrm{{SDP}} - \\\\mathrm{F}}^{ * }\\\\right)$, respectively, if and only if there exists an $\\\\bar{S} \\\\in {\\\\mathbb{S}}^{n}$ satisfying\\n\\n$$ \\n{A}_{i} \\\\bullet \\\\bar{X} = {b}_{i},\\\\;i = 1,\\\\ldots, m, \\\\tag{2.60a} \\n$$ \\n\\n$$ \\n\\\\bar{S} + \\\\mathop{\\\\sum }\\\\limits_{{i = 1}}^{m}{\\\\bar{z}}_{i}^{ * }{A}_{i} = C, \\\\tag{2.60b} \\n$$ \\n\\n$$ \\n\\\\bar{X} \\\\succcurlyeq O,\\\\;\\\\bar{S} \\\\succcurlyeq O,\\\\;\\\\bar{S} \\\\bullet \\\\bar{X} = 0. \\\\tag{2.60c} \\n$$",
        "informal_proof": "Proof. We begin by noting that the assumptions required in Proposition 2.2.9 are guaranteed to be satisfied by Proposition 2.3.2. Then Proposition 2.2.9 (c) asserts that $\\\\bar{X}$ and ${\\\\overline{\\\\mathbf{z}}}^{ * }$ are optimal for $\\\\left( {\\\\mathrm{P}}_{\\\\mathrm{{SDP}}}\\\\right)$ and $\\\\left( {\\\\mathrm{P}}_{\\\\mathrm{{SDP}} - \\\\mathrm{F}}^{ * }\\\\right)$, respectively, if and only if they satisfy\\n\\n$$ \\n{\\\\Lambda }^{ * }{\\\\overline{\\\\mathbf{z}}}^{ * } \\\\in \\\\partial f\\\\left( \\\\bar{X}\\\\right) \\\\tag{2.61a} \\n$$ \\n\\n$$ \\n- {\\\\overline{\\\\mathbf{z}}}^{ * } \\\\in \\\\partial g\\\\left( {\\\\Lambda \\\\bar{X}}\\\\right) \\\\text{.} \\\\tag{2.61b} \\n$$ \\n\\nTo see that (2.60) is equivalent to (2.61), we derive the following expressions:\\n\\n$$ \\n\\\\partial f\\\\left( \\\\bar{X}\\\\right) = \\\\left\\\\{ \\\\begin{array}{ll} \\\\left\\\\{ {{X}^{ * } \\\\in {\\\\mathbb{S}}^{n} \\\\mid \\\\left\\\\langle {C - {X}^{ * },\\\\bar{X}}\\\\right\\\\rangle = 0, C - {X}^{ * } \\\\succcurlyeq O}\\\\right\\\\} & \\\\text{ if }\\\\bar{X} \\\\succcurlyeq O, \\\\\\\\ \\\\varnothing & \\\\text{ otherwise,} \\\\end{array}\\\\right. \\\\tag{2.62} \\n$$ \\n\\n$$ \\n\\\\partial g\\\\left( {\\\\Lambda \\\\bar{X}}\\\\right) = \\\\left\\\\{ \\\\begin{array}{ll} {\\\\mathbb{R}}^{m} & \\\\text{ if }\\\\left\\\\langle {{A}_{i},\\\\bar{X}}\\\\right\\\\rangle = {b}_{i}\\\\left( {i = 1,\\\\ldots, m}\\\\right) , \\\\\\\\ \\\\varnothing & \\\\text{ otherwise. } \\\\end{array}\\\\right. \\\\tag{2.63} \\n$$ \\n\\nTo obtain (2.62), we make use of the relation that ${X}^{ * } \\\\in \\\\partial f\\\\left( \\\\bar{X}\\\\right)$ if and only if ${}^{25}$\\n\\n$$ \\nf\\\\left( \\\\bar{X}\\\\right) + {f}^{ * }\\\\left( {X}^{ * }\\\\right) = \\\\left\\\\langle {{X}^{ * },\\\\bar{X}}\\\\right\\\\rangle . \\\\tag{2.64} \\n$$ \\n\\n${}^{25}$ See Table 2.2 and Proposition 2.1.12.\\n\\nFrom (2.49) and (2.54), the left-hand side of (2.64) is finite if and only if\\n\\n$$ \\n\\\\bar{X} \\\\succcurlyeq O,\\\\;C - {X}^{ * } \\\\succcurlyeq O \\n$$ \\n\\nis satisfied. In this case, equation (2.64) is reduced to\\n\\n$$ \\n\\\\langle C,\\\\bar{X}\\\\rangle = \\\\left\\\\langle {{X}^{ * },\\\\bar{X}}\\\\right\\\\rangle \\n$$ \\n\\ni.e., $\\\\left\\\\langle {C - {X}^{ * },\\\\bar{X}}\\\\right\\\\rangle = 0$, and hence we obtain (2.62).\\n\\nConcerning (2.63), we also make use of the fact that ${\\\\mathbf{z}}^{ * } \\\\in \\\\partial g\\\\left( \\\\overline{\\\\mathbf{z}}\\\\right)$ if and only if ${}^{26}$\\n\\n$$ \\ng\\\\left( \\\\overline{\\\\mathbf{z}}\\\\right) + {g}^{ * }\\\\left( {\\\\mathbf{z}}^{ * }\\\\right) = \\\\left\\\\langle {{\\\\mathbf{z}}^{ * },\\\\overline{\\\\mathbf{z}}\\\\right\\\\rangle \\\\tag{2.65} \\n$$ \\n\\nholds. By using (2.50) and (2.55), we see that the left-hand side of (2.65) is finite if and only if\\n\\n$$ \\n\\\\bar{z} = \\\\mathbf{b} \\n$$ \\n\\nis satisfied. In this case, (2.65) holds identically. Thus we obtain\\n\\n$$ \\n\\\\partial g\\\\left( \\\\overline{\\\\mathbf{z}}\\\\right) = \\\\left\\\\{ \\\\begin{array}{ll} {\\\\mathbb{R}}^{m} & \\\\text{ if }\\\\overline{\\\\mathbf{z}} = \\\\mathbf{b}, \\\\\\\\ \\\\varnothing & \\\\text{ otherwise,} \\\\end{array}\\\\right. \\n$$ \\n\\nfrom which and (2.51) we arrive at (2.63).\\n\\nConsequently, by using (2.62) and (2.63), together with (2.56), we see that (2.61) can be written as\\n\\n$$ \\n\\\\mathop{\\\\sum }\\\\limits_{{i = 1}}^{m}{\\\\bar{z}}_{i}^{ * }{A}_{i} \\\\in \\\\left\\\\{ \\\\begin{array}{ll} \\\\left\\\\{ {{X}^{ * } \\\\in {\\\\mathbb{S}}^{n} \\\\mid \\\\left\\\\langle {C - {X}^{ * },\\\\bar{X}}\\\\right\\\\rangle = 0, C - {X}^{ * } \\\\succcurlyeq O}\\\\right\\\\} , & \\\\text{ if }\\\\bar{X} \\\\succcurlyeq O, \\\\\\\\ \\\\varnothing & \\\\text{ otherwise,} \\\\end{array}\\\\right. \\n$$ \\n\\n$$ \\n- {\\\\overline{\\\\mathbf{z}}}^{ * } \\\\in \\\\left\\\\{ \\\\begin{array}{ll} {\\\\mathbb{R}}^{m} & \\\\text{ if }\\\\left\\\\lang",
        "id": "college_math_84029"
    },
    {
        "informal_statement": "Proposition 2.19. Given any metric space \\( \\left( {E, d}\\right) \\), for any subset \\( A \\) of \\( E \\) and any point \\( x \\in E \\), we have \\( x \\in \\bar{A} \\) iff there is a sequence \\( \\left( {a}_{n}\\right) \\) of points \\( {a}_{n} \\in A \\) converging to \\( x \\) .",
        "informal_proof": "Proof. If the sequence \\( \\left( {a}_{n}\\right) \\) of points \\( {a}_{n} \\in A \\) converges to \\( x \\), then for every open subset \\( U \\) of \\( E \\) containing \\( x \\), there is some \\( {n}_{0} \\) such that \\( {a}_{n} \\in U \\) for all \\( n \\geq {n}_{0} \\), so \\( U \\cap A \\neq \\varnothing \\), and Proposition 2.3 implies that \\( x \\in \\bar{A} \\) .\\n\\nConversely, assume that \\( x \\in \\bar{A} \\) . Then for every \\( n \\geq 1 \\), consider the open ball \\( {B}_{0}\\left( {x,1/n}\\right) \\) . By Proposition 2.3, we have \\( {B}_{0}\\left( {x,1/n}\\right) \\cap A \\neq \\varnothing \\), so we can pick some \\( {a}_{n} \\in {B}_{0}\\left( {x,1/n}\\right) \\cap A \\) . This way, we define a sequence \\( \\left( {a}_{n}\\right) \\) of points in \\( A \\), and by construction \\( d\\left( {x,{a}_{n}}\\right) < 1/n \\) for all \\( n \\geq 1 \\), so the sequence \\( \\left( {a}_{n}\\right) \\) converges to \\( x \\) .",
        "id": "college_math_199356"
    },
    {
        "informal_statement": "Example 13.4 Find the shortest curve between two points $A$ and $B$ on the surface of a circular cylinder of radius $a$ . The cylindrical coordinate is $\\\\left( {r,\\\\theta z}\\\\right)$ .",
        "informal_proof": "Solution: The curve length on the surface of a circular cylinder is\\n\\n$$ {\\\\left( ds\\\\right) }^{2} = {\\\\left( dr\\\\right) }^{2} + {\\\\left( rd\\\\theta \\\\right) }^{2} + {\\\\left( dz\\\\right) }^{2} \\\\tag{13.204} $$\\n\\nOn the surface of the cylinder, we have $r = a$ . Thus, we have ${dr} = 0$, and (13.204) becomes\\n\\n$$ {\\\\left( \\\\frac{ds}{d\\\\theta }\\\\right) }^{2} = {a}^{2} + {\\\\left( \\\\frac{dz}{d\\\\theta }\\\\right) }^{2} \\\\tag{13.205} $$\\n\\nUsing (13.205), we have the curve path\\n\\n$$ {ds} = \\\\sqrt{{a}^{2} + {\\\\left( \\\\frac{dz}{d\\\\theta }\\\\right) }^{2}}{d\\\\theta } \\\\tag{13.206} $$\\n\\nThe curve length between two points $A\\\\left( {a,\\\\theta }_{1},{z}_{1}\\\\right)$ and $B\\\\left( {a,\\\\theta }_{2},{z}_{2}\\\\right)$ is\\n\\n$$ s = {\\\\int }_{{\\\\theta }_{1}}^{{\\\\theta }_{2}}\\\\sqrt{{a}^{2} + {\\\\left( \\\\frac{dz}{d\\\\theta }\\\\right) }^{2}}{d\\\\theta } \\\\tag{13.207} $$\\n\\nClearly, we want to minimize the functional $s$ for the shortest curve. Thus, we identify that\\n\\n$$ F = \\\\sqrt{{a}^{2} + {\\\\left( \\\\frac{dz}{d\\\\theta }\\\\right) }^{2}} = F\\\\left( {z}^{\\\\prime }\\\\right) \\\\tag{13.208} $$\\n\\nNote that $F$ is independent of $z$ and $\\\\theta$, and thus we have case (iii) in Section 13.5 or the Euler-Lagrange equation becomes\\n\\n$$ \\\\frac{{d}^{2}z}{d{\\\\theta }^{2}} = 0 \\\\tag{13.209} $$\\n\\nOr, the shortest curve is\\n\\n$$ z = {C}_{1}\\\\theta + {C}_{2},\\\\;r = a \\\\tag{13.210} $$\\n\\nThis is the equation of a circular helix. The problem and its solution in terms of the helix are illustrated in Figure 3.12.",
        "id": "college_math_129262"
    },
    {
        "informal_statement": "Lemma 3. If ${e}_{0},{e}_{3}$ and the first end point of ${e}_{1}$ are on the same grid plane, and ${t}_{{i}_{0}}$ is a root of $\\\\frac{\\\\partial {d}_{i}}{\\\\partial {t}_{i}} = 0$, then ${t}_{{i}_{0}} = 0$, where $i$ equals 1 or 2 .",
        "informal_proof": "Proof. From ${p}_{0}\\\\left( {t}_{0}\\\\right) {p}_{1}\\\\left( 0\\\\right) \\\\bot {e}_{1}$ it follows that\\n\\n$$\\n{d}_{e}\\\\left( {{p}_{0}\\\\left( {t}_{0}\\\\right) {p}_{1}\\\\left( 0\\\\right) }\\\\right) = \\\\min \\\\left\\\\{ {{d}_{e}\\\\left( {{p}_{0}\\\\left( {t}_{0}\\\\right) ,{p}_{1}\\\\left( {t}_{1}\\\\right) }\\\\right) : {t}_{1} \\\\in \\\\left\\\\lbrack {0,1}\\\\right\\\\rbrack }\\\\right\\\\}\\n$$\\n\\n(see Figure 4). Analogously, we have ${d}_{e}\\\\left( {{p}_{2}\\\\left( 0\\\\right) {p}_{3}\\\\left( {t}_{3}\\\\right) }\\\\right) = \\\\min \\\\left\\\\{ {{d}_{e}\\\\left( {{p}_{2}\\\\left( {t}_{2}\\\\right) ,{p}_{3}\\\\left( {t}_{3}\\\\right) }\\\\right) : }\\\\right.$ $\\\\left. {{t}_{2} \\\\in \\\\left\\\\lbrack {0,1}\\\\right\\\\rbrack }\\\\right\\\\}$ and ${d}_{e}\\\\left( {{p}_{1}\\\\left( 0\\\\right) {p}_{2}\\\\left( 0\\\\right) }\\\\right) = \\\\min \\\\left\\\\{ {{d}_{e}\\\\left( {{p}_{1}\\\\left( {t}_{1}\\\\right) ,{p}_{2}\\\\left( {t}_{2}\\\\right) }\\\\right) : {t}_{1},{t}_{2} \\\\in \\\\left\\\\lbrack {0,1}\\\\right\\\\rbrack }\\\\right\\\\}$ . Therefore we have\\n\\n$\\\\min \\\\left\\\\{ {{d}_{e}\\\\left( {{p}_{0}\\\\left( {t}_{0}\\\\right) ,{p}_{1}\\\\left( {t}_{1}\\\\right) }\\\\right) + {d}_{e}\\\\left( {{p}_{1}\\\\left( {t}_{1}\\\\right) ,{p}_{2}\\\\left( {t}_{2}\\\\right) }\\\\right) + {d}_{e}\\\\left( {{p}_{2}\\\\left( {t}_{2}\\\\right) ,{p}_{3}\\\\left( {t}_{3}\\\\right) }\\\\right) : {t}_{1},{t}_{2} \\\\in \\\\left\\\\lbrack {0,1}\\\\right\\\\rbrack }\\\\right\\\\}$\\n\\n$\\\\geq {d}_{e}\\\\left( {{p}_{0}\\\\left( {t}_{0}\\\\right) ,{p}_{1}\\\\left( 0\\\\right) }\\\\right) + {d}_{e}\\\\left( {{p}_{1}\\\\left( 0\\\\right) ,{p}_{2}\\\\left( 0\\\\right) }\\\\right) + {d}_{e}\\\\left( {{p}_{2}\\\\left( 0\\\\right) ,{p}_{3}\\\\left( {t}_{3}\\\\right) }\\\\right)$\\n\\nAssume that we have ${e}_{0} \\\\bot {e}_{1},{e}_{m} \\\\bot {e}_{m + 1}$, and ${e}_{i}\\\\parallel {e}_{i + 1}$ ,(i.e., the set $\\\\left\\\\{ {e}_{1}\\\\right.$ , $\\\\left. {{e}_{2},\\\\ldots ,{e}_{m}}\\\\right\\\\}$ is a set of maximal parallel critical edges of $g$, and ${e}_{0}$ or ${e}_{m + 1}$ is an adjacent critical edge of this set). Furthermore, let $p\\\\left( {t}_{{i}_{0}}\\\\right)$ be a vertex of the MLP of $g$, where $i = 1,2,\\\\ldots, m - 1$ . Analogously, we have the following two lemmas:",
        "id": "college_math_58116"
    },
    {
        "informal_statement": "Lemma 47 Ker \\( \\left( \\psi \\right) \\) is isomorphic to the direct product of \\( Q \\) with a free group \\( {\\mathbb{F}}_{2}\\left( {x, y}\\right) \\) of rank 2, for which a basis \\( \\left( {x, y}\\right) \\) is given by:\\n\\n\\[ x = {\\alpha }_{0}^{2}{\\Delta }_{4}{\\sigma }_{1}^{2}\\text{ and }y = {\\Delta }_{4}{\\sigma }_{2}^{2}. \\tag{3.23} \\]",
        "informal_proof": "Proof By Remark 36(b) and Proposition 37, \\( {B}_{4}\\left( {\\mathbb{S}}^{2}\\right) \\) is isomorphic to the group \\( {\\mathrm{T}}^{ * }{ * }_{{\\mathcal{Q}}_{8}}{\\mathcal{Q}}_{16} \\), where the \\( {\\mathrm{T}}^{ * } \\) -factor \\( {G}_{1} \\) of \\( {B}_{4}\\left( {\\mathbb{S}}^{2}\\right) \\) is generated by \\( Q \\) and \\( {\\alpha }_{1}^{2} \\), and the \\( {\\mathcal{Q}}_{16} \\) -factor \\( {G}_{2} \\) of \\( {B}_{4}\\left( {\\mathbb{S}}^{2}\\right) \\) is generated by \\( Q \\) and \\( {\\alpha }_{0} \\), so \\( {G}_{1} \\cap {G}_{2} = Q \\) . Consider the canonical projection:\\n\\n\\[ \\rho : {B}_{4}\\left( {\\mathbb{S}}^{2}\\right) \\rightarrow {B}_{4}\\left( {\\mathbb{S}}^{2}\\right) /Q.\\]\\n\\nAs in the proof of Proposition 37, we identify the quotient \\( {B}_{4}\\left( {\\mathbb{S}}^{2}\\right) /Q \\) with the free product \\( {\\mathbb{Z}}_{3} * {\\mathbb{Z}}_{2} \\), the \\( {\\mathbb{Z}}_{3} \\) - (resp. \\( {\\mathbb{Z}}_{2} \\) -) factor being generated by \\( a = \\rho \\left( {\\alpha }_{1}\\right) \\) (resp. \\( b = \\rho \\left( {\\alpha }_{0}\\right) ) \\) . Consider the surjective homomorphism \\( \\widehat{\\psi } : {\\mathbb{Z}}_{3} * {\\mathbb{Z}}_{2} \\rightarrow {S}_{3} \\) defined by \\( \\widehat{\\psi }\\left( a\\right) = \\left( {1,3,2}\\right) \\) and \\( \\widehat{\\psi }\\left( b\\right) = \\left( {1,3}\\right) \\) . Since\\n\\n\\[ \\psi \\left( {\\alpha }_{0}\\right) = \\psi \\left( {{\\sigma }_{1}{\\sigma }_{2}{\\sigma }_{3}}\\right) = \\left( {1,2}\\right) \\left( {2,3}\\right) \\left( {1,2}\\right) = \\left( {1,3}\\right) ,\\]\\n\\n\\[ \\psi \\left( {\\alpha }_{1}\\right) = \\psi \\left( {{\\sigma }_{1}{\\sigma }_{2}{\\sigma }_{3}^{2}}\\right) = \\left( {1,2}\\right) \\left( {2,3}\\right) = \\left( {1,3,2}\\right) \\]\\n\\nand \\( {B}_{4}\\left( {\\mathbb{S}}^{2}\\right) = \\left\\langle {{\\alpha }_{0},{\\alpha }_{1}}\\right\\rangle \\) by [8, Theorem 3], it follows that \\( \\widehat{\\psi } \\circ \\rho = \\psi \\), so \\( \\rho \\) induces a homomorphism \\( \\widehat{\\rho } : \\operatorname{Ker}\\left( \\psi \\right) \\rightarrow \\operatorname{Ker}\\left( \\widehat{\\psi }\\right) \\) of the respective kernels. We thus obtain the following commutative diagram of short exact sequences:\\n\\n\\[ \\rho \\left( {\\sigma }_{1}^{2}\\right) = \\rho \\left( {\\sigma }_{3}^{2}\\right) = {\\left( \\rho \\left( {\\alpha }_{0}^{-1}{\\alpha }_{1}\\right) \\right) }^{2} = {\\left( ba\\right) }^{2} \\]\\n\\n\\[ \\rho \\left( {\\sigma }_{2}^{2}\\right) = \\rho \\left( {{\\alpha }_{0}{\\sigma }_{1}^{2}{\\alpha }_{0}^{-1}}\\right) = {\\left( ab\\right) }^{2}, \\]\\n\\n![019188de-de0b-79a5-baaf-419d5c4bb71a_64_566373.jpg](images/019188de-de0b-79a5-baaf-419d5c4bb71a_64_566373.jpg)\\n\\n(3.24)\\n\\nas well as the equality \\( \\operatorname{Ker}\\left( \\widehat{\\rho }\\right) = Q \\) . Taking \\( \\left\\{ {1, a,{a}^{2}, b,{ab},{a}^{2}b}\\right\\} \\) to be the Schreier transversal for \\( \\widehat{\\psi } \\) and applying the Reidemeister-Schreier rewriting process [13], we see that \\( \\operatorname{Ker}\\left( \\widehat{\\psi }\\right) \\) is a free group of rank 2 with basis \\( \\left( {{\\left( ab\\right) }^{2},{\\left( ba\\right) }^{2}}\\right) \\), which implies that \\( \\operatorname{Ker}\\left( \\psi \\right) \\cong {\\mathcal{Q}}_{8} \\rtimes {\\mathbb{F}}_{2} \\) by the commutative diagram (3.24). To determine the action of \\( \\operatorname{Ker}\\left( \\widehat{\\psi }\\right) \\) on \\( Q \\), note by (3.13) and (3.14) that \\( {\\sigma }_{1}^{2} \\) and \\( {\\sigma }_{2}^{2} \\) belong to \\( \\operatorname{Ker}\\left( \\psi \\right) \\), and that:\\n\\nso \\( \\widehat{\\rho }\\left( {\\sigma }_{1}^{2}\\right) = {\\left( ba\\right) }^{2} \\) and \\( \\widehat{\\rho }\\left( {\\sigma }_{2}^{2}\\right) = {\\left( ab\\right) }^{2} \\) . The same equations imply that the actions by conjugation of \\( {\\sigma }_{1}^{2} \\) and \\( {\\sigma }_{2}^{2} \\) on \\( Q \\) yield",
        "id": "college_math_326261"
    },
    {
        "informal_statement": "Proposition 8. Let \\( R \\) be a ring.\\n\\n(i) An ideal \\( \\mathfrak{p} \\subset R \\) is prime if and only if \\( R/\\mathfrak{p} \\) is an integral domain.\\n\\n(ii) An ideal \\( \\mathfrak{m} \\subset R \\) is maximal if and only if \\( R/\\mathfrak{m} \\) is a field.\\n\\nIn particular, every maximal ideal is prime.",
        "informal_proof": "Proof. First of all, note that \\( \\mathfrak{p} \\) is a proper ideal in \\( R \\) if and only if the residue class ring \\( R/\\mathfrak{p} \\) is nonzero, similarly for \\( \\mathfrak{m} \\) . Now assertion (i) is easy to verify. Look at residue classes \\( \\bar{a},\\bar{b} \\in R/\\mathfrak{p} \\) of elements \\( a, b \\in R \\) . Then\\n\\n\\[ a \\cdot b \\in \\mathfrak{p}\\; \\Rightarrow \\;a \\in \\mathfrak{p}\\text{ or }b \\in \\mathfrak{p} \\]\\n\\nis clearly equivalent to\\n\\n\\[ \\bar{a} \\cdot \\bar{b} = 0\\; \\Rightarrow \\;\\bar{a} = 0\\text{ or }\\bar{b} = 0. \\]\\n\\nFurthermore, assertion (ii) is a consequence of the following two lemmas:",
        "id": "college_math_189177"
    },
    {
        "informal_statement": "Theorem 2.4.6 Assume the family $\\\\left( {\\\\mathcal{F}}_{t}^{ \\\\star }\\\\right)$ satisfies the usual conditions and let $A$ be a progressive subset of ${\\\\mathbb{R}}_{ + } \\\\times \\\\Omega$ (Definition 2.3.20). Then, the debut ${D}_{A}$ of $A$ is a stopping time.",
        "informal_proof": "Proof. Suppose the family $\\\\left( {\\\\mathcal{F}}_{t}\\\\right)$ does not satisfy the usual conditions. The set $\\\\left\\\\{ {{D}_{A} < t}\\\\right\\\\}$ is the projection on $\\\\Omega$ of the set $\\\\{ \\\\left( {s,\\\\omega }\\\\right) : s < t,\\\\left( {s,\\\\omega }\\\\right) \\\\in A\\\\}$ which belongs to $\\\\mathcal{B}\\\\left( {\\\\mathbb{R}}_{ + }\\\\right) \\\\times {\\\\mathcal{F}}_{t}$ by the definition of progressive sets (2.2.4). Then, (cf. Theorem 1.3.33)\\n\\n$$ \\n\\\\text{for all}t,\\\\left\\\\{ {{D}_{A} < t}\\\\right\\\\} \\\\text{is}{\\\\mathcal{F}}_{t}\\\\text{analytic.} \\\\tag{2.46}\\n$$ \\n\\nUnder the hypotheses of the statement, the $\\\\sigma$ -field ${\\\\mathcal{F}}_{t}^{ \\\\star }$ is complete and hence equal to a $\\\\mathrm{a}\\\\left( {\\\\mathcal{F}}_{t}\\\\right)$ (1.3.4.1) and (2.41) implies that ${D}_{A}$ is a stopping time of the family $\\\\left( {\\\\mathcal{F}}_{t + }\\\\right)$, which is equal to $\\\\left( {\\\\mathcal{F}}_{t}^{ \\\\star }\\\\right)$ by right-continuity. -",
        "id": "college_math_89855"
    },
    {
        "informal_statement": "Problem 4.3.10. Justify, ${a}^{k} \\equiv {b}^{k}\\\\left( {\\\\operatorname{mod}n}\\\\right)$ and $k \\equiv j\\\\left( {\\\\operatorname{mod}n}\\\\right)$ need not imply that ${a}^{j} \\equiv {b}^{j}\\\\left( {\\\\operatorname{mod}n}\\\\right)$ .",
        "informal_proof": "Solution 4.3.10. Since $4 \\equiv 9\\\\left( {\\\\operatorname{mod}5}\\\\right)$, therefore\\n\\n$$\\n{2}^{2} \\equiv {3}^{2}\\\\left( {\\\\operatorname{mod}5}\\\\right)\\n$$\\n\\n$$\\n2 \\equiv 7\\\\left( {\\\\operatorname{mod}5}\\\\right)\\n$$\\n\\n$$\\n{2}^{7} ≢ {3}^{7}\\\\left( {\\\\operatorname{mod}5}\\\\right) \\\\left\\\\lbrack \\\\text{ Verify! }\\\\right\\\\rbrack .\\n$$",
        "id": "college_math_85228"
    },
    {
        "informal_statement": "Theorem 3.7. Suppose $f : \\\\left\\\\lbrack {a, b}\\\\right\\\\rbrack \\\\rightarrow \\\\mathbb{R}$ is continuous and differentiable at each point of $\\\\left( {a, b}\\\\right)$ and suppose ${f}^{\\\\prime }\\\\left( x\\\\right) > 0$, all $x \\\\in \\\\left( {a, b}\\\\right)$ . Then $f$ is strictly increasing on $\\\\left( {a, b}\\\\right)$ . For each $y \\\\in \\\\left\\\\lbrack {f\\\\left( a\\\\right), f\\\\left( b\\\\right) }\\\\right\\\\rbrack$ there is a unique point $x = g\\\\left( y\\\\right) \\\\in \\\\left\\\\lbrack {a, b}\\\\right\\\\rbrack$ such that $f\\\\left( x\\\\right) = y$ . The function $g = {f}^{-1}$ is differentiable at each point of $\\\\left( {f\\\\left( a\\\\right), f\\\\left( b\\\\right) }\\\\right)$ and $$ {g}^{\\\\prime }\\\\left( y\\\\right) = {\\\\left\\\\lbrack {f}^{\\\\prime }\\\\left( g\\\\left( y\\\\right) \\\\right) \\\\right\\\\rbrack }^{-1}. $$",
        "informal_proof": "Proof. If $x, y \\\\in \\\\left\\\\lbrack {a, b}\\\\right\\\\rbrack$ and $x < y$, application of Theorem 3.4 to $\\\\left\\\\lbrack {x, y}\\\\right\\\\rbrack$ shows that $f\\\\left( x\\\\right) < f\\\\left( y\\\\right)$ . In particular, $f\\\\left( a\\\\right) < f\\\\left( b\\\\right)$ . By Theorem 1.6, if $f\\\\left( a\\\\right) \\\\leq y \\\\leq f\\\\left( b\\\\right)$ there is $x \\\\in \\\\left\\\\lbrack {a, b}\\\\right\\\\rbrack$ with $f\\\\left( x\\\\right) = y$ . Since $f$ is strictly increasing, $x$ is unique. Letting $g = {f}^{-1}$ we note that $g$ is continuous. In fact, suppose $y \\\\in \\\\left( {f\\\\left( a\\\\right), f\\\\left( b\\\\right) }\\\\right)$ and $\\\\varepsilon > 0$ . Take ${y}^{\\\\prime },{y}^{\\\\prime \\\\prime }$ such that $$ f\\\\left( a\\\\right) \\\\leq {y}^{\\\\prime } < y < {y}^{\\\\prime \\\\prime } \\\\leq f\\\\left( b\\\\right) $$ and ${y}^{\\\\prime \\\\prime } - y \\\\leq \\\\varepsilon, y - {y}^{\\\\prime } \\\\leq \\\\varepsilon$ . Let ${x}^{\\\\prime } = g\\\\left( {y}^{\\\\prime }\\\\right), x = g\\\\left( y\\\\right) ,{x}^{\\\\prime \\\\prime } = g\\\\left( {y}^{\\\\prime \\\\prime }\\\\right)$ . Then ${x}^{\\\\prime } < x < {x}^{\\\\prime \\\\prime }$ . Let $\\\\delta = \\\\min \\\\left\\\\{ {{x}^{\\\\prime \\\\prime } - x, x - {x}^{\\\\prime }}\\\\right\\\\}$ . If $\\\\left| {x - w}\\\\right| < \\\\delta$ then $w \\\\in \\\\left( {{x}^{\\\\prime },{x}^{\\\\prime \\\\prime }}\\\\right)$ , so $f\\\\left( w\\\\right) \\\\in \\\\left( {{y}^{\\\\prime },{y}^{\\\\prime \\\\prime }}\\\\right)$, so $\\\\left| {f\\\\left( w\\\\right) - f\\\\left( x\\\\right) }\\\\right| = \\\\left| {f\\\\left( w\\\\right) - y}\\\\right| < \\\\varepsilon$ . Continuity at $f\\\\left( a\\\\right)$ and $f\\\\left( b\\\\right)$ is proved similarly. Finally, let $x = g\\\\left( y\\\\right) ,{x}^{\\\\prime } = g\\\\left( {y}^{\\\\prime }\\\\right)$ . Then (3.3) $$ \\\\frac{g\\\\left( {y}^{\\\\prime }\\\\right) - g\\\\left( y\\\\right) }{{y}^{\\\\prime } - y} = \\\\frac{{x}^{\\\\prime } - x}{f\\\\left( {x}^{\\\\prime }\\\\right) - f\\\\left( x\\\\right) }. $$ As ${y}^{\\\\prime } \\\\rightarrow y$, we have shown that ${x}^{\\\\prime } \\\\rightarrow x$ . Thus (3.3) converges to ${f}^{\\\\prime }{\\\\left( x\\\\right) }^{-1} =$ ${\\\\left\\\\lbrack {f}^{\\\\prime }\\\\left( g\\\\left( y\\\\right) \\\\right) \\\\right\\\\rbrack }^{-1}$ .",
        "id": "college_math_2513"
    },
    {
        "informal_statement": "Theorem 10.102.\\n\\n(i) If $R$ is a domain, $Q = \\\\operatorname{Frac}\\\\left( R\\\\right)$, and $K = Q/R$, then the functor ${\\\\operatorname{Tor}}_{1}^{R}\\\\left( {K,}\\\\right)$ is naturally equivalent to the torsion functor.\\n\\n(ii) ${\\\\operatorname{Tor}}_{1}^{R}\\\\left( {K, A}\\\\right) \\\\cong {tA}$ for all $R$ -modules $A$ .",
        "informal_proof": "Proof. Exactness of\\n\\n$${\\\\operatorname{Tor}}_{2}\\\\left( {K, A/{tA}}\\\\right) \\\\rightarrow {\\\\operatorname{Tor}}_{1}\\\\left( {K,{tA}}\\\\right) \\\\overset{{\\\\iota }_{A}}{ \\\\rightarrow }{\\\\operatorname{Tor}}_{1}\\\\left( {K, A}\\\\right) \\\\rightarrow {\\\\operatorname{Tor}}_{1}\\\\left( {K, A/{tA}}\\\\right) .\\n$$\\n\\nThe first term is $\\\\{ 0\\\\}$, by Lemma 10.101(ii), and the last term is $\\\\{ 0\\\\}$, by Lemma 10.101(iii). Therefore, the map ${\\\\iota }_{A} : {\\\\operatorname{Tor}}_{1}\\\\left( {K,{tA}}\\\\right) \\\\rightarrow {\\\\operatorname{Tor}}_{1}\\\\left( {K, A}\\\\right)$ is an isomorphism.\\n\\nLet $f : A \\\\rightarrow B$ and let ${f}^{\\\\prime } : {tA} \\\\rightarrow {tB}$ be its restriction. The following diagram commutes, because ${\\\\operatorname{Tor}}_{1}\\\\left( {K,}\\\\right)$ is a functor, and this says that the isomorphisms ${\\\\iota }_{A}$ constitute a natural transformation.\\n\\n![images/884.jpg?x=590&y=576&w=385&h=147](images/884.jpg?x=590&y=576&w=385&h=147)",
        "id": "college_math_3442"
    },
    {
        "informal_statement": "Theorem 4.7 Suppose $\\\\left( {R,\\\\mathfrak{m}}\\\\right)$ is a local ring of dimension $d$ and $\\\\operatorname{char}\\\\left( {R/\\\\mathfrak{m}}\\\\right) \\\\neq 2$ . Assume $F \\\\in {\\\\mathcal{P}}^{\\\\mathfrak{m}}\\\\left( R\\\\right)$ is a non-acyclic complex of the form\\n\\n$$ F = \\\\left( {\\\\cdots \\\\rightarrow 0 \\\\rightarrow {F}_{d} \\\\rightarrow \\\\cdots \\\\rightarrow {F}_{0} \\\\rightarrow 0 \\\\rightarrow \\\\cdots }\\\\right) .\\n$$\\n\\nIf $R$ admits a test module relative to ${\\\\psi }_{cy}^{2}$ (for some surjection $Q \\\\rightarrow R$ ), then ${\\\\operatorname{rank}}_{R}\\\\left( F\\\\right) \\\\geq {2}^{d}$ . In particular, the Total Rank Conjecture holds for such a ring $R$ .",
        "informal_proof": "Proof The second assertion follows from first since by the Auslander-Buchsbaum formula [3], the minimal free resolution of a non-zero $R$ -module of finite length and finite projective dimension has the form of $F$ .\\n\\nSuppose $\\\\pi : Q \\\\rightarrow R$ is a surjection with kernel $I$ such that $Q$ is regular and that $M$ is a test module for $R$ relative to $\\\\pi$ and ${\\\\psi }_{cy}^{2}$ . As usual, let ${P}^{M}\\\\overset{ \\\\sim }{ \\\\rightarrow }M$ be the minimal $Q$ -free resolution of $M$ .\\n\\nAs discussed above, we have a pairing\\n\\n$$ - \\\\widetilde{ \\\\cap } - : {K}_{0}^{\\\\mathfrak{m}}\\\\left( R\\\\right) \\\\times {K}_{0}^{V\\\\left( I\\\\right) }\\\\left( Q\\\\right) \\\\rightarrow \\\\mathbb{Z}, $$\\n\\nwith the following two properties:\\n\\n1. For any $R$ -module $N$, if ${P}^{N}\\\\overset{ \\\\sim }{ \\\\rightarrow }N$ is a $Q$ -free resolution of $N$, then\\n\\n$$ \\\\left\\\\lbrack E\\\\right\\\\rbrack \\\\widetilde{ \\\\cap }\\\\left\\\\lbrack {P}^{N}\\\\right\\\\rbrack = \\\\chi \\\\left( {E{ \\\\otimes }_{R}N}\\\\right) = \\\\mathop{\\\\sum }\\\\limits_{i}{\\\\left( -1\\\\right) }^{i}{\\\\operatorname{length}}_{R}{H}_{i}\\\\left( {E{ \\\\otimes }_{R}N}\\\\right) , $$\\n\\nfor any $E \\\\in {\\\\mathcal{P}}^{\\\\mathfrak{m}}\\\\left( R\\\\right)$ .\\n\\n2. For $E \\\\in {\\\\mathcal{P}}^{\\\\mathfrak{m}}\\\\left( R\\\\right)$ and $P \\\\in {\\\\mathcal{P}}^{V\\\\left( I\\\\right) }\\\\left( Q\\\\right)$ we have ${\\\\psi }_{cy}^{2}\\\\left( \\\\left\\\\lbrack E\\\\right\\\\rbrack \\\\right) \\\\widetilde{ \\\\cap }{\\\\psi }_{cy}^{2}\\\\left( \\\\left\\\\lbrack P\\\\right\\\\rbrack \\\\right) =$ ${2}^{\\\\dim \\\\left( Q\\\\right) }\\\\left\\\\lbrack E\\\\right\\\\rbrack \\\\widetilde{ \\\\cap }\\\\left\\\\lbrack P\\\\right\\\\rbrack$\\n\\nSince, by assumption, we have ${\\\\psi }_{cy}^{2}\\\\left( \\\\left\\\\lbrack {P}^{M}\\\\right\\\\rbrack \\\\right) = {2}^{c}\\\\left\\\\lbrack {P}^{M}\\\\right\\\\rbrack$, the second property yields\\n\\n$$ {2}^{\\\\dim \\\\left( Q\\\\right) }\\\\left\\\\lbrack F\\\\right\\\\rbrack \\\\widetilde{ \\\\cap }\\\\left\\\\lbrack {P}^{M}\\\\right\\\\rbrack = {\\\\psi }_{cy}^{2}\\\\left( \\\\left\\\\lbrack F\\\\right\\\\rbrack \\\\right) \\\\widetilde{ \\\\cap }{\\\\psi }_{cy}^{2}\\\\left( \\\\left\\\\lbrack {P}^{M}\\\\right\\\\rbrack \\\\right) = {2}^{c}{\\\\psi }_{cy}^{2}\\\\left( \\\\left\\\\lbrack F\\\\right\\\\rbrack \\\\right) \\\\widetilde{ \\\\cap }\\\\left\\\\lbrack {P}^{M}\\\\right\\\\rbrack . $$\\n\\nUsing the first property, we deduce that\\n\\n$$ {\\\\psi }_{cy}^{2}\\\\left( \\\\left\\\\lbrack F\\\\right\\\\rbrack \\\\right) \\\\widetilde{ \\\\cap }\\\\left\\\\lbrack {P}^{M}\\\\right\\\\rbrack = {2}^{\\\\dim \\\\left( Q\\\\right) - c}\\\\left\\\\lbrack F\\\\right\\\\rbrack \\\\widetilde{ \\\\cap }\\\\left\\\\lbrack {P}^{M}\\\\right\\\\rbrack = {2}^{\\\\dim \\\\left( R\\\\right) }\\\\chi \\\\left( {F{ \\\\otimes }_{R}M}\\\\right) . $$\\n\\nThe next part of the proof is very similar to the proof of Theorem 4.6 given above. In detail, we have\\n\\n$$ {\\\\psi }_{cy}^{2}\\\\left( \\\\left\\\\lbrack F\\\\right\\\\rbrack \\\\right) \\\\widetilde{ \\\\cap }\\\\left\\\\lbrack {P}^{M}\\\\right\\\\rbrack = \\\\left\\\\lbrack {{S}^{2}\\\\left( F\\\\right) }\\\\right\\\\rbrack \\\\widetilde{ \\\\cap }\\\\left\\\\lbrack {P}^{M}\\\\right\\\\rbrack - \\\\left\\\\lbrack {{\\\\Lambda }^",
        "id": "college_math_152229"
    },
    {
        "informal_statement": "Theorem 3.7 Let \\( B\\\\left( t\\\\right) \\) be Brownian Motion. Then\\n\\n1. \\( B\\\\left( t\\\\right) \\) is a martingale.\\n\\n2. \\( B{\\\\left( t\\\\right) }^{2} - t \\) is a martingale.\\n\\n3. For any \\( u,{e}^{{uB}\\\\left( t\\\\right) - \\\\frac{{u}^{2}}{2}t} \\) is a martingale.",
        "informal_proof": "Proof: The key idea in establishing the martingale property is that for any function \\( g \\), the conditional expectation of \\( g\\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) \\) given \\( {\\\\mathcal{F}}_{t} \\) equals to the unconditional one,\\n\\n\\[ \\n\\\\mathrm{E}\\\\left( {g\\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) \\\\mid {\\\\mathcal{F}}_{t}}\\\\right) = \\\\mathrm{E}\\\\left( {g\\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) }\\\\right) , \\\\tag{3.10} \\n\\]\\n\\ndue to independence of \\( B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) \\) and \\( {\\\\mathcal{F}}_{t} \\) . The latter expectation is just \\( \\\\mathrm{E}g\\\\left( X\\\\right) \\), where \\( X \\) Normal \\( N\\\\left( {0, s}\\\\right) \\) random variable.\\n\\n1. By definition, \\( B\\\\left( t\\\\right) \\\\sim N\\\\left( {0, t}\\\\right) \\), so that \\( B\\\\left( t\\\\right) \\) is integrable with \\( \\\\mathrm{E}\\\\left( {B\\\\left( t\\\\right) }\\\\right) = 0 \\) .\\n\\n\\[ \\n\\\\mathrm{E}\\\\left( {B\\\\left( {t + s}\\\\right) |{\\\\mathcal{F}}_{t}}\\\\right) \\\\; = \\\\;\\\\mathrm{E}\\\\left( {B\\\\left( t\\\\right) + \\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) |{\\\\mathcal{F}}_{t}}\\\\right) \\n\\]\\n\\n\\[ \\n= \\\\mathrm{E}\\\\left( {B\\\\left( t\\\\right) |{\\\\mathcal{F}}_{t}}\\\\right) + \\\\mathrm{E}\\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) |{\\\\mathcal{F}}_{t}}\\\\right) \\n\\]\\n\\n\\[ \\n= B\\\\left( t\\\\right) + \\\\mathrm{E}\\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) = B\\\\left( t\\\\right) .\\n\\]\\n\\n2. By definition, \\( \\\\mathrm{E}\\\\left( {{B}^{2}\\\\left( t\\\\right) }\\\\right) = t < \\\\infty \\), therefore \\( {B}^{2}\\\\left( t\\\\right) \\) is integrable. Since\\n\\n\\[ \\n{B}^{2}\\\\left( {t + s}\\\\right) = {\\\\left( B\\\\left( t\\\\right) + B\\\\left( t + s\\\\right) - B\\\\left( t\\\\right) \\\\right) }^{2} \\n\\]\\n\\n\\[ \\n= {B}^{2}\\\\left( t\\\\right) + {2B}\\\\left( t\\\\right) \\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) + {\\\\left( B\\\\left( t + s\\\\right) - B\\\\left( t\\\\right) \\\\right) }^{2}, \\n\\]\\n\\n\\[ \\n\\\\mathrm{E}\\\\left( {{B}^{2}\\\\left( {t + s}\\\\right) \\\\mid {\\\\mathcal{F}}_{t}}\\\\right) \\n\\]\\n\\n\\[ \\n= {B}^{2}\\\\left( t\\\\right) + 2\\\\mathrm{E}\\\\left( {B\\\\left( t\\\\right) \\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) |{\\\\mathcal{F}}_{t}}\\\\right) + \\\\mathrm{E}\\\\left( {{\\\\left( B\\\\left( t + s\\\\right) - B\\\\left( t\\\\right) \\\\right) }^{2}|{\\\\mathcal{F}}_{t}}\\\\right) \\n\\]\\n\\n\\[ \\n= {B}^{2}\\\\left( t\\\\right) + s \\n\\]\\n\\nwhere we used that \\( B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) \\) is independent of \\( {\\\\mathcal{F}}_{t} \\) and has mean 0, and (3.10) with \\( g\\\\left( x\\\\right) = {x}^{2} \\) . Subtracting \\( \\\\left( {t + s}\\\\right) \\) from both sides gives the martingale property of \\( {B}^{2}\\\\left( t\\\\right) - t \\) .\\n\\n3. Consider the moment generating function of \\( B\\\\left( t\\\\right) \\), \\n\\n\\[ \\n\\\\mathrm{E}\\\\left( {e}^{{uB}\\\\left( t\\\\right) }\\\\right) = {e}^{t{u}^{2}/2} < \\\\infty \\n\\]\\n\\nsince \\( B\\\\left( t\\\\right) \\) has the \\( N\\\\left( {0, t}\\\\right) \\) distribution. This implies integrablity of \\( {e}^{{uB}\\\\left( t\\\\right) - t{u}^{2}/2} \\), moreover \\n\\n\\[ \\n\\\\mathrm{E}\\\\left( {e}^{{uB}\\\\left( t\\\\right) - t{u}^{2}/2}\\\\right) = 1 \\n\\]\\n\\nThe martingale property is established by using (3.10) with \\( g\\\\left( x\\\\right) = {e}^{ux} \\). \\n\\n\\[ \\n\\\\mathrm{E}\\\\left( {{e}^{{uB}\\\\left( {t + s}\\\\right) }|{\\\\mathcal{F}}_{t}}\\\\right) = \\\\mathrm{E}\\\\left( {{e}^{{uB}\\\\left( t\\\\right) + u\\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) }|{\\\\mathcal{F}}_{t}}\\\\right) \\n\\]\\n\\n\\[ \\n= {e}^{{uB}\\\\left( t\\\\right) }\\\\mathrm{E}\\\\left( {{e}^{u\\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) } \\\\mid {\\\\mathcal{F}}_{t}}\\\\right) \\\\left( {\\\\text{ since }B\\\\left( t\\\\right) \\\\text{ is }{\\\\mathcal{F}}_{t}\\\\text{-measurable }}\\\\right) \\n\\]\\n\\n\\[ \\n= {e}^{{uB}\\\\left( t\\\\right) }\\\\mathrm{E}\\\\left( {e}^{u\\\\left( {B\\\\left( {t + s}\\\\right) - B\\\\left( t\\\\right) }\\\\right) }\\\\right) \\\\left( {\\\\text{ since increment is independent of }{\\\\mathcal{F}}_{t}}\\\\right) \\n\\]\\n\\n\\[ \\n= {e}^{\\\\frac{{u}^{2}}{2}s}{e}^{{uB}\\\\left( t\\\\right) } \\n\\]\\n\\nThe martingale property of \\( {e}",
        "id": "college_math_229337"
    },
    {
        "informal_statement": "Proposition 3.127. (a) The equivalence relation\\n\\n$$ {d}_{\\widehat{w}} \\approx \\widetilde{\\omega } $$\\n\\nholds with some numerical constants.\\n\\n(b) If $B \\subset {B}^{\\prime }$, then\\n\\n$$ w\\\\left( {B,{B}^{\\prime }}\\\\right) \\approx \\widetilde{\\omega }\\\\left( {B,{B}^{\\prime }}\\\\right) $$\\n\\nwith numerical constants of equivalence.",
        "informal_proof": "Proof. (a) Show that for some constant $C$ ,\\n\\n$$ {d}_{\\widehat{\\omega }} \\leq C\\widetilde{\\omega } \\\\tag{3.141} $$\\n\\nFirst, let open balls $B \\subset {B}^{\\prime }$ have a common center. Define an integer $s$ by\\n\\n$$ {2}^{s}{r}_{B} < {r}_{{B}^{\\prime }} \\leq {2}^{s + 1}{r}_{B} $$\\n\\nand let ${\\\\left\\{ {B}_{i}\\\\right\\} }_{0 \\leq i \\leq s + 1}$ be the family of balls such that ${B}_{i} \\\\mathrel{\\\\text{:=}} {2}^{i}B$ for $i \\leq s$ and ${B}_{s + 1} \\\\mathrel{\\\\text{:=}} {B}^{\\prime }$ .\\n\\nBy the definition of ${d}_{\\widehat{\\omega }}$ ,\\n\\n$$ {d}_{\\widehat{\\omega }}\\\\left( {B,{B}^{\\prime }}\\\\right) \\leq \\\\mathop{\\\\sum }\\\\limits_{{i = 1}}^{s}\\\\widehat{\\\\omega }\\\\left( \\\\left\\\\lbrack {{B}_{i},{B}_{i + 1}}\\\\right\\\\rbrack \\\\right) = \\\\mathop{\\\\sum }\\\\limits_{{i = 0}}^{{s - 1}}\\\\frac{\\\\omega \\\\left( {{2}^{i + 1}{r}_{B}}\\\\right) }{{2}^{i}{r}_{B}} + \\\\frac{\\\\omega \\\\left( {r}_{{B}^{\\prime }}\\\\right) }{{2}^{s}{r}_{B}}. $$\\n\\nSince $\\\\omega \\\\left( t\\\\right) /{t}^{2}$ is nonincreasing, we have\\n\\n$$ \\\\frac{\\\\omega \\\\left( {a + b}\\\\right) }{4b} \\leq {\\\\int }_{a}^{a + b}\\\\frac{\\\\omega \\\\left( t\\\\right) }{{t}^{2}}{dt}\\\\;\\\\text{ for }\\\\;0 < a \\leq b. $$\\n\\nApplying this to the $i$ -th term of the previous inequality with $a = b \\\\mathrel{\\\\text{:=}} {2}^{i}{r}_{B}$ and summing on $i$, we get\\n\\n$$ {d}_{\\widehat{\\omega }}\\\\left( {B,{B}^{\\prime }}\\\\right) \\leq 4{\\\\int }_{{r}_{B}}^{{2}^{s}{r}_{B}}\\\\frac{\\\\omega \\\\left( t\\\\right) }{{t}^{2}}{dt} + 4{\\\\int }_{{2}^{s}{r}_{B}}^{{r}_{{B}^{\\prime }}}\\\\frac{\\\\omega \\\\left( t\\\\right) }{{t}^{2}}{dt} $$\\n\\n$$ = 4{\\\\int }_{{r}_{B}}^{{r}_{{B}^{\\prime }}}\\\\frac{\\\\omega \\\\left( t\\\\right) }{{t}^{2}}{dt} \\leq 4\\\\widetilde{\\\\omega }\\\\left( {B,{B}^{\\prime }}\\\\right) . \\\\tag{3.142} $$\\n\\nThis proves (3.141) for balls with a common center.\\n\\nNow let $B \\neq {B}^{\\prime }$ be arbitrary balls in $\\\\mathcal{B}\\\\left( \\\\mathcal{M}\\\\right)$ . Consider the open balls\\n\\n$$ \\\\widehat{B} \\\\mathrel{\\\\text{:=}} {B}_{M}\\\\left( {c}_{B}\\\\right) \\\\;\\\\text{ and }\\\\;{\\\\widehat{B}}^{\\\\prime } \\\\mathrel{\\\\text{:=}} {B}_{M}\\\\left( {c}_{{B}^{\\prime }}\\\\right) ; $$\\n\\nrecall that $M = M\\\\left( {B,{B}^{\\prime }}\\\\right) \\\\mathrel{\\\\text{:=}} {r}_{B} + {r}_{{B}^{\\prime }} + d\\\\left( {{c}_{B},{c}_{{B}^{\\prime }}}\\\\right)$ . By the triangle inequality\\n\\n$$ {d}_{\\widehat{\\omega }}\\\\left( {B,{B}^{\\prime }}\\\\right) \\leq {d}_{\\widehat{\\omega }}\\\\left( {B,\\\\widehat{B}}\\\\right) + {d}_{\\widehat{\\omega }}\\\\left( {{B}^{\\prime },{\\\\widehat{B}}^{\\\\prime }}\\\\right) + {d}_{\\widehat{\\omega }}\\\\left( {\\\\widehat{B},2\\\\widehat{B}}\\\\right) + {d}_{\\widehat{\\omega }}\\\\left( {2\\\\widehat{B},{\\\\widehat{B}}^{\\\\prime }}\\\\right) . \\\\tag{3.143} $$\\n\\nThe first two summands are estimated by (3.142) and their contribution into the final inequality (3.141) is\\n\\n$$ 4\\\\left( {{\\\\int }_{{r}_{B}}^{M + {r}_{B}} + {\\\\int }_{{r}_{{B}^{\\prime }}}^{M + {r}_{{B}^{\\prime }}}}\\\\right) \\\\frac{\\\\omega \\\\left( t\\\\right) }{{t}^{2}}{dt} \\leq 8{\\\\int }_{{r}_{B}}^{2M}\\\\frac{\\\\omega \\\\left( t\\\\right) }{{t}^{2}}{dt} $$\\n\\nin the last inequality we assume for definiteness that ${r}_{B} \\leq {r}_{{B}^{\\prime }}$ . Since $M \\geq 2{r}_{B}$ and $\\\\omega \\\\left( t\\\\right) /{t}^{2}$ is nonincreasing,\\n\\n$$ {\\\\int }_{M}^{2M}\\\\frac{\\\\omega \\\\left( t\\\\right) }{{t}^{2}}{dt} \\leq 2{\\\\int }_{\\\\frac{M}{2}}^{M}\\\\frac{\\\\omega \\\\left( t\\\\right) }{{t}^{2}}{dt} $$\\n\\nthe right-hand side of the previous inequality is at most\\n\\n$$ 8\\\\left( {{\\\\int }_{{r}_{B}}^{M} + 2{\\\\int }_{\\\\frac{M}{2}}^{M}}\\\\right) \\\\frac{\\\\omega \\\\left( t\\\\right) }{{t}^{2}}{dt} \\leq {24}\\\\widetilde{\\\\omega }\\\\left( {B,\\\\widehat{B}}\\\\right) . $$\\n\\nIt remains to obtain a similar bound for the sum of the final two terms in (3.143). Due to the definitions of the balls involved and the metric ${d}_{\\widehat{\\omega }}$ this sum is at most\\n\\n$$ \\\\widehat{\\\\omega }\\\\left( \\\\left\\\\lbrack {\\\\widehat{B},2\\\\widehat{B}}\\\\right\\\\rb",
        "id": "college_math_74551"
    },
    {
        "informal_statement": "Proposition 6.20. Suppose \\( X \\) satisfies global Poincaré duality and admits a \\( T \\) - equivariant resolution of singularities \\( f : Z \\rightarrow X \\) such that \\( {Z}^{T} \\) has trivial odd cohomology. Then \\( X \\) has trivial odd cohomology.",
        "informal_proof": "Proof. By the homology decomposition formula (4.3), the odd homology of \\( Z \\) is trivial since the odd homology of \\( {Z}^{T} \\) is. Therefore \\( I{H}^{ * }\\left( X\\right) \\) is trivial in odd degrees. But \\( I{H}^{ * }\\left( X\\right) = {H}^{ * }\\left( X\\right) \\), by the above remark, so we get the result.",
        "id": "college_math_263496"
    },
    {
        "informal_statement": "Proposition 3.7. With notation as above, let \\( {T}_{i}\\left( \\cdot \\right), i = 1,2 \\) be stopping times such that \\( {T}_{i}\\left( {X}_{i}\\right), i = 1,2 \\) are \\( \\mathbb{P} \\) -almost surely finite. Assume \\( {X}_{1}\\left( 0\\right) = {Y}_{1}\\left( 0\\right) \\) and \\( {X}_{2}\\left( 0\\right) = \\) \\( {Y}_{2}\\left( 0\\right) \\) . Set\\n\\n\\[ \\n{I}_{X} \\mathrel{\\text{:=}} \\left\\{ {{\\left\\{ {X}_{1}\\left( n\\right) \\right\\} }_{n = 0}^{{T}_{1}\\left( {X}_{1}\\right) }\\bigcap {\\left\\{ {X}_{2}\\left( n\\right) \\right\\} }_{n = 0}^{{T}_{2}\\left( {X}_{2}\\right) } = \\varnothing }\\right\\} \\n\\]\\n\\nand\\n\\n\\[ \\n{I}_{Y} \\mathrel{\\text{:=}} \\left\\{ {{\\left\\{ {Y}_{1}\\left( n\\right) \\right\\} }_{n = 0}^{{T}_{1}\\left( {Y}_{1}\\right) }\\bigcap {\\left\\{ {Y}_{2}\\left( n\\right) \\right\\} }_{n = 0}^{{T}_{2}\\left( {Y}_{2}\\right) } = \\varnothing }\\right\\} .\\n\\]\\n\\nThen, for any nearest neighbor deterministic paths \\( {\\left\\{ {\\lambda }_{i}\\left( n\\right) \\right\\} }_{n \\geq 0}, i = 1,2 \\) ,\\n\\n\\[ \\n\\mathbf{P}\\left( {{Y}_{i}\\left( n\\right) = {\\lambda }_{i}\\left( n\\right) ,0 \\leq n \\leq {T}_{i}\\left( {Y}_{i}\\right), i = 1,2;{I}_{Y}}\\right) \\n\\]\\n\\n\\[ \\n= \\mathbb{P}\\left( {{X}_{i}\\left( n\\right) = {\\lambda }_{i}\\left( n\\right) ,0 \\leq n \\leq {T}_{i}\\left( {X}_{i}\\right), i = 1,2;{I}_{X}}\\right) . \\tag{3.27} \\n\\]",
        "informal_proof": "Proof. For every pair of non-intersecting paths \\( {\\left\\{ {\\lambda }_{i}\\left( n\\right) \\right\\} }_{n \\geq 0} \\), define three i.i.d. environments \\( {\\omega }^{\\left( 1\\right) },{\\omega }^{\\left( 2\\right) } \\) and \\( {\\omega }^{\\left( 3\\right) } \\) as follows: Let \\( \\{ J\\left( z\\right) {\\} }_{z \\in {\\lambda }_{1} \\cup {\\lambda }_{2}} \\) be a collection of i.i.d. variables, of marginal law \\( Q \\) . At the same time, let \\( {\\left\\{ {\\eta }^{j}\\left( z\\right) \\right\\} }_{z \\in {\\mathbb{Z}}^{d}}, j = 1,2,3 \\) be three independent i.i.d. environments, each \\( P \\) -distributed. Then define\\n\\n\\[ \\n{\\omega }^{\\left( 1\\right) }\\left( z\\right) = \\left\\{ \\begin{array}{ll} J\\left( z\\right) & \\text{ if }z \\in {\\lambda }^{\\left( 1\\right) } \\\\ {\\eta }^{\\left( 1\\right) }\\left( z\\right) & \\text{ otherwise,} \\end{array}\\right. \\n\\]\\n\\n\\[ \\n{\\omega }^{\\left( 2\\right) }\\left( z\\right) = \\left\\{ \\begin{array}{ll} J\\left( z\\right) & \\text{ if }z \\in {\\lambda }^{\\left( 2\\right) } \\\\ {\\eta }^{\\left( 2\\right) }\\left( z\\right) & \\text{ otherwise,} \\end{array}\\right. \\n\\]\\n\\nand\\n\\n\\[ \\n{\\omega }^{\\left( 3\\right) }\\left( z\\right) = \\left\\{ \\begin{array}{ll} J\\left( z\\right) & \\text{ if }z \\in {\\lambda }^{\\left( 1\\right) } \\cup {\\lambda }^{\\left( 2\\right) } \\\\ {\\eta }^{\\left( 3\\right) }\\left( z\\right) & \\text{ otherwise,} \\end{array}\\right. \\n\\]\\n\\nand let \\( {Y}_{1} \\) evolve in \\( {\\omega }^{\\left( 1\\right) } \\), let \\( {Y}_{2} \\) evolve in \\( {\\omega }^{\\left( 2\\right) } \\) and let \\( {X}_{1} \\) and \\( {X}_{2} \\) evolve in \\( {\\omega }^{\\left( 3\\right) } \\) . Then by construction,\\n\\n\\[ \\n{P}_{{\\omega }^{\\left( 1\\right) },{\\omega }^{\\left( 2\\right) }}\\left( {{Y}_{i}\\left( n\\right) = {\\lambda }_{i}\\left( n\\right) ,0 \\leq n \\leq {T}_{i}\\left( {Y}_{i}\\right) }\\right) = {P}_{{\\omega }^{\\left( 3\\right) }}\\left( {{X}_{i}\\left( n\\right) = {\\lambda }_{i}\\left( n\\right) ,0 \\leq n \\leq {T}_{i}\\left( {X}_{i}\\right) }\\right) .\\n\\]\\n\\nIntegrating and then summing we get (3.27).",
        "id": "college_math_203692"
    },
    {
        "informal_statement": "Theorem 3 ([19]). Let \\( H \\) be the operator defined on (26) and \\( \\alpha \\in \\mathbb{R} \\smallsetminus \\mathbb{Z} \\). There exist \\( {a}_{0} > 0 \\) such that for any \\( 0 < \\frac{d}{d} < {a}_{0} \\), we have\\n\\n\\[ \\n{\\sigma }_{d}\\left( H\\right) = \\varnothing \\text{.}\\n\\]\\n\\nThere exist \\( {a}_{1} > 0 \\), such that \\( \\frac{a}{d} > {a}_{1} \\), we have\\n\\n\\[ \\n{\\sigma }_{d}\\left( H\\right) \\neq \\varnothing \\text{.}\\n\\]",
        "informal_proof": "Proof. The proof follows the same steps as in the previous two subsections. The main difference is by introducing the magnetic field we get a new Bessel equation we obtain \\( \\frac{1}{P}{\\left( i\\frac{\\partial }{\\partial \\theta } + \\alpha \\right) }^{2}P \\) should be a constant \\( - {\\left( m - \\alpha \\right) }^{2} = - {v}^{2} \\) for \\( m \\in \\mathbb{Z} \\). Finally, we get the new equation for \\( R \\)\\n\\n\\[ \\n{R}^{\\prime \\prime }\\left( r\\right) + \\frac{1}{r}{R}^{\\prime }\\left( r\\right) + \\left\\lbrack {\\lambda - {k}_{z}^{2} - \\frac{{v}^{2}}{{r}^{2}}}\\right\\rbrack R\\left( r\\right) = 0. \\tag{30}\\n\\]\\n\\nWe notice that the equation (30) is a Bessel equation which by the introduction of the term \\( \\alpha \\) is different from equation (5). The solutions of (30) could be expressed in terms of Bessel functions. More explicit solutions could be given by considering boundary conditions.\\n\\nThe solution of the equation (30) is given by \\( R\\left( r\\right) = c{J}_{v}\\left( {\\beta r}\\right) \\), where \\( c \\in {\\mathbb{R}}^{ \\star } \\), \\( {\\beta }^{2} = \\lambda - {k}_{z}^{2} \\) and \\( {J}_{v} \\) is the Bessel function of first kind of order \\( v \\).\\n\\nWe assume that\\n\\n\\[ \\n{R}^{\\prime }\\left( a\\right) = 0 \\Leftrightarrow {J}_{v}\\left( {\\beta a}\\right) = 0\\n\\]\\n\\n\\[ \\n\\Leftrightarrow {a\\beta } = {x}_{v, n}^{\\prime } \\tag{31}\\n\\]\\n\\nwhere \\( {x}_{v, n}^{\\prime } \\) is the \\( n \\) -th positive zero of the Bessel function \\( {J}_{v}^{\\prime } \\).\\n\\nConsequently to equation (31), \\( {H}_{a}^{-, N} \\) has a sequence of eigenvalues given by\\n\\n\\[ \\n{\\lambda }_{j, v, n} = \\frac{{x}_{v, n}^{\\prime 2}}{{a}^{2}} + {k}_{z}^{2}\\n\\]\\n\\n\\[ \\n= \\frac{{x}_{v, n}^{\\prime 2}}{{a}^{2}} + {\\left( \\frac{\\left( {{2j} + 1}\\right) \\pi }{2d}\\right) }^{2}.\\n\\]\\n\\nAs we are interested in discrete eigenvalues which belong to \\( \\left\\lbrack {{\\left( \\frac{\\pi }{2d}\\right) }^{2},{\\left( \\frac{\\pi }{d}\\right) }^{2}}\\right. \\) ) only \\( {\\lambda }_{0, v, n} \\) intervenes.\\n\\nIf\\n\\n\\[ \\n{\\left( \\frac{\\pi }{d}\\right) }^{2} \\leq {\\lambda }_{0, v, n} \\tag{32}\\n\\]\\n\\nthen there \\( H \\) does not have a discrete spectrum. We recall that \\( {v}^{2} = {\\left( m - \\alpha \\right) }^{2} \\) and it is related to magnetic flux, also recall that \\( {x}_{v, n}^{\\prime } \\) are the positive zeros of the Bessel function \\( {J}_{n}^{\\prime } \\) and \\( \\forall v > 0,\\forall n \\in {\\mathbb{N}}^{ \\star };0 < {x}_{v, n}^{\\prime } < {x}_{v, n + 1}^{\\prime } \\) (see [2]). So, for any eigenvalue of \\( {H}_{a}^{-, N} \\),\\n\\n\\[ \\n\\frac{{x}_{v,1}^{\\prime 2}}{{a}^{2}} + {\\left( \\frac{\\pi }{2d}\\right) }^{2} < \\frac{{x}_{v, n}^{\\prime 2}}{{a}^{2}} + {\\left( \\frac{\\pi }{2d}\\right) }^{2} = {\\lambda }_{0, v, n}.\\n\\]\\n\\nAn immediate consequence of the last inequality is that to satisfy (32) it is sufficient to have\\n\\n\\[ \\n3{\\left( \\frac{\\pi }{2d}\\right) }^{2} < \\frac{{x}_{v,1}^{\\prime 2}}{{a}^{2}}\\n\\]\\n\\ntherefore\\n\\n\\[ \\n\\frac{\\sqrt{3}\\pi }{2d} < \\frac{{x}_{v,1}^{\\prime }}{a}\\n\\]\\n\\nthen\\n\\n\\[ \\n\\frac{a}{d} < \\frac{2{x}_{v,1}^{\\prime }}{\\sqrt{3}\\pi }\\n\\]",
        "id": "college_math_180990"
    },
    {
        "informal_statement": "Corollary 5.12. Let \\( E \\) be a vector lattice with a total set \\( \\Delta \\left( E\\right) \\) . Then any sufficient subset of \\( E \\) is also complete.",
        "informal_proof": "Indeed, if an element \\( z \\) is disjoint to a sufficient set \\( A \\), then for \\( f \\in \\Delta \\left( E\\right) \\) there is an element \\( a \\in A \\) with \\( f\\left( \\left| a\\right| \\right) > 0 \\) . From \\( a \\bot z \\) one has \\( f\\left( \\left| a\\right| \\right) \\land f\\left( \\left| z\\right| \\right) = f\\left( {\\left| a\\right| \\land \\left| z\\right| }\\right) = 0 \\) , which implies \\( f\\left( \\left| z\\right| \\right) = 0 \\) . Since \\( \\Delta \\left( E\\right) \\) is total, \\( z = 0 \\) follows.",
        "id": "college_math_244200"
    },
    {
        "informal_statement": "Proposition 17. Let \\( \\eta \\in \\mathbb{Z} \\) . Define \\( \\pi = u\\left( \\eta \\right) - u\\left( {\\eta + 1}\\right) \\) and \\( e = \\left( {\\eta + 1}\\right) u\\left( \\eta \\right) - \\) \\( {\\eta u}\\left( {\\eta + 1}\\right) \\) . Then, for all \\( \\chi \\in \\mathbb{Z} \\) ,\\n\\n\\[ u\\left( \\chi \\right) \\geq e - {\\pi \\chi } \\tag{3.68} \\]\\n\\nMoreover\\n\\n\\[ u\\left( \\eta \\right) = e - \\pi \\cdot \\eta \\tag{3.69} \\]",
        "informal_proof": "Proof. Consider the case where \\( \\chi \\geq \\eta \\) . Then\\n\\n\\[ u\\left( \\chi \\right) - u\\left( \\eta \\right) = \\mathop{\\sum }\\limits_{{k = 0}}^{{\\chi - \\eta - 1}}\\left\\lbrack {u\\left( {\\chi - k}\\right) - u\\left( {\\chi - k - 1}\\right) }\\right\\rbrack .\\n\\nBy Proposition 16, each term in the sum is bounded below by \\( u\\left( {\\eta + 1}\\right) - u\\left( \\eta \\right) \\) . Hence \\( u\\left( \\chi \\right) - u\\left( \\eta \\right) \\geq \\left( {\\chi - \\eta }\\right) \\left( {u\\left( {\\eta + 1}\\right) - u\\left( \\eta \\right) }\\right) \\) .\\n\\nThe inequality (3.68) follows. The case where \\( \\chi \\leq \\eta \\) is similar. Finally,(3.69) is obtained by straightforward computation of \\( e - {\\pi \\eta } \\) .",
        "id": "college_math_181963"
    },
    {
        "informal_statement": "Theorem 5.2 (Cauchy-Schwarz inequality). For any random variables $X$ and $Y$ ,\\n\\n$${\\\\left( \\\\mathbb{E}\\\\left\\\\lbrack XY\\\\right\\\\rbrack \\\\right) }^{2} \\\\leq \\\\mathbb{E}\\\\left\\\\lbrack {X}^{2}\\\\right\\\\rbrack \\\\mathbb{E}\\\\left\\\\lbrack {Y}^{2}\\\\right\\\\rbrack \\\\tag{5.16}$$\\n\\nThe following proof can be skipped if you are reading the book the first time.",
        "informal_proof": "Proof. Let $t \\\\in \\\\mathbb{R}$ be a constant. Consider $\\\\mathbb{E}\\\\left\\\\lbrack {\\\\left( X + tY\\\\right) }^{2}\\\\right\\\\rbrack = \\\\mathbb{E}\\\\left\\\\lbrack {{X}^{2} + {2tXY} + {t}^{2}{Y}^{2}}\\\\right\\\\rbrack$ . Since $\\\\mathbb{E}\\\\left\\\\lbrack {\\\\left( X + tY\\\\right) }^{2}\\\\right\\\\rbrack \\\\geq 0$ for any $t$, it follows that\\n\\n$$\\\\mathbb{E}\\\\left\\\\lbrack {{X}^{2} + {2tXY} + {t}^{2}{Y}^{2}}\\\\right\\\\rbrack \\\\geq 0.$$\\n\\nExpanding the left-hand side yields ${t}^{2}\\\\mathbb{E}\\\\left\\\\lbrack {Y}^{2}\\\\right\\\\rbrack + {2t}\\\\mathbb{E}\\\\left\\\\lbrack {XY}\\\\right\\\\rbrack + \\\\mathbb{E}\\\\left\\\\lbrack {X}^{2}\\\\right\\\\rbrack \\\\geq 0$ . This is a quadratic equation in $t$, and we know that for any quadratic equation $a{t}^{2} + {bt} + c \\\\geq 0$ we must have ${b}^{2} - {4ac} \\\\leq 0$ . Therefore, in our case, we have that\\n\\n$${\\\\left( 2\\\\mathbb{E}\\\\left\\\\lbrack XY\\\\right\\\\rbrack \\\\right) }^{2} - 4\\\\mathbb{E}\\\\left\\\\lbrack {Y}^{2}\\\\right\\\\rbrack \\\\mathbb{E}\\\\left\\\\lbrack {X}^{2}\\\\right\\\\rbrack \\\\leq 0,$$\\n\\nwhich means ${\\\\left( \\\\mathbb{E}\\\\left\\\\lbrack XY\\\\right\\\\rbrack \\\\right) }^{2} \\\\leq \\\\mathbb{E}\\\\left\\\\lbrack {X}^{2}\\\\right\\\\rbrack \\\\mathbb{E}\\\\left\\\\lbrack {Y}^{2}\\\\right\\\\rbrack$ . The equality holds when $\\\\mathbb{E}\\\\left\\\\lbrack {\\\\left( X + tY\\\\right) }^{2}\\\\right\\\\rbrack = 0$ . In this case, $X = - {tY}$ for some $t$, i.e., the random variable $X$ is a scaled version of $Y$ so that the vector formed by the states of $X$ is parallel to that of $Y$ .\\n\\nEnd of the proof.",
        "id": "college_math_147829"
    },
    {
        "informal_statement": "Theorem 3.84 (Plancherel). The Fourier-Plancherel transform:\\n\\n\\\\[ \\n\\\\widehat{\\\\mathcal{F}} : {L}^{2}\\\\left( {{\\\\mathbb{R}}^{n},{dx}}\\\\right) \\\\rightarrow {L}^{2}\\\\left( {{\\\\mathbb{R}}^{n},{dx}}\\\\right) \\n\\\\]\\n\\nis a bijective and isometric linear operator.",
        "informal_proof": "Proof. The proof was given immediately before Definition 3.83.",
        "id": "college_math_366623"
    },
    {
        "informal_statement": "Theorem 5.3.13 In a lattice if $a \\leq b \\leq c$, show that\\n\\n(i) $a \\oplus b = b \\star c$\\n\\n(ii) $\\\\left( {a \\star b}\\\\right) \\oplus \\\\left( {b \\star c}\\\\right) = \\\\left( {a \\oplus b}\\\\right) \\star \\\\left( {a \\oplus c}\\\\right) = b$ .",
        "informal_proof": "Proof.\\n\\nLet $a \\leq b \\leq c$ .\\n\\n$$\\na \\leq b \\Rightarrow a \\oplus b = b, a \\star b = a.\\n$$\\n\\n$$\\nb \\leq c \\Rightarrow b \\oplus d = c, b \\star c = b.\\n$$\\n\\n$a \\leq c \\Rightarrow a \\oplus c = c, a \\star c = a.$\\n\\nTherefore, $\\\\;a \\oplus b = b = b \\star c$, which is (i).\\n\\nNow, $\\\\;\\\\left( {a \\star b}\\\\right) \\oplus \\\\left( {b \\star c}\\\\right) = a \\oplus b = b$\\n\\n$\\\\left( {a \\oplus b}\\\\right) \\star \\\\left( {a \\oplus c}\\\\right) = b \\star c = b$, which is (ii).",
        "id": "college_math_58532"
    },
    {
        "informal_statement": "Proposition 12.3 Let \\( \\mathcal{L} \\) and \\( \\mathcal{F} \\) be two \\( {\\mathcal{O}}_{X} \\) -modules on \\( X \\) . Then there is a functorial canonical homomorphism\\n\\n\\[ \\n\\psi : {\\mathcal{{Hom}}}_{{\\mathcal{O}}_{X}}\\left( {\\mathcal{L},{\\mathcal{O}}_{X}}\\right) { \\otimes }_{{\\mathcal{O}}_{X}}\\mathcal{F} \\rightarrow {\\mathcal{{Hom}}}_{{\\mathcal{O}}_{X}}\\left( {\\mathcal{L},\\mathcal{F}}\\right) .\\n\\]\\n\\nIf \\( \\mathcal{L} \\) or \\( \\mathcal{F} \\) is locally free of finite rank, then this is an isomorphism. In particular, if \\( \\mathcal{L} \\) is locally free of rank 1, then\\n\\n\\[ \\n{\\mathcal{{Hom}}}_{{\\mathcal{O}}_{X}}\\left( {\\mathcal{L},\\mathcal{L}}\\right) \\cong {\\mathcal{O}}_{X}\\n\\]\\n\\nso\\n\\n\\[ \\n{\\mathcal{L}}^{ \\vee }{ \\otimes }_{{\\mathcal{O}}_{X}}\\mathcal{L} \\cong {\\mathcal{O}}_{X}\\n\\]",
        "informal_proof": "Proof We define \\( \\psi \\) on the level of presheaves, then pass to the associated sheaves. This process being functorial, the homomorphism of presheaves gives rise to a homomorphism of sheaves. So let \\( U \\subset X \\) be open, put \\( A = {\\mathcal{O}}_{X}\\left( U\\right) \\) and define\\n\\n\\[ \\n{\\psi }_{U} : {\\operatorname{Hom}}_{A}\\left( {\\mathcal{L}\\left( U\\right), A}\\right) { \\otimes }_{A}\\mathcal{F}\\left( U\\right) \\rightarrow {\\operatorname{Hom}}_{A}\\left( {\\mathcal{L}\\left( U\\right) ,\\mathcal{F}\\left( U\\right) }\\right)\\n\\]\\n\\nas being induced from\\n\\n\\[ \\nu \\otimes f \\mapsto \\varphi \\in {\\operatorname{Hom}}_{A}\\left( {\\mathcal{L}\\left( U\\right) ,\\mathcal{F}\\left( U\\right) }\\right) \\;\\text{ where }\\varphi \\left( \\ell \\right) = u\\left( \\ell \\right) f.\\n\\]\\n\\nIf \\( \\mathcal{L} \\) is locally free this is an isomorphism, indeed we may check this locally and thus assume \\( \\mathcal{L}\\left( U\\right) = {A}^{r} \\), in which case we have both sides equal to \\( \\mathcal{F}{\\left( U\\right) }^{r} \\) and\\n\\nreadily verify that \\( {\\psi }_{U} \\) is an isomorphism. This also proves the last assertion by taking \\( \\mathcal{F} = \\mathcal{L} \\), as \\( r = 1 \\) in that case.",
        "id": "college_math_361957"
    },
    {
        "informal_statement": "Example 6.3.4 (Toeplitz Operators). Choose any nonconstant real-valued continuous function $w\\\\left( {\\\\mathrm{e}}^{it}\\\\right) \\\\in {L}^{2}\\\\left( {-\\\\pi ,\\\\pi }\\\\right)$ . For any $f \\\\in {H}^{2}\\\\left( {\\\\partial \\\\mathbb{D}}\\\\right)$, the Toeplitz operator multiplies by $w$ and projects:\\n\\n$$ \\n{T}_{w}\\\\left( f\\\\right) \\\\left( {\\\\mathrm{e}}^{it}\\\\right) = P\\\\left\\\\lbrack {w\\\\left( {\\\\mathrm{e}}^{it}\\\\right) \\\\cdot f\\\\left( {\\\\mathrm{e}}^{it}\\\\right) }\\\\right\\\\rbrack , \\n$$ \\n\\nwhere $P$ is the projection \\n\\n$$ \\nP\\\\left\\\\lbrack {\\\\mathop{\\\\sum }\\\\limits_{{n = - \\\\infty }}^{\\\\infty }{a}_{n}{\\\\mathrm{e}}^{int}}\\\\right\\\\rbrack = \\\\mathop{\\\\sum }\\\\limits_{{n = 0}}^{\\\\infty }{a}_{n}{\\\\mathrm{e}}^{int} \\n$$ \\n\\nfrom ${L}^{2}\\\\left( {-\\\\pi ,\\\\pi }\\\\right)$ onto ${H}^{2}\\\\left( {\\\\partial \\\\mathbb{D}}\\\\right)$ . Linearity is obvious, and ${T}_{w}$ is bounded because $\\\\begin{Vmatrix}{T}_{w}\\\\end{Vmatrix} =$ $\\\\max w\\\\left( {\\\\mathrm{e}}^{it}\\\\right)$ . Furthermore, the Fourier transform maps ${T}_{w}$ onto the class of matrix",
        "informal_proof": "Toeplitz operators on ${\\\\ell }^{2}$ . In other words, the resulting infinite matrix has constant diagonals formed from the Fourier coefficients of $w$ . For such real-valued $w\\\\left( {\\\\mathrm{e}}^{it}\\\\right)$, the operator ${T}_{w}$ is self-adjoint, its spectrum is the interval \\n\\n$$ \\n\\\\sigma \\\\left( {T}_{w}\\\\right) = \\\\left\\\\lbrack {\\\\min w\\\\left( {\\\\mathrm{e}}^{it}\\\\right) ,\\\\max w\\\\left( {\\\\mathrm{e}}^{it}\\\\right) }\\\\right\\\\rbrack \\n$$ \\n\\n(see [191],[89]), and ${T}_{w}$ does not have any eigenvalues (see [159]). For example, let \\n\\n$$ \\nw\\\\left( {\\\\mathrm{e}}^{it}\\\\right) = 2\\\\cos t = 1{\\\\mathrm{e}}^{it} + 1{\\\\mathrm{e}}^{-{it}}. \\n$$ \\n\\nThen \\n\\n$$ \\n{T}_{w}\\\\left( {\\\\mathop{\\\\sum }\\\\limits_{{n = 0}}^{\\\\infty }{a}_{n}{\\\\mathrm{e}}^{int}}\\\\right) = {a}_{1} + \\\\mathop{\\\\sum }\\\\limits_{{n = 1}}^{\\\\infty }\\\\left( {{a}_{n - 1} + {a}_{n + 1}}\\\\right) {\\\\mathrm{e}}^{int}. \\n$$ \\n\\nHence ${T}_{2\\\\cos t}$ is self-adjoint with spectrum $\\\\sigma \\\\left( {T}_{w}\\\\right) = \\\\left\\\\lbrack {-2,2}\\\\right\\\\rbrack$ . As an operator on ${\\\\ell }^{2}$, the Fourier series coefficients determine ${T}_{2\\\\cos t}$ as multiplication by the matrix \\n\\n$$ \\n{T}_{2\\\\cos t} = \\\\left\\\\lbrack \\\\begin{matrix} 0 & 1 & 0 & 0 & \\\\cdots \\\\\\\\ 1 & 0 & 1 & 0 & \\\\cdots \\\\\\\\ 0 & 1 & 0 & 1 & \\\\cdots \\\\\\\\ \\\\vdots & \\\\vdots & \\\\vdots & \\\\ddots & \\\\end{matrix}\\\\right\\\\rbrack \\n$$ \\n\\nand it has no eigenvalues. In 1962, Marvin Rosenblum used advanced complex analysis to describe the explicit spectral theory for this matrix in [160]. He there determined this operator as unitarily equivalent 29 to multiplication by $x$ on an ${L}^{2}\\\\left( {-2,2}\\\\right)$ function space. The norms and inner products in this space are found by integration over $\\\\left( {-2,2}\\\\right)$ against the weight (the density function) $\\\\frac{\\\\sqrt{4 - {x}^{2}}}{2\\\\pi }$ .",
        "id": "college_math_131229"
    },
    {
        "informal_statement": "Lemma 6. Let \\( G \\) be a host-less \\( l \\) -bounded graph and \\( p \\subset {V}^{G} \\) . Then:\\n\\n\\[ I{O}^{{h}_{l}\\left( G\\right) }\\left( p\\right) = {2l}\\left| p\\right| - 2\\left| {E}^{G\\left( p\\right) }\\right| . \\]",
        "informal_proof": "Hence, the total number of inputs and outputs of a parcel \\( p \\) in \\( {h}_{l}\\left( G\\right) \\) depends on \\( l \\) and \\( G\\left( p\\right) \\) but does not depend otherwise on \\( G \\) .",
        "id": "college_math_246375"
    },
    {
        "informal_statement": "Problem 2. It is known from the newspapers that the cosmonaut Leonov, going for a walk in space, threw the lens cap of his movie camera toward the earth. Where did it go?",
        "informal_proof": "Solution. This is a problem involving the influence of a small perturbation in the initial conditions on the solution. The equation of motion, by the law of universal gravitation, can be written in the form $\\\\ddot{\\\\mathbf{r}} = - \\\\gamma \\\\mathbf{r}/{r}^{3}$ . The motion of both the cosmonaut and the lens cap occurs in the plane of the circular orbit, so that we may assume $\\\\mathbf{r} \\\\in {\\\\mathbf{R}}^{2}$ . Let us write the equation of motion in polar coordinates. To do this we introduce the unit vector ${\\\\mathbf{e}}_{r} = \\\\mathbf{r}/r$ and the unit vector ${\\\\mathbf{e}}_{\\\\varphi }$ perpendicular to it and directed forward along the circular orbit. It is clear that ${\\\\dot{\\\\mathbf{e}}}_{r} = \\\\dot{\\\\varphi }{\\\\mathbf{e}}_{\\\\varphi }$ and ${\\\\dot{\\\\mathbf{e}}}_{\\\\varphi } = - \\\\dot{\\\\varphi }{\\\\mathbf{e}}_{r}$ . Differentiating the quantity $\\\\mathbf{r} = r{\\\\mathbf{e}}_{r}$, we find $\\\\mathbf{r} = \\\\dot{r}{\\\\mathbf{e}}_{r} + r\\\\dot{\\\\varphi }{\\\\mathbf{e}}_{\\\\varphi }$ , $\\\\ddot{r} = \\\\ddot{r}{\\\\mathbf{e}}_{r} + 2\\\\dot{r}\\\\dot{\\\\varphi }{\\\\mathbf{e}}_{\\\\varphi } + r\\\\ddot{\\\\varphi }{\\\\mathbf{e}}_{\\\\varphi } - r{\\\\dot{\\\\varphi }}^{2}{\\\\mathbf{e}}_{r}$ . Consequently Newton’s equation in polar coordinates assumes the form of a system of two second-order equations\\n\\n$$ \\n\\\\ddot{r} - r{\\\\dot{\\\\varphi }}^{2} = - \\\\gamma {r}^{2},\\\\;r\\\\ddot{\\\\varphi } + 2\\\\dot{r}\\\\dot{\\\\varphi } = 0.\\n$$ \\n\\nWe take as a unit of length the radius of the circular orbit of the space station $\\\\left( { \\\\approx {6400}\\\\mathrm{\\\\;{km}}}\\\\right)$ . We choose the unit of time so that the angular velocity of the motion in the orbit is 1 . Then the motion over the orbit is described by the equations $r = 1$ , $\\\\varphi = t$, and so $\\\\gamma = 1$ . The initial conditions for the space station (and the cosmonaut) are $r\\\\left( 0\\\\right) = 1,\\\\dot{r}\\\\left( 0\\\\right) = 0,\\\\varphi \\\\left( 0\\\\right) = 0,\\\\dot{\\\\varphi }\\\\left( 0\\\\right) = 1$ . The initial conditions for the lens cap differ only in that $\\\\dot{r}\\\\left( 0\\\\right) = - v$ is the velocity of the throw, i.e., the initial velocity of the lens cap relative to the cosmonaut. Assume that the velocity of the throw is 10 $\\\\mathrm{m}/\\\\mathrm{{sec}}$ . Then $v \\\\approx 1/{800}$ (since our unit of velocity is nearly the first cosmic velocity, i.e., about $8\\\\mathrm{\\\\;{km}}/\\\\mathrm{{sec}}$ ).\\n\\nThe quantity $1/{800}$ is small compared to 1, and therefore we must study the influence of a small deviation in the initial condition on the unperturbed solution $r = 1,\\\\varphi = t$ . By the theorem on differentiability with respect to the initial condition we seek a solution close to the unperturbed solution in the form $r = 1 + {r}_{1} + \\\\cdots$ , $\\\\varphi = t + {\\\\varphi }_{1} + \\\\cdots$, where the dots indicate infinitesimals of order ${v}^{2}$ . Substituting these expressions in Newton’s equation with $\\\\gamma = 1$ and rejecting infinitesimals of order ${v}^{2}$, we obtain the equations of variations\\n\\n$$ \\n{\\\\ddot{r}}_{1} = 3{r}_{1} + 2{\\\\dot{\\\\varphi }}_{1},\\\\;{\\\\ddot{\\\\varphi }}_{1} + 2{\\\\dot{r}}_{1} = 0.\\n$$ \\n\\nThe solution of the equations of variations with the initial conditions of the lens cap $\\\\left( {{r}_{1}\\\\left( 0\\\\right) = {\\\\varphi }_{1}\\\\left( 0\\\\right) = {\\\\dot{\\\\varphi }}_{1}\\\\left( 0\\\\right) = 0,{\\\\dot{r}}_{1}\\\\left( 0\\\\right) = - v}}\\\\right)$ can be easily found by observing\\n\\nthat ${\\\\dot{\\\\varphi }}_{1} + 2{r}_{1} \\\\equiv 0$, so that ${\\\\ddot{r}}_{1} = - {r}_{1}$ . This solution has the form ${r}_{1} = - v\\\\sin t$ , ${\\\\varphi }_{1} = {2v}\\\\left( {1 - \\\\cos t}\\\\right)$ . By the theorem on differentiability the true solution of Newton’s equations differs from the one we have found by infinitesimals of order ${v}^{2}$ (for $t$ not too large). Consequently the lens cap describes an ellipse relative to the cosmonaut (Fig. 78) with semiaxes $v$ and ${2v}$ . Our unit of length is the radius of the orbit, and $v \\\\approx 1/{800}$ . Thus the lengths",
        "id": "college_math_148820"
    },
    {
        "informal_statement": "Proposition 6.1. An approach frame is spatial if and only if each element is the meet of prime elements.",
        "informal_proof": "Proof. If the approach frame comes from an approach space, then we know that each $\\\\delta \\\\left( {\\\\cdot ,\\\\{ x\\\\} }\\\\right)$ is prime and since $f - f\\\\left( x\\\\right) \\\\leq \\\\delta \\\\left( {\\\\cdot ,\\\\{ x\\\\} }\\\\right)$ for all contractions, we find that\\n\\n$$ f = \\\\bigwedge {A}_{f\\\\left( x\\\\right) }\\\\delta \\\\left( {\\\\cdot ,\\\\{ x\\\\} }\\\\right) .\\n$$\\n\\nConversely, take $S$ the set of approach prime elements of $L$ and as approach structure take $L$ with the elements seen as functions. This set of functions has all the properties of a regular function frame and it is clear that this approach space gives the approach frame we started with. Indeed, if $a \\\\neq b \\\\in L$ then there must exist an approach prime element $e$ such that ${h}_{e}\\\\left( a\\\\right) \\\\neq ${h}_{e}\\\\left( b\\\\right)$, because if not, then\\n\\n$$ a = \\\\mathop{\\\\bigwedge }\\\\limits_{{e \\\\in S}}{A}_{{h}_{e}\\\\left( a\\\\right) }e = \\\\mathop{\\\\bigwedge }\\\\limits_{{e \\\\in S}}{A}_{{h}_{e}\\\\left( b\\\\right) }e = b.\\n$$\\n\\n$\\\\square$\\n\\nHence an approach frame is spatial if and only if the underlying frame is spatial.",
        "id": "college_math_139393"
    },
    {
        "informal_statement": "Proposition 2.2.1 Let (H1)-(H2) hold. For any $x \\\\in {\\\\mathbb{R}}^{n}$ and $u \\\\in \\\\mathcal{U}\\\\left\\\\lbrack {t, T}\\\\right\\\\rbrack$ , let $\\\\left( {{X}_{x}^{u},{Y}_{x}^{u},{Z}_{x}^{u}}\\\\right)$ be the adapted solution to the following (decoupled) linear forward-backward stochastic differential equation (FBSDE, for short) on $\\\\left\\\\lbrack {t, T}\\\\right\\\\rbrack$ :\\n\\n$$ \\n\\\\left\\\\{ \\\\begin{array}{l} d{\\\\mathring{X}}_{x}^{u}\\\\left( s\\\\right) = \\\\left( {A{\\\\mathring{X}}_{x}^{u} + {Bu}}\\\\right) {ds} + \\\\left( {C{\\\\mathring{X}}_{x}^{u} + {Du}}\\\\right) {dW}\\\\left( s\\\\right) , \\\\\\\\ d{\\\\mathring{Y}}_{x}^{u}\\\\left( s\\\\right) = - \\\\left( {{A}^{\\\\top }{\\\\mathring{Y}}_{x}^{u} + {C}^{\\\\top }{\\\\mathring{Z}}_{x}^{u} + Q{\\\\mathring{X}}_{x}^{u} + {S}^{\\\\top }u}\\\\right) {ds} + {\\\\mathring{Z}}_{x}^{u}{dW}\\\\left( s\\\\right) , \\\\\\\\ {\\\\mathring{X}}_{x}^{u}\\\\left( t\\\\right) = x,\\\\;{\\\\mathring{Y}}_{x}^{u}\\\\left( T\\\\right) = G{\\\\mathring{X}}_{x}^{u}\\\\left( T\\\\right) . \\\\end{array}\\\\right. \\\\tag{2.2.5} \\n$$ \\n\\nThen \\n\\n$$ \\n\\\\left\\\\lbrack {{M}_{2}\\\\left( t\\\\right) u}\\\\right\\\\rbrack \\\\left( s\\\\right) = B{\\\\left( s\\\\right) }^{\\\\top }{\\\\mathring{Y}}_{0}^{u}\\\\left( s\\\\right) + D{\\\\left( s\\\\right) }^{\\\\top }{\\\\mathring{Z}}_{0}^{u}\\\\left( s\\\\right) + S\\\\left( s\\\\right) {\\\\mathring{X}}_{0}^{u}\\\\left( s\\\\right) + R\\\\left( s\\\\right) u\\\\left( s\\\\right) , \\n$$ \\n\\n$$ \\n\\\\left\\\\lbrack {{M}_{1}\\\\left( t\\\\right) x}\\\\right\\\\rbrack \\\\left( s\\\\right) = B{\\\\left( s\\\\right) }^{\\\\top }{\\\\mathring{Y}}_{x}^{0}\\\\left( s\\\\right) + D{\\\\left( s\\\\right) }^{\\\\top }{\\\\mathring{Z}}_{x}^{0}\\\\left( s\\\\right) + S\\\\left( s\\\\right) {\\\\mathring{X}}_{x}^{0}\\\\left( s\\\\right) , \\n$$ \\n\\n$$ \\n{M}_{0}\\\\left( t\\\\right) x = \\\\mathbb{E}\\\\left\\\\lbrack {{\\\\overset{ \\\\circ }{Y}}_{x}^{0}\\\\left( t\\\\right) }\\\\right\\\\rbrack . \\n$$ \\n\\nMoreover, ${M}_{0}$ solves the Lyapunov equation \\n\\n$$ \\n\\\\begin{cases} {\\\\dot{M}}_{0}\\\\left( t\\\\right) + {M}_{0}\\\\left( t\\\\right) A\\\\left( t\\\\right) + A{\\\\left( t\\\\right) }^{\\\\top }{M}_{0}\\\\left( t\\\\right) & \\\\\\\\ \\\\; + C{\\\\left( t\\\\right) }^{\\\\top }{M}_{0}\\\\left( t\\\\right) C\\\\left( t\\\\right) + Q\\\\left( t\\\\right) & = 0,\\\\;t \\\\in \\\\left\\\\lbrack {0, T}\\\\right\\\\rbrack , \\\\\\\\ {M}_{0}\\\\left( T\\\\right) = G, & \\\\end{cases} \\\\tag{2.2.6} \\n$$ \\n\\nand admits the following representation: \\n\\n$$ \\n{M}_{0}\\\\left( t\\\\right) = \\\\mathbb{E}\\\\left\\\\{ {{\\\\left\\\\lbrack \\\\Phi \\\\left( T\\\\right) \\\\Phi {\\\\left( t\\\\right) }^{-1}\\\\right\\\\rbrack }^{\\\\top }G\\\\left\\\\lbrack {\\\\Phi \\\\left( T\\\\right) \\\\Phi {\\\\left( t\\\\right) }^{-1}}\\\\right\\\\rbrack } \\n$$ \\n\\n$$ \\n\\\\left. {+{\\\\int }_{t}^{T}{\\\\left\\\\lbrack \\\\Phi \\\\left( s\\\\right) \\\\Phi {\\\\left( t\\\\right) }^{-1}\\\\right\\\\rbrack }^{\\\\top }Q\\\\left( s\\\\right) \\\\left\\\\lbrack {\\\\Phi \\\\left( s\\\\right) \\\\Phi {\\\\left( t\\\\right) }^{-1}}\\\\right\\\\rbrack {ds}}\\\\right\\\\} , \\\\tag{2.2.7} \\n$$ \\n\\nwhere $\\\\Phi$ is the solution of (2.2.1).",
        "informal_proof": "Proof Let us first identify the adjoint operators ${L}_{t}^{ * },{\\\\Gamma }_{t}^{ * },{\\\\widehat{L}}_{t}^{ * }$, and ${\\\\widehat{\\\\Gamma }}_{t}^{ * }$ . To this end, we let $\\\\left( {{\\\\mathcal{Y}}_{\\\\eta }^{\\\\xi },{\\\\mathcal{Z}}_{\\\\eta }^{\\\\xi }}\\\\right)$ be the adapted solution to the following backward stochastic differential equation (BSDE, for short): \\n\\n$$ \\n\\\\left\\\\{ \\\\begin{array}{l} d{\\\\mathcal{Y}}_{\\\\eta }^{\\\\xi }\\\\left( s\\\\right) = - \\\\left( {{A}^{\\\\top }{\\\\mathcal{Y}}_{\\\\eta }^{\\\\xi } + {C}^{\\\\top }{\\\\mathcal{Z}}_{\\\\eta }^{\\\\xi } + \\\\xi }\\\\right) {ds} + {\\\\mathcal{Z}}_{\\\\eta }^{\\\\xi }{dW}\\\\left( s\\\\right) ,\\\\;s \\\\in \\\\left\\\\lbrack {t, T}\\\\right\\\\rbrack , \\\\\\\\ {\\\\mathcal{Y}}_{\\\\eta }^{\\\\xi }\\\\left( T\\\\right) = \\\\eta , \\\\end{array}\\\\right. \\n$$ \\n\\nwhere $\\\\xi \\\\in {L}_{\\\\mathbb{F}}^{2}\\\\left( {\\\\Omega ;{L}^{1}\\\\left( {t, T;{\\\\mathbb{R}}^{n}}\\\\right) }\\\\right)$ and $\\\\eta \\\\in {\\\\mathcal{X}}_{T}$ . Applying Itô’s formula to $s \\\\mapsto \\\\left\\\\langle {{\\\\overset{ \\\\circ }{X}}_{x}^{u}\\\\left( s\\\\right) }\\\\right.$ , $\\\\left. {{Y}_{\\\\eta }^{\\\\xi }\\\\left( s\\\\right) }\\\\right\\\\rangle$, we obtain \\n\\n$$ \\n\\\\mathbb{E}\\\\left\\\\lbrack {\\\\left\\\\langle {{\\\\overset{ \\\\circ }{X}}_{x}^{u}\\\\left( T\\\\right) ,\\\\eta }\\\\right\\\\rangle - \\\\left\\\\langle {x,{\\\\mathcal{Y}}_{\\\\eta }^{\\\\xi }\\\\left( t\\\\right) }\\\\right\\\\rangle }\\\\right\\\\rbrack \\n$$ \\n\\n$$ \\n= \\\\mathbb{E}{\\\\int }_{t}^{T}\\\\left\\\\lbrack {\\\\left\\\\langle {A{\\\\overset{ \\\\circ }{X}}_{x}^{u} + {Bu},{\\\\mathcal{Y}}_{\\\\eta }^{\\\\xi }}\\\\right\\\\rangle - \\\\left\\\\langle {{\\\\overset{ \\\\circ }{X}}_{x}^",
        "id": "college_math_121606"
    },
    {
        "informal_statement": "Proposition 2. The quantities ${P}_{c}^{t}\\left( i\\right)$ and ${P}_{u}^{t}\\left( i\\right)$ are non-decreasing in $t$ .",
        "informal_proof": "Proof. We first prove the proposition for ${P}_{c}^{t}\\left( i\\right)$ . We will prove that ${P}_{c}^{t}\\left( i\\right) \\geq$ ${P}_{c}^{t - 1}\\left( i\\right)$ for all $t \\geq 1$ .\\n\\nBase Case: If $i$ is the destination then for all $t \\geq 1,{P}_{c}^{t}\\left( i\\right) = 1$ so ${P}_{c}^{t}\\left( i\\right) =$ ${P}_{c}^{t - 1}\\left( i\\right)$ . If $i$ is not the destination, then for $t = 1,{P}_{c}^{t - 1}\\left( i\\right) = {P}_{c}^{0}\\left( i\\right) = 0$, so ${P}_{c}^{t}\\left( i\\right) = {P}_{c}^{1}\\left( i\\right) \\geq {P}_{c}^{0}\\left( i\\right) .\\n\\nInductive Hypothesis. Assume ${P}_{c}^{t}\\left( i\\right) \\geq {P}_{c}^{t - 1}\\left( i\\right)$ and ${P}_{u}^{t}\\left( i\\right) \\geq {P}_{u}^{t - 1}\\left( i\\right)$ .\\n\\nWe will show ${P}_{c}^{t + 1}\\left( i\\right) \\geq {P}_{c}^{t}\\left( i\\right)$ . From 10, we have:\\n\\n$$\\n{P}_{c}^{t + 1}\\left( i\\right) = \\max \\left\\{ \\begin{matrix} \\mathop{\\max }\\limits_{{\\forall j \\in {N}_{c}^{t}\\left( i\\right) }}\\left\\{ {{P}_{c}\\left( {j \\mid i}\\right) {P}_{c}^{t + 1 - {t}_{c}\\left( {i, j}\\right) }\\left( j\\right) }\\right. & \\\\ \\left. {+\\left( {1 - {P}_{c}\\left( {j \\mid i}\\right) }\\right) {P}_{u}^{t + 1 - {t}_{c}\\left( {i, j}\\right) }\\left( j\\right) }\\right\\} & \\text{ (Not waiting at }i) \\\\ \\mathop{\\max }\\limits_{{\\forall w \\leq {T}_{\\max }}}\\left\\{ {{P}_{c}^{ * }\\left( {i, w}\\right) {P}_{c}^{t + 1 - w}\\left( i\\right) }\\right. & \\\\ \\left. {+\\left( {1 - {P}_{c}^{ * }\\left( {i, w}\\right) }\\right) {P}_{u}^{t + 1 - w}\\left( i\\right) }\\right\\} & \\text{ (Waiting at }i) \\end{matrix}\\right.\\n$$\\n\\n$$\\n\\geq \\max \\left\\{ \\begin{matrix} \\mathop{\\max }\\limits_{{\\forall j \\in {N}_{c}^{t}\\left( i\\right) }}\\left\\{ {{P}_{c}\\left( {j \\mid i}\\right) {P}_{c}^{t - {t}_{c}\\left( {i, j}\\right) }\\left( j\\right) }\\right. \\\\ \\left. {+\\left( {1 - {P}_{c}\\left( {j \\mid i}\\right) }\\right) {P}_{u}^{t - {t}_{c}\\left( {i, j}\\right) }\\left( j\\right) }\\right\\} \\\\ \\mathop{\\max }\\limits_{{\\forall w \\leq {T}_{\\max }}}\\left\\{ {{P}_{c}^{ * }\\left( {i, w}\\right) {P}_{c}^{t - w}\\left( i\\right) }\\right. \\\\ \\left. {+\\left( {1 - {P}_{c}^{ * }\\left( {i, w}\\right) }\\right) {P}_{u}^{t - w}\\left( i\\right) }\\right\\} \\end{matrix}\\right. \\tag{By Ind. Hyp.}\\n$$\\n\\n$$\\n= {P}_{c}^{t}\\left( i\\right)\\n$$",
        "id": "college_math_17016"
    },
    {
        "informal_statement": "Theorem 1. Consider the linear model\\n\\n$$ \\n\\\\mathbf{Y} = \\\\mathbf{X}\\\\mathbf{\\\\beta } + \\\\mathbf{\\\\varepsilon },\\n$$\\n\\nwhere $\\\\mathbf{X}$ is an $n \\\\times k$ matrix, $\\\\left( {x}_{ij}\\\\right), i = 1,2,\\\\ldots, n, j = 1,2,\\\\ldots, k$, of known constants and full rank $k < n,\\\\mathbf{\\\\beta }$ is a vector of unknown parameters ${\\\\beta }_{1},{\\\\beta }_{2},\\\\ldots ,{\\\\beta }_{k}$ , and $\\\\varepsilon = \\\\left( {{\\\\varepsilon }_{1},{\\\\varepsilon }_{2},\\\\ldots ,{\\\\varepsilon }_{n}}\\\\right)$ is a vector of nonobservable independent normal RVs with common variance ${\\\\sigma }^{2}$ and mean ${E\\\\varepsilon } = 0$ . The GLR test for testing the linear hypothesis ${H}_{0} : \\\\mathbf{H}\\\\mathbf{\\\\beta } = \\\\mathbf{0}$, where $\\\\mathbf{H}$ is an $r \\\\times k$ matrix of full rank $r \\\\leq k$, is to reject ${H}_{0}$ at level $\\\\alpha$ if $F \\\\geq {F}_{\\\\alpha }$, where ${P}_{{H}_{0}}\\\\left\\\\{ {F \\\\geq {F}_{\\\\alpha }}\\\\right\\\\} = \\\\alpha$ and $F$ is the RV given by\\n\\n(9)\\n\\n$$ \\nF = \\\\frac{{\\\\left( \\\\mathbf{Y} - \\\\mathbf{X}\\\\widehat{\\\\widehat{\\\\mathbf{\\\\beta }}}\\\\right) }^{\\\\prime }\\\\left( {\\\\mathbf{Y} - \\\\mathbf{X}\\\\widehat{\\\\widehat{\\\\mathbf{\\\\beta }}}}\\\\right) - {\\\\left( \\\\mathbf{Y} - \\\\mathbf{X}\\\\widehat{\\\\mathbf{\\\\beta }}}\\\\right) }^{\\\\prime }\\\\left( {\\\\mathbf{Y} - \\\\mathbf{X}\\\\widehat{\\\\mathbf{\\\\beta }}}\\\\right) }{{\\\\left( \\\\mathbf{Y} - \\\\mathbf{X}\\\\widehat{\\\\mathbf{\\\\beta }}}\\\\right) }^{\\\\prime }\\\\left( {\\\\mathbf{Y} - \\\\mathbf{X}\\\\widehat{\\\\mathbf{\\\\beta }}}\\\\right) }.\\n$$\\n\\nIn (9), $\\\\widehat{\\\\mathbf{\\\\beta }}$, and $\\\\widehat{\\\\widehat{\\\\mathbf{\\\\beta }}}$ are the MLEs of $\\\\mathbf{\\\\beta }$ under $\\\\Theta$ and ${\\\\Theta }_{0}$, respectively. Moreover, the RV $\\\\left\\\\lbrack {\\\\left( {n - k}\\\\right) /r}\\\\right\\\\rbrack F$ has the $F$ -distribution with $\\\\left( {r, n - k}\\\\right)$ d.f. under ${H}_{0}$ .",
        "informal_proof": "Proof. The GLR test of ${H}_{0} : \\\\mathbf{H}\\\\mathbf{\\\\beta } = \\\\mathbf{0}$ is to reject ${H}_{0}$ if and only if $\\\\lambda \\\\left( \\\\mathbf{y}\\\\right) < c$ , where\\n\\n(10)\\n\\n$$ \\n\\\\lambda \\\\left( \\\\mathbf{y}\\\\right) = \\\\frac{\\\\mathop{\\\\sup }\\\\limits_{{\\\\mathbf{\\\\theta } \\\\in {\\\\Theta }_{0}}}{f}_{\\\\mathbf{\\\\beta },{\\\\sigma }^{2}}\\\\left( \\\\mathbf{y}\\\\right) }{\\\\mathop{\\\\sup }\\\\limits_{{\\\\mathbf{\\\\theta } \\\\in \\\\Theta }}{f}_{\\\\mathbf{\\\\beta },{\\\\sigma }^{2}}\\\\left( \\\\mathbf{y}\\\\right) },\\n$$\\n\\n$\\\\mathbf{\\\\theta } = {\\\\left( {\\\\mathbf{\\\\beta }}^{\\\\prime },{\\\\sigma }^{2}\\\\right) }^{\\\\prime }$, and ${\\\\Theta }_{0} = \\\\left\\\\{ {{\\\\left( {\\\\mathbf{\\\\beta }}^{\\\\prime },{\\\\sigma }^{2}\\\\right) }^{\\\\prime } : \\\\mathbf{H}\\\\mathbf{\\\\beta } = \\\\mathbf{0}}\\\\right\\\\}$ . Let $\\\\widehat{\\\\mathbf{\\\\theta }} = {\\\\left( {\\\\widehat{\\\\mathbf{\\\\beta }}}^{\\\\prime },{\\\\widehat{\\\\sigma }}^{2}\\\\right) }^{\\\\prime }$ be the MLE of ${\\\\mathbf{\\\\theta }}^{\\\\prime } \\\\in \\\\Theta$, and $\\\\widehat{\\\\widehat{\\\\mathbf{\\\\theta }}} = {\\\\left( {\\\\widehat{\\\\widehat{\\\\mathbf{\\\\beta }}}}^{\\\\prime },{\\\\widehat{\\\\widehat{\\\\sigma }}}^{2}\\\\right) }^{\\\\prime }$ be the MLE of $\\\\mathbf{\\\\theta }$ under ${H}_{0}$, that is, when $\\\\mathbf{H}\\\\mathbf{\\\\beta } = \\\\mathbf{0}$ . It is easily seen that $\\\\widehat{\\\\mathbf{\\\\beta }}$ is the value of $\\\\mathbf{\\\\beta }$ that minimizes ${\\\\left( \\\\mathbf{y} - \\\\mathbf{X}\\\\mathbf{\\\\beta }\\\\right) }^{\\\\prime }\\\\left( {\\\\mathbf{y} - \\\\mathbf{X}\\\\mathbf{\\\\beta }}\\\\right)$, and\\n\\n(11)\\n\\n$$ \\n{\\\\widehat{\\\\sigma }}^{2} = {n}^{-1}{\\\\left( \\\\mathbf{y} - \\\\mathbf{X}\\\\widehat{\\\\mathbf{\\\\beta }}\\\\right) }^{\\\\prime }\\\\left( {\\\\mathbf{y} - \\\\mathbf{X}\\\\widehat{\\\\mathbf{\\\\beta }}}\\\\right) .\\n$$\\n\\nSimilarly, $\\\\widehat{\\\\mathbf{\\\\beta }}$ is the value of $\\\\mathbf{\\\\beta }$ that minimizes ${\\\\left( \\\\mathbf{y} - \\\\mathbf{X}\\\\mathbf{\\\\beta }\\\\right) }^{\\\\prime }\\\\left( {\\\\mathbf{y} - \\\\mathbf{X}\\\\mathbf{\\\\beta }}\\\\right)$ subject to $\\\\mathbf{H}\\\\mathbf{\\\\beta } = \\\\mathbf{0}$ ,\\n\\nand\\n\\n(12)\\n\\n$$ \\n{\\\\widehat{\\\\widehat{\\\\sigma }}}^{2} = {n}^{-1}{\\\\left( \\\\mathbf{y} - \\\\mathbf{X}\\\\widehat{\\\\widehat{\\\\mathbf{\\\\beta }}}\\\\right) }^{\\\\prime }\\\\left( {\\\\mathbf{y} - \\\\mathbf{X}\\\\widehat{\\\\widehat{\\\\mathbf{\\\\beta }}}}\\\\right) .\\n$$\\n\\nIt follows that\\n\\n(13)\\n\\n$$ \\n\\\\lambda \\\\left( \\\\mathbf{y}\\\\right) = {\\\\left( \\\\frac{{\\\\widehat{\\\\sigma }}^{2}}{{\\\\widehat{\\\\widehat{\\\\sigma }}}^{2}}\\\\right) }^{n/2}.\\n$$\\n\\nThe critical region $\\\\lambda \\\\left( \\\\mathbf{y}\\\\right) < c$ is equivalent to the region $\\{ \\\\l",
        "id": "college_math_20178"
    },
    {
        "informal_statement": "Lemma 3.5.2. Let \\( B,{B}_{1},{B}_{2},\\ldots \\) be independent. Then \\( \\{ B\\} \\) and \\( \\sigma \\left( {{B}_{1},{B}_{2},\\ldots }\\right) \\) are independent classes, i.e. if \\( S \\in \\sigma \\left( {{B}_{1},{B}_{2},\\ldots }\\right) \\), then \\( \\mathbf{P}(S \\cap \\) \\( B) = \\mathbf{P}\\left( S\\right) \\mathbf{P}\\left( B\\right) \\) .",
        "informal_proof": "Proof. Assume that \\( \\mathbf{P}\\left( B\\right) > 0 \\), otherwise the statement is trivial.\\n\\nLet \\( \\mathcal{J} \\) be the collection of all sets of the form \\( {D}_{{i}_{1}} \\cap {D}_{{i}_{2}} \\cap \\ldots \\cap {D}_{{i}_{n}} \\), where \\( n \\in \\mathbf{N} \\) and where \\( {D}_{{i}_{j}} \\) is either \\( {B}_{{i}_{j}} \\) or \\( {B}_{{i}_{j}}^{C} \\), together with \\( \\varnothing \\) and \\( \\Omega \\) . Then for \\( A \\in \\mathcal{J} \\), we have by independence that \\( \\mathbf{P}\\left( A\\right) = \\mathbf{P}\\left( {B \\cap A}\\right) /\\mathbf{P}\\left( B\\right) \\).\\n\\nNow define a new probability measure \\( \\mathbf{Q} \\) on \\( \\sigma \\left( {{B}_{1},{B}_{2},\\ldots }\\right) \\) by \\( \\mathbf{Q}\\left( S\\right) = \\) \\( \\mathbf{P}\\left( {B \\cap S}\\right) /\\mathbf{P}\\left( B\\right) \\), for \\( S \\in \\sigma \\left( {{B}_{1},{B}_{2},\\ldots }\\right) \\) . Then \\( \\mathbf{Q}\\left( \\varnothing \\right) = 0,\\mathbf{Q}\\left( \\Omega \\right) = 1 \\), and \\( \\mathbf{Q} \\) is countably additive since \\( \\mathbf{P} \\) is, so \\( \\mathbf{Q} \\) is indeed a probability measure. Furthermore, \\( \\mathbf{Q} \\) and \\( \\mathbf{P} \\) agree on \\( \\mathcal{J} \\) . Hence, by Proposition 2.5.8, \\( \\mathbf{Q} \\) and \\( \\mathbf{P} \\) agree on \\( \\sigma \\left( \\mathcal{J}\\right) = \\sigma \\left( {{B}_{1},{B}_{2},\\ldots }\\right) \\) . That is, \\( \\mathbf{P}\\left( S\\right) = \\mathbf{Q}\\left( S\\right) = \\mathbf{P}\\left( {B \\cap S}\\right) /\\mathbf{P}\\left( B\\right) \\) for all \\( S \\in \\sigma \\left( {{B}_{1},{B}_{2},\\ldots }\\right) \\), as required.",
        "id": "college_math_203191"
    },
    {
        "informal_statement": "Lemma 3.2. If $X$ is compact and $f : X \\rightarrow I$ is a function, then ${f}^{-1}\\left( w\\right) = {A}_{w}^{f}$ for all but countably many $w \\in I$ .",
        "informal_proof": "Proof. Suppose the lemma is false. Without loss of generality, we may assume that there is an uncountable $W \\subseteq I$ and numbers $\\varepsilon ,\\delta > 0$ such that $\\mathrm{d}\\left( {{x}_{w},{f}^{-1}\\left( \\left( {w, w + \\delta }\\right) \\right) }\\right) > \\varepsilon$ where for each $w \\in W$ we have ${x}_{w} \\in {f}^{-1}\\left( w\\right)$ . Since $W$ is uncountable, there is a decreasing convergent sequence ${\\left\\{ {w}_{n}\\right\\} }_{n = 1}^{\\infty }$ such that ${w}_{n} \\in W$ for all $n$ . There is an integer $k$ large enough that $\\left| {{w}_{n} - {w}_{m}}\\right| < \\delta$ for all $m, n \\geq k$ . By our choice of $\\delta$ we must have $\\mathrm{d}\\left( {{x}_{{w}_{n}},{x}_{{w}_{m}}}\\right) > \\varepsilon$ for all $n, m \\geq k$, this however contradicts the compactness of $X$ . Thus, we have the lemma.",
        "id": "college_math_138206"
    },
    {
        "informal_statement": "Example 3.3.10 Convergence a.s. \\( \\left\\lbrack P\\right\\rbrack \\) does not imply convergence in \\( {\\mathcal{L}}^{1} \\) . Let \\( (\\Omega = \\left\\lbrack {0,1}\\right\\rbrack ,\\mathcal{F} = \\) \\( \\mathcal{B}\\left( \\left\\lbrack {0,1}\\right\\rbrack \\right), P = \\lambda ) \\), where \\( \\lambda \\) is the Lebesgue measure. Define for \\( n \\geq 1 \\) \\[ {X}_{n}\\left( \\omega \\right) = n{1}_{\\left\\lbrack 0,1/n\\right\\rbrack }\\left( \\omega \\right) \\;\\text{ for every }\\;\\omega \\in \\left\\lbrack {0,1}\\right\\rbrack . \\] Let \\( X\\left( \\omega \\right) = 0 \\), for every \\( \\omega \\in \\Omega \\) . If we let \\( E = \\{ 0\\} \\in \\mathcal{B}\\left( \\left\\lbrack {0,1}\\right\\rbrack \\right) \\), we have that \\( \\lambda \\left( E\\right) = 0 \\) . And if \\( \\omega \\in \\left\\lbrack {0,1}\\right\\rbrack \\smallsetminus E = (0,1\\rbrack \\), then there exists \\( N \\geq 1 \\), such that \\( 1/N < \\omega \\) . Hence, if \\( n \\geq N \\), we have that \\( {X}_{n}\\left( \\omega \\right) = n{1}_{\\left\\lbrack 0,1/n\\right\\rbrack }\\left( \\omega \\right) = 0 = X\\left( \\omega \\right) \\), from here \\( {X}_{n}\\overset{\\text{ a.s. }}{ \\rightarrow }X \\) .",
        "informal_proof": "Also, by Theorem 3.2.4, \\( {X}_{n}\\overset{P}{ \\rightarrow }X \\) . On the other hand, \\[ E\\left( \\left| {{X}_{n} - X}\\right| \\right) = E\\left( {X}_{n}\\right) = {\\int }_{0}^{1}n{1}_{\\left\\lbrack 0,1/n\\right\\rbrack }\\left( \\omega \\right) {d\\omega } = {n\\lambda }\\left( \\left\\lbrack {0,1/n}\\right\\rbrack \\right) = 1\\;\\text{ for every }\\;n \\geq 1. \\] Therefore, \\( {X}_{n} \\) does not converge in \\( {\\mathcal{L}}^{1} \\) to \\( X \\) .",
        "id": "college_math_314807"
    },
    {
        "informal_statement": "Theorem 10. If for all $f \\in C\\left( X\\right) ,\\left| {{f}^{ \\rightarrow }\\left( X\\right) }\\right| < {2}^{\\omega }$ and if $X$ is pseudocompact, then $X$ is scattered.",
        "informal_proof": "Proof. Suppose that for all $f \\in C\\left( X\\right) ,\\left| {{f}^{ \\rightarrow }\\left( X\\right) }\\right| < {2}^{\\omega }$ and that $X$ is not scattered. Then ${\\beta X}$ is not scattered and so by [6], there is $g \\in C\\left( {\\beta X}\\right)$ such that $\\left| {{g}^{ \\rightarrow }\\left( {\\beta X}\\right) }\\right| = {2}^{\\omega }$ . Now by the hypothesis on $X$, there is $r \\in {g}^{ \\rightarrow }\\left( {\\beta X}\\right) - {g}^{ \\rightarrow }\\left( X\\right)$ . Then ${g}^{ \\leftarrow }\\{ r\\}$ is a zero-set of ${\\beta X}$ that misses $X$ and so $X$ is not pseudocompact (see $\\left\\lbrack {3,6\\mathrm{I}\\left( 1\\right) }\\right\\rbrack$ ).",
        "id": "college_math_137378"
    },
    {
        "informal_statement": "Proposition 1.1.2 If the functions \\( \\xi \\) and \\( \\eta \\) have the form (1.1.18) and satisfy the condition (1.1.19), then \\( \\overline{{a}_{12}} = 0 \\) .",
        "informal_proof": "Proof From \\( {a}_{12}^{2} = {a}_{11}{a}_{22} \\), we deduce that \\( {a}_{11} \\) and \\( {a}_{22} \\) have simultaneously the same sign and we do not restrict the generality if we assume that \\( {a}_{11} > 0 \\) and \\( {a}_{22} > 0 \\) . Then we obtain \\( {a}_{12} = \\pm {\\sqrt{a}}_{11}{\\sqrt{a}}_{22} \\) . According to Proposition 1.1.1 we deduce that \\( \\overline{{a}_{11}} = 0 \\) and therefore\\n\\n\\[ 0 = {a}_{11}{\\left( \\frac{\\partial \\xi }{\\partial x}\\right) }^{2} + 2{a}_{12}\\frac{\\partial \\xi }{\\partial x}\\frac{\\partial \\xi }{\\partial y} + {a}_{11}{\\left( \\frac{\\partial \\xi }{\\partial y}\\right) }^{2} \\]\\n\\n\\[ = {\\left( {\\sqrt{a}}_{11}\\frac{\\partial \\xi }{\\partial x}\\right) }^{2} \\pm {\\sqrt{a}}_{11}{\\sqrt{a}}_{22}\\frac{\\partial \\xi }{\\partial x}\\frac{\\partial \\xi }{\\partial y} + {\\left( {\\sqrt{a}}_{22}\\frac{\\partial \\xi }{\\partial x}\\right) }^{2} \\]\\n\\n\\[ = {\\left( {\\sqrt{a}}_{11}\\frac{\\partial \\xi }{\\partial x} \\pm {\\sqrt{a}}_{22}\\frac{\\partial \\xi }{\\partial y}\\right) }^{2}. \\]\\n\\nFrom here, we get\\n\\n\\[ \\sqrt{{a}_{11}}\\frac{\\partial \\xi }{\\partial x} \\pm \\sqrt{{a}_{22}}\\frac{\\partial \\xi }{\\partial y} = 0 \\tag{1.1.20} \\]\\n\\nBy using (1.1.9), it follows that\\n\\n\\[ \\overline{{a}_{12}} = {a}_{11}\\frac{\\partial \\xi }{\\partial x}\\frac{\\partial \\eta }{\\partial x} + {a}_{12}\\left( {\\frac{\\partial \\xi }{\\partial x}\\frac{\\partial \\eta }{\\partial y} + \\frac{\\partial \\xi }{\\partial y}\\frac{\\partial \\eta }{\\partial x}}\\right) + {a}_{22}\\frac{\\partial \\xi }{\\partial y}\\frac{\\partial \\eta }{\\partial y} \\]\\n\\n\\[ = \\left( {\\sqrt{{a}_{11}}\\frac{\\partial \\xi }{\\partial x} \\pm \\sqrt{{a}_{22}}\\frac{\\partial \\xi }{\\partial y}}\\right) \\left( {\\sqrt{{a}_{11}}\\frac{\\partial \\eta }{\\partial x} \\pm \\sqrt{{a}_{22}}\\frac{\\partial \\eta }{\\partial y}}\\right) ,\\]\\n\\nso that by taking into account (1.1.20) we deduce that \\( \\overline{{a}_{12}} = 0 \\) .",
        "id": "college_math_203600"
    },
    {
        "informal_statement": "Theorem 5.4. Let $A$ be the full automorphism group of $t - \\left( {v, k,\\lambda }\\right)$ designs where $v = {2k} + 1$ and $t$ is even or $\\lambda = \\frac{1}{2}\\left( \\begin{matrix} v - t \\\\ k - t \\end{matrix}\\right)$ . If $A$ acts transitively on the point set but has no transitive extension then two Alltop extensions ${\\mathcal{D}}_{1}^{ + }$ and ${\\mathcal{D}}_{2}^{ + }$ of $t - \\left( {v, k,\\lambda }\\right)$ designs ${\\mathcal{D}}_{1}$ and ${\\mathcal{D}}_{2}$ with full automorphism group $A$ are isomorphic if and only if ${\\mathcal{D}}_{1}$ and ${\\mathcal{D}}_{2}$ are isomorphic.",
        "informal_proof": "The proof is immediate from the fact that the full automorphism group of an Alltop extension here either is transitive or has the new point as a fixed point. So, if $A$ cannot be transitively extended then the derived design at the new point is not isomorphic to any other derived design and thus characterizes the isomorphism type of the Alltop extension.",
        "id": "college_math_6136"
    },
    {
        "informal_statement": "Consider the relation $\\\\Vdash$ of our basic Example 1.2.1.",
        "informal_proof": "We can compute some example of neighborhoods: $\\\\Vdash \\\\left( a\\\\right) = \\\\left\\\\{ {b,{b}^{\\\\prime }}\\\\right\\\\} , \\\\Vdash \\\\left( {a}^{\\\\prime }\\\\right) = \\\\left\\\\{ {{b}^{\\\\prime },{b}^{\\\\prime \\\\prime \\\\prime }}\\\\right\\\\}$ , $\\\\Vdash \\\\left( \\\\left\\\\{ {a,{a}^{\\\\prime }}\\\\right\\\\} \\\\right) = \\\\left\\\\{ {b,{b}^{\\\\prime },{b}^{\\\\prime \\\\prime \\\\prime }}\\\\right\\\\} = \\\\Vdash \\\\left( a\\\\right) \\\\cup \\\\Vdash \\\\left( {a}^{\\\\prime }\\\\right)$ . One can verify that $a \\\\in \\\\Vdash - \\\\left( \\\\left\\\\{ {b,{b}^{\\\\prime \\\\prime }}\\\\right\\\\} \\\\right)$ implies $\\\\Vdash \\\\left( a\\\\right) \\\\cap \\\\left\\\\{ {b,{b}^{\\\\prime \\\\prime }}\\\\right\\\\} = \\\\left\\\\{ {b,{b}^{\\\\prime }}\\\\right\\\\} \\\\cap \\\\left\\\\{ {b,{b}^{\\\\prime \\\\prime }}\\\\right\\\\} = \\\\{ b\\\\} \\\\neq \\\\varnothing$ ; and vice-versa.",
        "id": "college_math_29942"
    },
    {
        "informal_statement": "Lemma 10.1.2. Let \\( f \\) be a Borel function as in (10.1.1) satisfying (10.1.11), and \\( \\bar{F} \\) be given by (10.1.3). Let \\( A \\in {\\mathcal{A}}_{0} \\), and \\( u \\in {W}^{1,1}\\left( A\\right) \\) be such that \\( \\bar{F}\\left( {A, u}\\right) < + \\infty \\) . Then\\n\\n(10.1.14)\\n\\n\\[ \\n\\nabla u\\left( x\\right) \\in \\overline{\\operatorname{dom}f}\\text{for a.e.}x \\in A\\text{.} \\n\\]",
        "informal_proof": "Proof. Since \\( \\bar{F}\\left( {A, u}\\right) < + \\infty \\), there exists \\( \\left\\{ {u}_{h}\\right\\} \\subseteq {W}_{\\text{loc }}^{1,\\infty }\\left( {\\mathbf{R}}^{n}\\right) \\) such that \\( {u}_{h} \\rightarrow u \\) in \\( {L}^{1}\\left( A\\right) \\) and\\n\\n(10.1.15)\\n\\n\\[ \\n\\text{for every}h \\in \\mathbf{N},\\nabla {u}_{h}\\left( x\\right) \\in \\operatorname{dom}f\\text{for a.e.}x \\in A\\text{.} \\n\\]\\n\\nWe now observe that, being by (10.1.11) \\( \\overline{\\operatorname{dom}f} \\) closed and convex, there exist two families \\( {\\left\\{ {\\alpha }_{\\theta }\\right\\} }_{\\theta \\in \\mathcal{T}} \\subseteq {\\mathbf{R}}^{n} \\), and \\( {\\left\\{ {\\beta }_{\\theta }\\right\\} }_{\\theta \\in \\mathcal{T}} \\subseteq \\mathbf{R} \\) such that \\( z \\in \\overline{\\operatorname{dom}f} \\) if and only if \\( {\\alpha }_{\\theta } \\cdot z + {\\beta }_{\\theta } \\geq 0 \\) for every \\( \\theta \\in \\mathcal{I} \\) . Therefore, by (10.1.15) we obtain that\\n\\n(10.1.16)\\n\\n\\[ \\n{\\alpha }_{\\theta } \\cdot {\\int }_{A}\\varphi \\nabla {u}_{h}{dx} + {\\beta }_{\\theta } \\geq 0 \\n\\]\\n\\nfor every \\( h \\in \\mathbf{N},\\theta \\in \\mathcal{T} \\), and every \\( \\varphi \\in {C}_{0}^{1}\\left( A\\right) \\) with \\( \\varphi \\geq 0,{\\int }_{A}{\\varphi dx} = 1 \\) .\\n\\nBy (10.1.16), taking the limit as \\( h \\) diverges, we deduce that\\n\\n\\[ \\n{\\int }_{A}\\varphi \\nabla {udx} \\in \\overline{\\operatorname{dom}f}\\text{ for every }\\varphi \\in {C}_{0}^{1}\\left( A\\right) \\text{ with }\\varphi \\geq 0,{\\int }_{A}{\\varphi dx} = 1, \\n\\]\\n\\nfrom which (10.1.14) follows. -",
        "id": "college_math_372897"
    }
]